{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "[nltk_data] Downloading package reuters to /home/fmena/nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/fmena/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras,gc,nltk\n",
    "from keras.layers import *\n",
    "from keras.models import Sequential,Model\n",
    "from keras import backend as K\n",
    "from astropy.table import Table\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import expit\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "nltk.download('reuters')\n",
    "nltk.download('wordnet')\n",
    "%matplotlib inline\n",
    "\n",
    "from base_networks import *\n",
    "\n",
    "np.random.seed(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "### 20 Newsgroup\n",
    "---\n",
    "\n",
    "Utilizado en trabajos previos de *Hashing* de texto (http://people.csail.mit.edu/jrennie/20Newsgroups), también disponible en **sklearn**. El dataset contiene textos de usuarios asociados a temáticas de noticias etiquetados como pertenenciente a uno de 20 grupos de noticias, el detalle de los conjuntos se detalla a continuación:\n",
    "\n",
    "|Tipo set|Datos|Label|\n",
    "|---|---|---|\n",
    "|Entrenamiento|11.314|20|\n",
    "|Pruebas|7.532|20|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos de entrenamiento:  (11314,)\n",
      "Datos de prueba:  (7532,)\n"
     ]
    }
   ],
   "source": [
    "dat_n = \"20News\"\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_t = fetch_20newsgroups(subset='train')\n",
    "newsgroups_test = fetch_20newsgroups(subset='test')\n",
    "labels = newsgroups_t.target_names\n",
    "\n",
    "texts_t = newsgroups_t.data\n",
    "y_t = newsgroups_t.target\n",
    "labels_t = [labels[valor] for valor in y_t]\n",
    "\n",
    "texts_test = newsgroups_test.data\n",
    "y_test = newsgroups_test.target\n",
    "labels_test = [labels[valor] for valor in y_test]\n",
    "\n",
    "print(\"Datos de entrenamiento: \",y_t.shape)\n",
    "print(\"Datos de prueba: \",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es el dataset con los documentos más extensos en información o textos, lo que podría ser un indicador que es el más difícil ya que se deberá comprimir toda la información presente en estos textos.\n",
    "\n",
    "### Reuters21578\n",
    "---\n",
    "Similar a 20NewsGroup es un datataset de textos de noticias del periodico de Reuters en 1987, citado también en el estado del arte (https://www.nltk.org/book/ch02.html) y disponible en la librería **nltk**. El detalle de los conjuntos se muestra a continuación:\n",
    "\n",
    "|Tipo set|Datos|Label|\n",
    "|---|---|---|\n",
    "|Entrenamiento|7.769|90|\n",
    "|Pruebas|3.019|90|\n",
    "\n",
    "Los documentos pueden pertenecer a **múltiples tópicos** dentro de 90 disponibles en el dataset manualmente etiquetadas, ésto es porque un texto asociado a una noticia puede hablar de varios tópicos a la vez. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10788 documents\n",
      "7769 total training documents\n",
      "3019 total test documents\n"
     ]
    }
   ],
   "source": [
    "dat_n = \"Reuters\"\n",
    "\n",
    "from nltk.corpus import reuters\n",
    "documents_stat = reuters.fileids()\n",
    "print(str(len(documents_stat)) + \" documents\")\n",
    "\n",
    "train_docs_stat = list(filter(lambda doc: doc.startswith(\"train\"), documents_stat))\n",
    "print(str(len(train_docs_stat)) + \" total training documents\")\n",
    "test_docs_stat = list(filter(lambda doc: doc.startswith(\"test\"), documents_stat))\n",
    "print(str(len(test_docs_stat)) + \" total test documents\")\n",
    "\n",
    "texts_t = [reuters.raw(archivo) for archivo in train_docs_stat]\n",
    "labels_t = [reuters.categories(archivo) for archivo in train_docs_stat]\n",
    "\n",
    "texts_test = [reuters.raw(archivo) for archivo in test_docs_stat]\n",
    "labels_test = [reuters.categories(archivo) for archivo in test_docs_stat]\n",
    "\n",
    "labels = reuters.categories()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### top 20 labels (as VDSH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category with most data (earn) has = 3964, the top-K category (bop) has = 105\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['earn',\n",
       " 'acq',\n",
       " 'money-fx',\n",
       " 'grain',\n",
       " 'crude',\n",
       " 'trade',\n",
       " 'interest',\n",
       " 'ship',\n",
       " 'wheat',\n",
       " 'corn',\n",
       " 'dlr',\n",
       " 'money-supply',\n",
       " 'oilseed',\n",
       " 'sugar',\n",
       " 'coffee',\n",
       " 'gnp',\n",
       " 'gold',\n",
       " 'veg-oil',\n",
       " 'soybean',\n",
       " 'bop']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import get_topK_labels,set_newlabel_list,enmask_data\n",
    "new_labels = get_topK_labels(labels_t+labels_test, labels, K=20)\n",
    "labels = new_labels\n",
    "new_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reuters Corpus Volume 1\n",
    "---\n",
    "Corpus de Reuters extendido, con los 103 tópicos originales. Disponible en trabajos previos (https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multilabel.html) y a través de **sklearn** (https://scikit-learn.org/0.17/datasets/rcv1.html). La representación son las *features* ya extraídas.\n",
    "\n",
    "|Tipo set|Datos|Label|\n",
    "|---|---|---|\n",
    "|Entrenamiento|23.149|103|\n",
    "|Pruebas|781.265|103|\n",
    "\n",
    "Representación de **sklearn** viene pre-procesada con la transformación logarítmica de TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-e85c480aa03f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfetch_rcv1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mrcv1_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_rcv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mrcv1_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_rcv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrcv1_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fmena/.local/lib/python3.5/site-packages/sklearn/datasets/_rcv1.py\u001b[0m in \u001b[0;36mfetch_rcv1\u001b[0;34m(data_home, subset, download_if_missing, random_state, shuffle, return_X_y)\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_refresh_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msamples_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_id_path\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m         \u001b[0;31m# TODO: Revert to the following two lines in v0.23\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;31m# X = joblib.load(samples_path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fmena/.local/lib/python3.5/site-packages/sklearn/datasets/_base.py\u001b[0m in \u001b[0;36m_refresh_cache\u001b[0;34m(files, compress)\u001b[0m\n\u001b[1;32m    926\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 928\u001b[0;31m                 \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    929\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             message = (\"This dataset will stop being loadable in scikit-learn \"\n",
      "\u001b[0;32m/home/fmena/.local/lib/python3.5/site-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(value, filename, compress, protocol, cache_size)\u001b[0m\n\u001b[1;32m    500\u001b[0m         with _write_fileobject(filename, compress=(compress_method,\n\u001b[1;32m    501\u001b[0m                                                    compress_level)) as f:\n\u001b[0;32m--> 502\u001b[0;31m             \u001b[0mNumpyPickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_filename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fmena/anaconda3/envs/tesis/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_framing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTOP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_framing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fmena/.local/lib/python3.5/site-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    293\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fmena/anaconda3/envs/tesis/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0;31m# Save the reduce() output and finally memoize the object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpersistent_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fmena/anaconda3/envs/tesis/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36msave_reduce\u001b[0;34m(self, func, args, state, listitems, dictitems, obj)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m             \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m             \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBUILD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fmena/.local/lib/python3.5/site-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    293\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fmena/anaconda3/envs/tesis/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fmena/anaconda3/envs/tesis/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36msave_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_setitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m     \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fmena/anaconda3/envs/tesis/lib/python3.5/pickle.py\u001b[0m in \u001b[0;36m_batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    838\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m                 \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSETITEMS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fmena/.local/lib/python3.5/site-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0;31m# And then array bytes are written right after the wrapper.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m             \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fmena/.local/lib/python3.5/site-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36mwrite_array\u001b[0;34m(self, array, pickler)\u001b[0m\n\u001b[1;32m    102\u001b[0m                                            \u001b[0mbuffersize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffersize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                                            order=self.order):\n\u001b[0;32m--> 104\u001b[0;31m                 \u001b[0mpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_handle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtostring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fmena/.local/lib/python3.5/site-packages/joblib/compressor.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    506\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m             \u001b[0mcompressed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompressed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dat_n = \"RCV1\"\n",
    "\n",
    "from sklearn.datasets import fetch_rcv1\n",
    "rcv1_train = fetch_rcv1(subset='train')\n",
    "rcv1_test = fetch_rcv1(subset='test')\n",
    "X_t = rcv1_train.data\n",
    "y_t = rcv1_train.target\n",
    "X_test = rcv1_test.data\n",
    "y_test = rcv1_test.target\n",
    "labels = rcv1_train.target_names\n",
    "\n",
    "del rcv1_train,rcv1_test\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_t = np.concatenate((X_t,X_test),axis=0)\n",
    "labels_t = np.concatenate((X_t,X_test),axis=0) #labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "labels_t = np.asarray(labels_t)\n",
    "X_t,X_test,labels_t,labels_test  = train_test_split(X_t,labels_t,random_state=20,test_size=0.1)\n",
    "\n",
    "X_train,X_val,labels_train,labels_val  = train_test_split(X_t,labels_t,random_state=20,test_size=0.1)\n",
    "\n",
    "del X_t, labels_t\n",
    "gc.collect()\n",
    "\n",
    "print(\"Vocabulario size = \",X_train.shape[1])\n",
    "print(\"Cantidad de datos Entrenamiento: \",len(labels_train))\n",
    "print(\"Cantidad de datos Validación: \",len(labels_val))\n",
    "print(\"Cantidad de datos Pruebas: \",len(labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TMC\n",
    "---\n",
    "textos relacionados con reportes aeros de tráfico provistos por la NASA y usados en la competencia de text mining SIAM. Cada texto tiene 22 etiquetas no exlucenyetes.\n",
    "\n",
    "\n",
    "|Tipo set|Datos|Label|\n",
    "|---|---|---|\n",
    "|Entrenamiento|21,519|22|\n",
    "|Validación|3,498|22|\n",
    "|Pruebas|3,498|22|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_n = \"TMC\"\n",
    "\n",
    "labels = ['a','b','c','d','e','f','e','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El dataset anterior también es extenso en sus documentos pero no tan extremo como 20 Newsgroup.\n",
    "\n",
    "### SearchSnipet\n",
    "---\n",
    "Dataset de Google search *snippets*-- pequeñas porciones de texto que le dan a usuarios una idea de lo que hay en el sitio web en el buscador de Google. Pertenecientes a 8 clases únicas (dominio). Disponibles a través de http://jwebpro.sourceforge.net/data-web-snippets.tar.gz.\n",
    "\n",
    "\n",
    "|Tipo set|Datos|Label|\n",
    "|---|---|---|\n",
    "|Entrenamiento|10.060|8|\n",
    "|Pruebas|2.280|8|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos de entrenamiento:  10060\n",
      "Datos de pruebas:  2280\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['health',\n",
       " 'culture-arts-entertainment',\n",
       " 'computers',\n",
       " 'engineering',\n",
       " 'politics-society',\n",
       " 'business',\n",
       " 'sports',\n",
       " 'education-science']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat_n = \"Snippets\"\n",
    "\n",
    "def read_file(archivo,symb=' '):\n",
    "    with open(archivo,'r') as f:\n",
    "        lineas = f.readlines()\n",
    "        tokens_f = [linea.strip().split(symb) for linea in lineas]\n",
    "        labels = [tokens[-1] for tokens in tokens_f]\n",
    "        tokens = [' '.join(tokens[:-1]) for tokens in tokens_f]\n",
    "    return labels,tokens\n",
    "labels_t,texts_t = read_file(\"Data/data-web-snippets/train.txt\")\n",
    "labels_test,texts_test = read_file(\"Data/data-web-snippets/test.txt\")\n",
    "print(\"Datos de entrenamiento: \",len(texts_t))\n",
    "print(\"Datos de pruebas: \",len(texts_test))\n",
    "\n",
    "labels = list(set(labels_t))\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de datos Entrenamiento:  9054\n",
      "Cantidad de datos Validación:  1006\n",
      "Cantidad de datos Pruebas:  2280\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "labels_t = np.asarray(labels_t)\n",
    "labels_test = np.asarray(labels_test)\n",
    "texts_train,texts_val,labels_train,labels_val  = train_test_split(texts_t,labels_t,random_state=20,test_size=0.1)\n",
    "\n",
    "print(\"Cantidad de datos Entrenamiento: \",len(texts_train))\n",
    "print(\"Cantidad de datos Validación: \",len(texts_val))\n",
    "print(\"Cantidad de datos Pruebas: \",len(texts_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process\n",
    "---\n",
    "Para obtener la representación de los datos, en primer lugar se normalizan con experimentar dos tipos de pre-procesamiento:\n",
    "\n",
    "1. El primero es el siguiente (Basado en Deep Semantic Hashing)\n",
    "    * Pasar letras a minúsculas\n",
    "    * Eliminar extra espacios (saltos de línea por ejemplo)\n",
    "    * Remover stop words\n",
    "    * Borrar todo lo que no sea letras (eliminar números y puntuaciones)\n",
    "    * Conservar las top $k$ palabras/*tokens* más frecuentes\n",
    "2. El segundo añade lo siguiente (Basado en Semantic Hashing)\n",
    "    * Se realiza un *stemming* (Snowball) -- Lemmatization\n",
    "    * Remover palabras de menos de 3 largo\n",
    "\n",
    "\n",
    "Para la representación vectorial se utiliza lo siguiente:\n",
    "* TF-IDF: \n",
    "$$ w_f(d) \\cdot \\left(1 + log\\left( \\frac{1+n_d}{1+df_w} \\right) \\right)$$\n",
    "* **TF (*term frequency*)**: $$ w_f(d) $$\n",
    "* Binary: $$I(w_f(d) \\neq 0)$$\n",
    "\n",
    "La representación *term frequency* es la que se toma como base, puesto que resulta natural al momento de reconstruir el dato y poder utilizar la función de pérdida *cross entropy*. Ya que para la representación *tf-idf* requeriría una función de pérdida adecuada para variable continua, que podría ser *mse*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.92 s, sys: 19 ms, total: 2.94 s\n",
      "Wall time: 3.23 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9054, 10000)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "#analyzer = TfidfVectorizer(ngram_range=(1, 3)).build_analyzer()\n",
    "tokenizer = TfidfVectorizer().build_tokenizer()\n",
    "stemmer = SnowballStemmer(\"english\") \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\"\"\"Extract features from raw input\"\"\"\n",
    "def preProcess(s): #String processor\n",
    "    return s.lower().strip().strip('-').strip('_')\n",
    "def number_normalize(doc):\n",
    "    results = []\n",
    "    for token in tokenizer(doc):\n",
    "        token_pro = preProcess(token)\n",
    "        if len(token_pro) != 0 and not token_pro[0].isdigit():\n",
    "            results.append(token_pro)\n",
    "    return results\n",
    "def stemmed_words(doc):\n",
    "    results = []\n",
    "    for token in tokenizer(doc):\n",
    "        pre_pro = preProcess(token)\n",
    "        #token_pro = stemmer.stem(pre_pro) #aumenta x10 el tiempo de procesamiento\n",
    "        token_pro = lemmatizer.lemmatize(pre_pro) #so can explain/interpretae -- aumenta x5 el tiempo de proce\n",
    "        if len(token_pro) > 2 and not token_pro[0].isdigit(): #elimina palabra largo menor a 2\n",
    "            results.append(token_pro)\n",
    "    return results\n",
    "\n",
    "def get_transform_representation(mode, analizer,min_count,max_feat):\n",
    "    smooth_idf_b = False\n",
    "    use_idf_b = False\n",
    "    binary_b = False\n",
    "\n",
    "    if mode == 'binary':\n",
    "        binary_b = True\n",
    "    elif mode == 'tf':     \n",
    "        pass #default is tf\n",
    "    elif mode == 'tf-idf':\n",
    "        use_idf_b = True\n",
    "        smooth_idf_b = True #inventa 1 conteo imaginario (como priors)--laplace smoothing\n",
    "    return TfidfVectorizer(stop_words='english',tokenizer=analizer,min_df=min_count, max_df=0.8, max_features=max_feat\n",
    "                                ,binary=binary_b, use_idf=use_idf_b, smooth_idf=smooth_idf_b,norm=None\n",
    "                                  ,ngram_range=(1, 3)) \n",
    "\n",
    "min_count = 1 #default = 1\n",
    "max_feat = 10000 #Best: 10000 -- Hinton (2000)\n",
    "\n",
    "\n",
    "vectorizer = get_transform_representation(\"tf\", stemmed_words,min_count,max_feat)\n",
    "\n",
    "%time vectorizer.fit(texts_train)\n",
    "vectors_train = vectorizer.transform(texts_train)\n",
    "vectors_val = vectorizer.transform(texts_val)\n",
    "vectors_test = vectorizer.transform(texts_test)\n",
    "\n",
    "token2idx = vectorizer.vocabulary_\n",
    "idx2token = {idx:token for token,idx in token2idx.items()}\n",
    "\n",
    "#vectorizer2 = get_transform_representation(\"tf-idf\", stemmed_words,min_count,max_feat)\n",
    "\n",
    "#%time vectorizer2.fit(texts_train)\n",
    "#vectors_train2 = vectorizer2.transform(texts_train)\n",
    "#vectors_val2 = vectorizer2.transform(texts_val)\n",
    "#vectors_test2 = vectorizer2.transform(texts_test)\n",
    "vectors_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la celda anterior se calculan dos representaciones deseables de los datos, *term frequency* y *tf-idf*. A continuación se muestra cómo quedó el vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#todense --get representation\n",
    "X_train = np.asarray(vectors_train.todense())\n",
    "X_val = np.asarray(vectors_val.todense())\n",
    "X_test = np.asarray(vectors_test.todense())\n",
    "\n",
    "#X_train2 = np.asarray(vectors_train2.todense())\n",
    "#X_val2 = np.asarray(vectors_val2.todense())\n",
    "#X_test2 = np.asarray(vectors_test2.todense())\n",
    "\n",
    "del vectors_train,vectors_val,vectors_test#,vectors_train2,vectors_val2,vectors_test2\n",
    "gc.collect()\n",
    "\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAVbklEQVR4nO3dfbBcdX3H8ffHhIA8yIO5Ak0CCdP4kOnQAlcKta2xoibUgemMrcloRQtmbIt90GmFsYMW21HUqdY2ChEprVYQ0dEMDaYO0uITyEUQQyBwCWiuoFkkYuXBPPDtH3uuLpu9u2fvPbt7fud8XjN3snvOb3e/5/zu/eTs7/x2jyICMzNL37NGXYCZmRXDgW5mVhEOdDOzinCgm5lVhAPdzKwi5o/qhRcuXBhLly4d1cubmSXptttueyQixjqtG1mgL126lImJiVG9vJlZkiR9b6Z1HnIxM6sIB7qZWUU40M3MKqJnoEu6QtJOSVt6tHuxpH2SXlNceWZmlleeI/QrgVXdGkiaB1wCbC6gJjMzm4WegR4RNwGP9mj2VuBzwM4iijIzs/7NeQxd0iLgD4BLc7RdJ2lC0kSj0ZjrS5uZWYsiTop+GHhHROzr1TAiNkTEeESMj411nBdvBsBPn9rDF+/4wajLKI29+57mmokdPP20v+66ly0/eIzbv79r4K/zpS0Pc3/jZwN/nX4U8cGiceBqSQALgTMl7Y2ILxTw3FZT77j2Tq7f8kNecMxhvPCY54y6nJG7/GsP8L7r7yEieO2Ljxt1OaX26n/5GgAPvu/3B/o6b/nUtzlh7BC+8vaVA32dfsw50CNi2fRtSVcC1znMba4eeuwpAJ7c3fONXy08+vhuAH7yxJ4RV2KttjceH3UJz9Az0CVdBawEFkqaAt4FHAAQET3Hzc3MbDh6BnpErM37ZBHxxjlVY2Zms+ZPipqZVYQD3cysIhzoZgnxpEXrxoFulgCNugBLggPdLAE+Mrc8HOhmCfGRunXjQDczqwgHuplZRTjQzRLisXTrxoFupeYAa/LYueXhQLdScoCZ9c+BbmZWEQ50M7OKcKCbJcDnEiwPB7pZQnxuwbpxoJslYDrIfaRu3TjQzcwqwoFuZlYRDnQzs4pwoJuZVUTPQJd0haSdkrbMsP51ku7Mfr4h6deLL9Pqxif/zPqX5wj9SmBVl/UPAC+NiBOB9wAbCqjLDPA0vXbh/+msi/m9GkTETZKWdln/jZa7NwOL516WWZPzyyy/osfQzwWun2mlpHWSJiRNNBqNgl/aqsRH5p3JO8a6KCzQJb2MZqC/Y6Y2EbEhIsYjYnxsbKyolzYzM3IMueQh6UTgcmB1RPy4iOc0M7P+zPkIXdJxwOeBP46Ie+dekpmZzUbPI3RJVwErgYWSpoB3AQcARMSlwEXAc4GPqjnAtzcixgdVsJmZdZZnlsvaHuvPA84rrCIz2192MtTTFq0bf1LUzKwiHOhmKfCRueXgQDdLiOehWzcOdDOzinCgm5lVhAPdLAWe5WI5ONCt1BxgZvk50K2UfPLPrH8OdDOzinCgm5lVhAPdzKwiHOhmZhXhQDczqwgHuplZRTjQzcwqwoFuZlYRDnQrJX9C1Kx/DnQrNX9itEl4R1hvDnSzBISvcGE59Ax0SVdI2ilpywzrJekjkiYl3Snp5OLLtLry0ItZfnmO0K8EVnVZvxpYnv2sAz4297Ks7jzUYta/noEeETcBj3ZpcjbwH9F0M3CEpGOLKtDMzPIpYgx9EbCj5f5Utmw/ktZJmpA00Wg0CnhpMzObVkSgd3pz3HHkMyI2RMR4RIyPjY0V8NJm9eBZLpZHEYE+BSxpub8YeKiA5zWzNp7tYt0UEegbgTdks11OAx6LiIcLeF4zM+vD/F4NJF0FrAQWSpoC3gUcABARlwKbgDOBSeAJ4E2DKtbMzGbWM9AjYm2P9QH8eWEVmdl+PNRiefiTomYJ8clR68aBbmZWEQ50swRMH5l76MW6caCbmVWEA93MrCIc6FZyHmIwy8uBbqXkuRxm/XOgm5lVhAPdLCGeh27dONDNEuJpi9aNA93MrCIc6GZmFeFANzOrCAe6lZJHis3650C3kvOsDgB5N1gODnSzhITfulgXDnSzBDjILQ8HullCPPRi3TjQreR8aGqWlwPdSskHomb9yxXoklZJ2iZpUtIFHdYfJ+lGSbdLulPSmcWXalZfHmqxPHoGuqR5wHpgNbACWCtpRVuzvwOuiYiTgDXAR4su1Mx8ctS6y3OEfiowGRHbI2I3cDVwdlubAJ6T3T4ceKi4Es3MLI88gb4I2NFyfypb1urdwOslTQGbgLd2eiJJ6yRNSJpoNBqzKNfMzGaSJ9A7jd61v/FbC1wZEYuBM4FPStrvuSNiQ0SMR8T42NhY/9Wa1ZSHWiyPPIE+BSxpub+Y/YdUzgWuAYiIbwIHAQuLKNDMzPLJE+i3AsslLZO0gOZJz41tbb4PvBxA0otoBrrHVMwKMj3LxbNdrJuegR4Re4Hzgc3A3TRns9wl6WJJZ2XN3g68WdJ3gKuAN0b4TaKZ2TDNz9MoIjbRPNnZuuyilttbgZcUW5qZtfNhknXjT4qamVWEA93MrCIc6FZqHmIwy8+BbqUkT+d4Bu8Ny8OBbpYAv1GxPBzoZmZ9KuusbAe6mVlFONDNzCqi1oH+2JN7eO+mu9mz7+lRl9KXJ3fv4x//aytP7dk36lIGZvot7VXf2tGjZefHfujL9zK164miyzIrtVoH+iVfuofLbtrOxjvS+vr2j391Ox//6gNc8fUHRl3KwH3u21N9P+b+xs/45xvu4y2fum0AFZmVV60Dfffe5pH5vpKe4JjJ9DuKvfvSqntYns52y8/3pPXOqxtPW7Q8ah3oZmazUdZjQAe6WQJKmh9WMg50M7OKcKCbmVWEAx2SfT9b1nG8svDusbqpdaCnOnMg1br7MZcv56ri/qniNlnxah3oZmZV4kA3M+tTWYfzcgW6pFWStkmalHTBDG3+SNJWSXdJ+nSxZZqZWS89LxItaR6wHngFMAXcKmljdmHo6TbLgQuBl0TELknPG1TBZmbWWZ4j9FOByYjYHhG7gauBs9vavBlYHxG7ACJiZ7FlDlaU9g1Ud6nWPSxl/c5qs0HJE+iLgNavvJvKlrV6PvB8SV+XdLOkVZ2eSNI6SROSJhqNxuwqLlCyVzlLtvDh8O6xusoT6J3+PNoPfeYDy4GVwFrgcklH7PegiA0RMR4R42NjY/3WWrhkD+CSLXw4vHusrvIE+hSwpOX+YqD9+2angC9GxJ6IeADYRjPgk6BEZ/mmWvew+ELTNihlHc7LE+i3AsslLZO0AFgDbGxr8wXgZQCSFtIcgtleZKFmZtZdz0CPiL3A+cBm4G7gmoi4S9LFks7Kmm0GfixpK3Aj8DcR8eNBFW1mZvvrOW0RICI2AZvall3UcjuAt2U/yUl1tkiqdQ9LWd8Wmw1KrT8pmuwQa7KFD0cVd08Vt8mKV+tAt/Jyfpn1z4FulgCPHpVLWbvDgW5mVhEOdDOzinCgk+7b2VTrzqOITavw7jHrqNaBnuonLdOsepiqt4c8y8XyqHWgm5lViQPdzKwiah3oqX7SMs2qh6m6e8iffi2HsnZDrQN9Wqrjk6nWPSxV2j1lDRArFwc66f6xpFr3sHj3WN3UOtA9y6WqqreHpt+N+TverZtaB7qZWZU40M3MKsKBbqXkgYXOPMulHMo6Q86BbmZWEQ500p0NkWrdQ+MdZDXjQDczq4haB3qqM8BSrXtYvH+srnIFuqRVkrZJmpR0QZd2r5EUksaLK9HMpnkeunXTM9AlzQPWA6uBFcBaSSs6tDsM+AvglqKLNLMmz3Iph7J2Q54j9FOByYjYHhG7gauBszu0ew/wfuCpAuszM7Oc8gT6ImBHy/2pbNkvSDoJWBIR13V7IknrJE1Immg0Gn0XW7Sy/i/bS6p1D4v3j9VVnkDvNGj3iz8ZSc8CPgS8vdcTRcSGiBiPiPGxsbH8VZqZWU95An0KWNJyfzHwUMv9w4BfA/5H0oPAacDGFE6Mpnp+KdW6h6WK+yfVL5Kz4coT6LcCyyUtk7QAWANsnF4ZEY9FxMKIWBoRS4GbgbMiYmIgFZuZWUc9Az0i9gLnA5uBu4FrIuIuSRdLOmvQBZpZeb87xMplfp5GEbEJ2NS27KIZ2q6ce1lWd1UcNjEbtFp/UtTKyzNVzPrnQCfh8Ei28OHw3rG6qXWgp/q23jMeuqvi3nGfWx61DnQzsypxoJuZVYQD3SwhPm1SDmXtBwe6mVlFONBJ90MbaVY9PP6qWasbB7pZQlKdmWXD4UAn3SlhaVY9PL66j9WNA93MrCIc6GZmFeFAt1LyaElnPs9bDmWdSOFAp7yd00uaVQ+PZ7lY3dQ80NM8DPTRa3dVPBlawU2yAah5oJulwW82LA8HuplZRTjQzRLioRfrxoFulhAPvZRDWfshV6BLWiVpm6RJSRd0WP82SVsl3SnpBknHF1/q4JS1c3pJte5h8e6xuukZ6JLmAeuB1cAKYK2kFW3NbgfGI+JE4Frg/UUXamZm3eU5Qj8VmIyI7RGxG7gaOLu1QUTcGBFPZHdvBhYXW+ZgpDoemWjZQ1PF/ZPq76oNV55AXwTsaLk/lS2bybnA9Z1WSFonaULSRKPRyF/lgKQ6ZJFo2UPj/WN1lSfQOx0bdPybkfR6YBz4QKf1EbEhIsYjYnxsbCx/lQOW6tFPqnUPi3eP1c38HG2mgCUt9xcDD7U3knQG8E7gpRHx82LKMzP45btJv/soh7L2Q54j9FuB5ZKWSVoArAE2tjaQdBJwGXBWROwsvszBSnboJdG6h8W7x+qmZ6BHxF7gfGAzcDdwTUTcJeliSWdlzT4AHAp8VtIdkjbO8HRmNgceRrJu8gy5EBGbgE1tyy5quX1GwXUNRapj0ImWPTRV3D+p/q7acPmTomZmFeFANzOrCAe6mVlFONBJdzZEqldaGpYqzgKq4CYlqaxXw3Kgm5lVRK0DPdWJA57x0J33j9VVrQPdLDX+v8q6caCbmVVErQO9nKc1eivp+ZjS8P6xuqp1oJulxv9XlUNZ+8GBbqUkjxab9a3WgZ5qZHgWR3dV3D8V3CQbgFoHuplZlTjQzRJQ1jFbKxcHuplZRTjQzcz6VNapsQ50KG/v9JBo2UPjLy+zuql1oKc6G0KpFj4kVZzyWL0tskGodaCbmVWJA93MrCJyBbqkVZK2SZqUdEGH9QdK+ky2/hZJS4su1MzMuusZ6JLmAeuB1cAKYK2kFW3NzgV2RcSvAh8CLim6UDMz6069LqUk6XTg3RHxquz+hQAR8d6WNpuzNt+UNB/4ITAWXZ58fHw8JiYm+i74f+9t8A/Xbe37cZ3ct/Nnv7i9/HmHFvKcw5Bq3f2YyzY+uWcfU7uenNVjy6oOfV6U6X01yP20L4Ltjcdn/TqvffESzvudE2b12pJui4jxTuvm53j8ImBHy/0p4DdnahMReyU9BjwXeKStkHXAOoDjjjsuV/HtDj1wPsuPLqajjjn8IL563yOc8aKjWTA/nXkExx11MDfcs5OVLxjj4AXzRl3OQBxx8AHc+uAugFn199SuJzlx8eEsPvLZRZc2EssWHsJ/b/0Rr1xxNPPnpfO7OgoP/eRJHt+9r7CcmMn2xuO88JjDOGHskL4fu/DQAwdQUb5A7/Tb037knacNEbEB2ADNI/Qcr72fU44/klOOP2U2DzUzq7Q8J0WngCUt9xcDD83UJhtyORx4tIgCzcwsnzyBfiuwXNIySQuANcDGtjYbgXOy268BvtJt/NzMzIrXc8glGxM/H9gMzAOuiIi7JF0MTETERuATwCclTdI8Ml8zyKLNzGx/ecbQiYhNwKa2ZRe13H4K+MNiSzMzs374k6JmZhXhQDczqwgHuplZRTjQzcwqoudH/wf2wlID+N4sH76Qtk+h1oC3uR68zfUwl20+PiLGOq0YWaDPhaSJmb7LoKq8zfXgba6HQW2zh1zMzCrCgW5mVhGpBvqGURcwAt7mevA218NAtjnJMXQzM9tfqkfoZmbWxoFuZlYRyQV6rwtWp0LSEkk3Srpb0l2S/jJbfpSkL0u6L/v3yGy5JH0k2+47JZ3c8lznZO3vk3TOTK9ZFpLmSbpd0nXZ/WXZxcXvyy42viBbPuPFxyVdmC3fJulVo9mSfCQdIelaSfdk/X161ftZ0l9nv9dbJF0l6aCq9bOkKyTtlLSlZVlh/SrpFEnfzR7zEUm9L1UVEcn80Pz63vuBE4AFwHeAFaOua5bbcixwcnb7MOBemhfhfj9wQbb8AuCS7PaZwPU0rw51GnBLtvwoYHv275HZ7SNHvX09tv1twKeB67L71wBrstuXAn+a3f4z4NLs9hrgM9ntFVnfHwgsy34n5o16u7ps778D52W3FwBHVLmfaV6S8gHg2S39+8aq9TPwu8DJwJaWZYX1K/At4PTsMdcDq3vWNOqd0ucOPB3Y3HL/QuDCUddV0LZ9EXgFsA04Nlt2LLAtu30ZsLal/bZs/Vrgspblz2hXth+aV7y6Afg94Lrsl/URYH57H9P8Dv7Ts9vzs3Zq7/fWdmX7AZ6ThZvalle2n/nlNYaPyvrtOuBVVexnYGlboBfSr9m6e1qWP6PdTD+pDbl0umD1ohHVUpjsLeZJwC3A0RHxMED27/OyZjNte2r75MPA3wJPZ/efC/wkIvZm91vrf8bFx4Hpi4+ntM0nAA3g37JhpsslHUKF+zkifgB8EPg+8DDNfruNavfztKL6dVF2u315V6kFeq6LUadE0qHA54C/ioifdmvaYVl0WV46kl4N7IyI21oXd2gaPdYls800jzhPBj4WEScBj9N8Kz6T5Lc5Gzc+m+Ywya8AhwCrOzStUj/30u82zmrbUwv0PBesToakA2iG+X9GxOezxT+SdGy2/lhgZ7Z8pm1PaZ+8BDhL0oPA1TSHXT4MHKHmxcXhmfXPdPHxlLZ5CpiKiFuy+9fSDPgq9/MZwAMR0YiIPcDngd+i2v08rah+ncputy/vKrVAz3PB6iRkZ6w/AdwdEf/Usqr1gtvn0Bxbn17+huxs+WnAY9lbus3AKyUdmR0ZvTJbVjoRcWFELI6IpTT77isR8TrgRpoXF4f9t7nTxcc3Amuy2RHLgOU0TyCVTkT8ENgh6QXZopcDW6lwP9McajlN0sHZ7/n0Nle2n1sU0q/Zuv+TdFq2D9/Q8lwzG/VJhVmchDiT5oyQ+4F3jrqeOWzHb9N8C3UncEf2cybNscMbgPuyf4/K2gtYn233d4Hxluf6E2Ay+3nTqLct5/av5JezXE6g+Yc6CXwWODBbflB2fzJbf0LL49+Z7Ytt5Dj7P+Jt/Q1gIuvrL9CczVDpfgb+HrgH2AJ8kuZMlUr1M3AVzXMEe2geUZ9bZL8C49n+ux/4V9pOrHf68Uf/zcwqIrUhFzMzm4ED3cysIhzoZmYV4UA3M6sIB7qZWUU40M3MKsKBbmZWEf8PdmS/wjmLeaMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "##representacion soft para TF ---mucho mejor!\n",
    "X_train_input = np.log(X_train+1) \n",
    "X_val_input = np.log(X_val+1) \n",
    "X_test_input = np.log(X_test+1) \n",
    "plt.plot(X_train_input[0])\n",
    "plt.show()\n",
    "\n",
    "#soft para tf-idf\n",
    "#X_train_input2 = np.log(X_train2+1) \n",
    "#X_val_input2 = np.log(X_val2+1) \n",
    "#X_test_input2 = np.log(X_test2+1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "---\n",
    "La arquitectura para afrontar el problema es una red clásica *Feed Forward* basado en el *baseline* de *Variational Deep Semantic Hashing* (VDSH), la cual es una arquitectura de *autoencoder* no simétrico con dos capas escondidas en el *encoder* y sin capas escondidas en el *decoder*.\n",
    "\n",
    "\n",
    "> Input(|V|) -> Relu(500) -> Relu (500) -> Laten variable(32)-> Sampling -> Softmax(|V|)\n",
    "\n",
    "\n",
    "Tamién se experimenta con una arquitectura de autoencoder simétrico, lo cual asimila de mejor manera lo que es una RBM (red bidireccional) como se realiza en el trabajo de *Semantic Hashing*.\n",
    "\n",
    "La primera experimentación se realiza con 32 *bits* en la representación latente, ya que los trabajos previos han mostrado que esta cantidad de *bits* parece ser lo suficiente antes de empezar a realizar *overfitting*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def REC_loss(x_true, x_pred):\n",
    "    x_pred = K.clip(x_pred, K.epsilon(), 1)\n",
    "    return - K.sum(x_true*K.log(x_pred), axis=-1) #keras.losses.categorical_crossentropy(x_true, x_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def traditional_VAE(data_dim,Nb,units,layers_e,layers_d,opt='adam',BN=True, summ=True, beta=0):\n",
    "    pre_encoder = define_pre_encoder(data_dim, layers=layers_e,units=units,BN=BN)\n",
    "    if summ:\n",
    "        print(\"pre-encoder network:\")\n",
    "        pre_encoder.summary()\n",
    "    generator = define_generator(Nb,data_dim,layers=layers_d,units=units,BN=BN)\n",
    "    if summ:\n",
    "        print(\"generator network:\")\n",
    "        generator.summary()\n",
    "\n",
    "    ## Encoder\n",
    "    x = Input(shape=(data_dim,))\n",
    "    hidden = pre_encoder(x)\n",
    "    z_mean = Dense(Nb,activation='linear', name='z-mean')(hidden)\n",
    "    z_log_var = Dense(Nb,activation='linear',name = 'z-log_var')(hidden)\n",
    "    encoder = Model(x, z_mean) # build a model to project inputs on the latent space\n",
    "\n",
    "    def sampling(args):\n",
    "        epsilon_std = 1.0\n",
    "        z_mean, z_log_var = args\n",
    "        epsilon = K.random_normal(shape=(K.shape(z_mean)[0], Nb),mean=0., stddev=epsilon_std)\n",
    "        return z_mean + K.exp(0.5*z_log_var) * epsilon #+sigma (desvest)\n",
    "    \n",
    "    ## Decoder\n",
    "    z_sampled = Lambda(sampling, output_shape=(Nb,), name='sampled')([z_mean, z_log_var])\n",
    "    output = generator(z_sampled)\n",
    "        \n",
    "    Recon_loss = REC_loss\n",
    "    kl_loss = KL_loss(z_mean,z_log_var)\n",
    "    def VAE_loss(y_true, y_pred): \n",
    "        return Recon_loss(y_true, y_pred) + beta*kl_loss(y_true, y_pred)\n",
    "\n",
    "    traditional_vae = Model(x, output)\n",
    "    traditional_vae.compile(optimizer=opt, loss=VAE_loss, metrics = [Recon_loss,kl_loss])\n",
    "    \n",
    "    return traditional_vae, encoder,generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "def sample_gumbel(shape,eps=K.epsilon()):\n",
    "    \"\"\"Inverse Sample function from Gumbel(0, 1)\"\"\"\n",
    "    U = K.random_uniform(shape, 0, 1)\n",
    "    return K.log(U + eps)- K.log(1-U + eps)\n",
    "\n",
    "\n",
    "def binary_VAE(data_dim,Nb,units,layers_e,layers_d,opt='adam',BN=True, summ=True,tau_ann=False,beta=0):\n",
    "    if tau_ann:\n",
    "        tau = K.variable(1.0, name=\"temperature\") \n",
    "    else:\n",
    "        tau = K.variable(0.67, name=\"temperature\") #o tau fijo en 0.67=2/3\n",
    "    \n",
    "    pre_encoder = define_pre_encoder(data_dim, layers=layers_e,units=units,BN=BN)\n",
    "    if summ:\n",
    "        print(\"pre-encoder network:\")\n",
    "        pre_encoder.summary()\n",
    "    generator = define_generator(Nb,data_dim,layers=layers_d,units=units,BN=BN)\n",
    "    if summ:\n",
    "        print(\"generator network:\")\n",
    "        generator.summary()\n",
    "\n",
    "    x = Input(shape=(data_dim,))\n",
    "    hidden = pre_encoder(x)\n",
    "    logits_b  = Dense(Nb, activation='linear', name='logits-b')(hidden) #log(B_j/1-B_j)\n",
    "    #proba = np.exp(logits_b)/(1+np.exp(logits_b)) = sigmoidal(logits_b) <<<<<<<<<< recupera probabilidad\n",
    "    #dist = Dense(Nb, activation='sigmoid')(hidden) #p(b) #otra forma de modelarlo\n",
    "    encoder = Model(x, logits_b)\n",
    "\n",
    "    def sampling(logits_b):\n",
    "        #logits_b = K.log(aux/(1-aux) + K.epsilon() )\n",
    "        b = logits_b + sample_gumbel(K.shape(logits_b)) # logits + gumbel noise\n",
    "        return keras.activations.sigmoid( b/tau )\n",
    "\n",
    "    b_sampled = Lambda(sampling, output_shape=(Nb,), name='sampled')(logits_b)\n",
    "    output = generator(b_sampled)\n",
    "        \n",
    "    Recon_loss = REC_loss\n",
    "    kl_loss = BKL_loss(logits_b)\n",
    "    def BVAE_loss(y_true, y_pred): \n",
    "        return Recon_loss(y_true, y_pred) + beta*kl_loss(y_true, y_pred)\n",
    "\n",
    "    binary_vae = Model(x, output)\n",
    "    binary_vae.compile(optimizer=opt, loss=BVAE_loss, metrics = [Recon_loss,kl_loss])\n",
    "    if tau_ann:\n",
    "        return binary_vae, encoder,generator ,tau\n",
    "    else:\n",
    "        return binary_vae, encoder,generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train details\n",
    "---\n",
    "\n",
    "* 30* epochs* \n",
    "* *batch size* de 100\n",
    "* optimizador Adam\n",
    "* Inicializador de Glorot (para los pesos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import  compare_hist_train, add_hist_plot\n",
    "\n",
    "batch_size = 100\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de datos Entrenamiento:  21286\n",
      "Cantidad de datos Validación:  3498\n",
      "Cantidad de datos Pruebas:  3498\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([array(['a', 'b'], dtype='<U1'),\n",
       "       array(['e', 'i', 'k', 'r'], dtype='<U1'),\n",
       "       array(['f', 'g', 'l'], dtype='<U1'), ...,\n",
       "       array(['i', 'l', 'q'], dtype='<U1'), array(['b'], dtype='<U1'),\n",
       "       array(['b', 'd', 'i', 'q', 'r'], dtype='<U1')], dtype=object)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from utils_VDSH import Load_Dataset\n",
    "from utils import Load_Dataset\n",
    "\n",
    "#https://github.com/unsuthee/VariationalDeepSemanticHashing\n",
    "## load data used for VDSH - for sake comparison\n",
    "\n",
    "\n",
    "#filename = 'Data/ng20.tfidf.mat'\n",
    "#filename = 'Data/reuters.tfidf.mat'\n",
    "filename = 'Data/tmc.tfidf.mat'\n",
    "data = Load_Dataset(filename)\n",
    "\n",
    "X_train_input = data[\"train\"]\n",
    "X_train = X_train_input \n",
    "X_val_input = data[\"cv\"]\n",
    "X_val = X_val_input \n",
    "X_test_input = data[\"test\"]\n",
    "X_test = X_test_input\n",
    "\n",
    "\n",
    "if \"ng20\" in filename:\n",
    "    labels_train = np.asarray([labels[value.argmax(axis=-1)] for value in data[\"gnd_train\"]])\n",
    "    labels_val = np.asarray([labels[value.argmax(axis=-1)] for value in data[\"gnd_cv\"]])\n",
    "    labels_test = np.asarray([labels[value.argmax(axis=-1)] for value in data[\"gnd_test\"]])\n",
    "elif \"reuters\" in filename or \"tmc\" in filename:\n",
    "    labels = np.asarray(labels)\n",
    "    labels_train = np.asarray([labels[value.astype(bool)] for value in data[\"gnd_train\"]])\n",
    "    labels_val = np.asarray([labels[value.astype(bool)] for value in data[\"gnd_cv\"]])\n",
    "    labels_test = np.asarray([labels[value.astype(bool)] for value in data[\"gnd_test\"]])\n",
    "\n",
    "print(\"Cantidad de datos Entrenamiento: \",len(labels_train))\n",
    "print(\"Cantidad de datos Validación: \",len(labels_val))\n",
    "print(\"Cantidad de datos Pruebas: \",len(labels_test))\n",
    "labels_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTANTE EJECUTAR, nueva modificación!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output target normalizado en dataset  Snippets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in true_divide\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in true_divide\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "#outputs as probabolities -- normalized over datasets..\n",
    "X_train = X_train/X_train.sum(axis=-1,keepdims=True) \n",
    "X_val = X_val/X_val.sum(axis=-1,keepdims=True)\n",
    "X_test = X_test/X_test.sum(axis=-1,keepdims=True)\n",
    "print(\"Output target normalizado en dataset \",dat_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[np.isnan(X_train)] = 0\n",
    "X_val[np.isnan(X_val)] = 0\n",
    "X_test[np.isnan(X_test)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import find_beta\n",
    "\n",
    "def create_model_T(beta_V):\n",
    "    return traditional_VAE(X_train.shape[1],Nb=32,units=500,layers_e=2,layers_d=0\n",
    "                                                                  ,beta=beta_V, summ=False)\n",
    "def create_model_B(beta_V):\n",
    "    return binary_VAE(X_train.shape[1],Nb=32,units=500,layers_e=2,layers_d=0\n",
    "                                                                  ,beta=beta_V, summ=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************************\n",
      "*********** SUMMARY RESULTS ***********\n",
      "***************************************\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beta</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.1295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.1145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.1211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.1493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.2820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.5243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.5240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.5182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.5076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.5174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.5128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000488</td>\n",
       "      <td>0.5170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000244</td>\n",
       "      <td>0.5157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.5206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.5230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.5231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.5277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.5097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.5331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.5252</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        beta   score\n",
       "0   1.000000  0.1295\n",
       "1   0.500000  0.1145\n",
       "2   0.250000  0.1211\n",
       "3   0.125000  0.1493\n",
       "4   0.062500  0.2820\n",
       "5   0.031250  0.5243\n",
       "6   0.015625  0.5240\n",
       "7   0.007812  0.5182\n",
       "8   0.003906  0.5076\n",
       "9   0.001953  0.5174\n",
       "10  0.000977  0.5128\n",
       "11  0.000488  0.5170\n",
       "12  0.000244  0.5157\n",
       "13  0.000122  0.5206\n",
       "14  0.000061  0.5230\n",
       "15  0.000031  0.5231\n",
       "16  0.000015  0.5277\n",
       "17  0.000008  0.5097\n",
       "18  0.000004  0.5331\n",
       "19  0.000002  0.5252"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best value is 0.5331 with beta 0.000004\n",
      "Worst value is 0.1145 with beta 0.500000\n",
      "***************************************\n"
     ]
    }
   ],
   "source": [
    "beta_T = find_beta(create_model_T, X_train_input, X_train, X_val_input,labels_train,labels_val,\n",
    "                   binary=False, E=50)\n",
    "beta_B = find_beta(create_model_B, X_train_input, X_train, X_val_input,labels_train,labels_val,\n",
    "                   binary=True, E=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### valores para 20NEWS!!\n",
    "\n",
    "beta_T = 0.06250\n",
    "beta_B = 0.015625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "### valores para REUTERS!!\n",
    "beta_T = 0.06250\n",
    "beta_B = 0.000008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "### valores para TMC!!\n",
    "beta_T = 0.06250\n",
    "beta_B = 0.000244"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "### valores para SNIPPETS!!\n",
    "beta_T = 0.125000\n",
    "beta_B = 0.015625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_total_input = np.concatenate((X_train_input,X_val_input),axis=0)\n",
    "X_total = np.concatenate((X_train,X_val),axis=0)\n",
    "labels_total = np.concatenate((labels_train,labels_val),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-encoder network:\n",
      "Model: \"pre-encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 500)               5000500   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 5,255,000\n",
      "Trainable params: 5,253,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "Model: \"generator/decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 10000)             330000    \n",
      "=================================================================\n",
      "Total params: 330,000\n",
      "Trainable params: 330,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 10060 samples, validate on 2280 samples\n",
      "Epoch 1/30\n",
      "10060/10060 [==============================] - 3s 268us/step - loss: 11.1090 - REC_loss: 9.1749 - KL: 15.4732 - val_loss: 9.6777 - val_REC_loss: 9.1307 - val_KL: 4.3760\n",
      "Epoch 2/30\n",
      "10060/10060 [==============================] - 2s 192us/step - loss: 9.4281 - REC_loss: 9.1074 - KL: 2.5660 - val_loss: 9.2569 - val_REC_loss: 9.0654 - val_KL: 1.5320\n",
      "Epoch 3/30\n",
      "10060/10060 [==============================] - 2s 190us/step - loss: 9.1781 - REC_loss: 9.0378 - KL: 1.1221 - val_loss: 9.1102 - val_REC_loss: 8.9964 - val_KL: 0.9099\n",
      "Epoch 4/30\n",
      "10060/10060 [==============================] - 2s 192us/step - loss: 9.0592 - REC_loss: 8.9553 - KL: 0.8314 - val_loss: 8.9984 - val_REC_loss: 8.9009 - val_KL: 0.7795\n",
      "Epoch 5/30\n",
      "10060/10060 [==============================] - 2s 190us/step - loss: 8.9508 - REC_loss: 8.8383 - KL: 0.9001 - val_loss: 8.8938 - val_REC_loss: 8.7594 - val_KL: 1.0751\n",
      "Epoch 6/30\n",
      "10060/10060 [==============================] - 2s 188us/step - loss: 8.8242 - REC_loss: 8.6636 - KL: 1.2852 - val_loss: 8.7761 - val_REC_loss: 8.5906 - val_KL: 1.4846\n",
      "Epoch 7/30\n",
      "10060/10060 [==============================] - 2s 189us/step - loss: 8.6883 - REC_loss: 8.4765 - KL: 1.6945 - val_loss: 8.6481 - val_REC_loss: 8.4193 - val_KL: 1.8306\n",
      "Epoch 8/30\n",
      "10060/10060 [==============================] - 2s 187us/step - loss: 8.5926 - REC_loss: 8.3557 - KL: 1.8947 - val_loss: 8.5832 - val_REC_loss: 8.3605 - val_KL: 1.7822\n",
      "Epoch 9/30\n",
      "10060/10060 [==============================] - 2s 190us/step - loss: 8.5276 - REC_loss: 8.2899 - KL: 1.9017 - val_loss: 8.5448 - val_REC_loss: 8.3140 - val_KL: 1.8467\n",
      "Epoch 10/30\n",
      "10060/10060 [==============================] - 2s 187us/step - loss: 8.4840 - REC_loss: 8.2457 - KL: 1.9064 - val_loss: 8.5125 - val_REC_loss: 8.3117 - val_KL: 1.6071\n",
      "Epoch 11/30\n",
      "10060/10060 [==============================] - 2s 188us/step - loss: 8.4387 - REC_loss: 8.2047 - KL: 1.8723 - val_loss: 8.4697 - val_REC_loss: 8.2520 - val_KL: 1.7417\n",
      "Epoch 12/30\n",
      "10060/10060 [==============================] - 2s 189us/step - loss: 8.4114 - REC_loss: 8.1716 - KL: 1.9179 - val_loss: 8.4691 - val_REC_loss: 8.2572 - val_KL: 1.6948\n",
      "Epoch 13/30\n",
      "10060/10060 [==============================] - 2s 188us/step - loss: 8.3822 - REC_loss: 8.1418 - KL: 1.9233 - val_loss: 8.4350 - val_REC_loss: 8.2368 - val_KL: 1.5860\n",
      "Epoch 14/30\n",
      "10060/10060 [==============================] - 2s 189us/step - loss: 8.3627 - REC_loss: 8.1123 - KL: 2.0033 - val_loss: 8.4091 - val_REC_loss: 8.2038 - val_KL: 1.6424\n",
      "Epoch 15/30\n",
      "10060/10060 [==============================] - 2s 189us/step - loss: 8.3337 - REC_loss: 8.0713 - KL: 2.0991 - val_loss: 8.3895 - val_REC_loss: 8.1831 - val_KL: 1.6516\n",
      "Epoch 16/30\n",
      "10060/10060 [==============================] - 2s 190us/step - loss: 8.2995 - REC_loss: 8.0266 - KL: 2.1831 - val_loss: 8.3794 - val_REC_loss: 8.1375 - val_KL: 1.9350\n",
      "Epoch 17/30\n",
      "10060/10060 [==============================] - 2s 189us/step - loss: 8.2715 - REC_loss: 7.9798 - KL: 2.3335 - val_loss: 8.3740 - val_REC_loss: 8.1432 - val_KL: 1.8460\n",
      "Epoch 18/30\n",
      "10060/10060 [==============================] - 2s 189us/step - loss: 8.2457 - REC_loss: 7.9341 - KL: 2.4933 - val_loss: 8.3546 - val_REC_loss: 8.1108 - val_KL: 1.9504\n",
      "Epoch 19/30\n",
      "10060/10060 [==============================] - 2s 189us/step - loss: 8.2188 - REC_loss: 7.8861 - KL: 2.6620 - val_loss: 8.3222 - val_REC_loss: 8.0652 - val_KL: 2.0560\n",
      "Epoch 20/30\n",
      "10060/10060 [==============================] - 2s 188us/step - loss: 8.1736 - REC_loss: 7.8186 - KL: 2.8396 - val_loss: 8.2964 - val_REC_loss: 8.0248 - val_KL: 2.1724\n",
      "Epoch 21/30\n",
      "10060/10060 [==============================] - 2s 189us/step - loss: 8.1400 - REC_loss: 7.7598 - KL: 3.0414 - val_loss: 8.2812 - val_REC_loss: 8.0218 - val_KL: 2.0757\n",
      "Epoch 22/30\n",
      "10060/10060 [==============================] - 2s 189us/step - loss: 8.1019 - REC_loss: 7.6933 - KL: 3.2685 - val_loss: 8.2868 - val_REC_loss: 8.0053 - val_KL: 2.2515\n",
      "Epoch 23/30\n",
      "10060/10060 [==============================] - 2s 189us/step - loss: 8.0659 - REC_loss: 7.6300 - KL: 3.4876 - val_loss: 8.2557 - val_REC_loss: 7.9693 - val_KL: 2.2910\n",
      "Epoch 24/30\n",
      "10060/10060 [==============================] - 2s 189us/step - loss: 8.0247 - REC_loss: 7.5625 - KL: 3.6973 - val_loss: 8.2247 - val_REC_loss: 7.9290 - val_KL: 2.3656\n",
      "Epoch 25/30\n",
      "10060/10060 [==============================] - 2s 190us/step - loss: 7.9888 - REC_loss: 7.5002 - KL: 3.9086 - val_loss: 8.2123 - val_REC_loss: 7.8952 - val_KL: 2.5363\n",
      "Epoch 26/30\n",
      "10060/10060 [==============================] - 2s 189us/step - loss: 7.9545 - REC_loss: 7.4377 - KL: 4.1348 - val_loss: 8.2054 - val_REC_loss: 7.8800 - val_KL: 2.6034\n",
      "Epoch 27/30\n",
      "10060/10060 [==============================] - 2s 190us/step - loss: 7.9138 - REC_loss: 7.3773 - KL: 4.2915 - val_loss: 8.1727 - val_REC_loss: 7.8158 - val_KL: 2.8547\n",
      "Epoch 28/30\n",
      "10060/10060 [==============================] - 2s 190us/step - loss: 7.8809 - REC_loss: 7.3169 - KL: 4.5121 - val_loss: 8.1569 - val_REC_loss: 7.8075 - val_KL: 2.7955\n",
      "Epoch 29/30\n",
      "10060/10060 [==============================] - 2s 189us/step - loss: 7.8437 - REC_loss: 7.2543 - KL: 4.7149 - val_loss: 8.1579 - val_REC_loss: 7.7975 - val_KL: 2.8833\n",
      "Epoch 30/30\n",
      "10060/10060 [==============================] - 2s 189us/step - loss: 7.7971 - REC_loss: 7.1934 - KL: 4.8300 - val_loss: 8.1362 - val_REC_loss: 7.7461 - val_KL: 3.1207\n"
     ]
    }
   ],
   "source": [
    "traditional_vae,encoder_Tvae,generator_Tvae = traditional_VAE(X_train.shape[1],Nb=32,units=500,layers_e=2,layers_d=0,beta=beta_T)\n",
    "\n",
    "hist1 = traditional_vae.fit(X_total_input, X_total, epochs=epochs, batch_size=batch_size\n",
    "                           ,validation_data=(X_test_input,X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-encoder network:\n",
      "Model: \"pre-encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 500)               5000500   \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 5,255,000\n",
      "Trainable params: 5,253,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "Model: \"generator/decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 10000)             330000    \n",
      "=================================================================\n",
      "Total params: 330,000\n",
      "Trainable params: 330,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 10060 samples, validate on 2280 samples\n",
      "Epoch 1/30\n",
      "10060/10060 [==============================] - 3s 268us/step - loss: 8.4021 - REC_loss: 8.6820 - KL: -17.9109 - val_loss: 7.9799 - val_REC_loss: 8.1870 - val_KL: -13.2544\n",
      "Epoch 2/30\n",
      "10060/10060 [==============================] - 2s 186us/step - loss: 7.8638 - REC_loss: 8.1167 - KL: -16.1875 - val_loss: 7.8397 - val_REC_loss: 8.1115 - val_KL: -17.3908\n",
      "Epoch 3/30\n",
      "10060/10060 [==============================] - 2s 185us/step - loss: 7.7753 - REC_loss: 8.0689 - KL: -18.7899 - val_loss: 7.8078 - val_REC_loss: 8.1088 - val_KL: -19.2621\n",
      "Epoch 4/30\n",
      "10060/10060 [==============================] - 2s 185us/step - loss: 7.7462 - REC_loss: 8.0491 - KL: -19.3857 - val_loss: 7.7884 - val_REC_loss: 8.0985 - val_KL: -19.8515\n",
      "Epoch 5/30\n",
      "10060/10060 [==============================] - 2s 184us/step - loss: 7.7268 - REC_loss: 8.0284 - KL: -19.3013 - val_loss: 7.7751 - val_REC_loss: 8.0851 - val_KL: -19.8405\n",
      "Epoch 6/30\n",
      "10060/10060 [==============================] - 2s 185us/step - loss: 7.7047 - REC_loss: 7.9984 - KL: -18.7985 - val_loss: 7.7575 - val_REC_loss: 8.0637 - val_KL: -19.5982\n",
      "Epoch 7/30\n",
      "10060/10060 [==============================] - 2s 184us/step - loss: 7.6764 - REC_loss: 7.9577 - KL: -18.0045 - val_loss: 7.7339 - val_REC_loss: 8.0303 - val_KL: -18.9704\n",
      "Epoch 8/30\n",
      "10060/10060 [==============================] - 2s 186us/step - loss: 7.6402 - REC_loss: 7.9013 - KL: -16.7052 - val_loss: 7.7036 - val_REC_loss: 7.9829 - val_KL: -17.8732\n",
      "Epoch 9/30\n",
      "10060/10060 [==============================] - 2s 184us/step - loss: 7.5871 - REC_loss: 7.8188 - KL: -14.8330 - val_loss: 7.6560 - val_REC_loss: 7.9097 - val_KL: -16.2359\n",
      "Epoch 10/30\n",
      "10060/10060 [==============================] - 2s 186us/step - loss: 7.5130 - REC_loss: 7.7132 - KL: -12.8139 - val_loss: 7.6052 - val_REC_loss: 7.8299 - val_KL: -14.3778\n",
      "Epoch 11/30\n",
      "10060/10060 [==============================] - 2s 184us/step - loss: 7.4245 - REC_loss: 7.5946 - KL: -10.8851 - val_loss: 7.5384 - val_REC_loss: 7.7335 - val_KL: -12.4882\n",
      "Epoch 12/30\n",
      "10060/10060 [==============================] - 2s 183us/step - loss: 7.3233 - REC_loss: 7.4680 - KL: -9.2620 - val_loss: 7.4682 - val_REC_loss: 7.6406 - val_KL: -11.0332\n",
      "Epoch 13/30\n",
      "10060/10060 [==============================] - 2s 184us/step - loss: 7.2186 - REC_loss: 7.3428 - KL: -7.9476 - val_loss: 7.3971 - val_REC_loss: 7.5477 - val_KL: -9.6396\n",
      "Epoch 14/30\n",
      "10060/10060 [==============================] - 2s 185us/step - loss: 7.1110 - REC_loss: 7.2188 - KL: -6.8991 - val_loss: 7.3256 - val_REC_loss: 7.4610 - val_KL: -8.6690\n",
      "Epoch 15/30\n",
      "10060/10060 [==============================] - 2s 184us/step - loss: 7.0044 - REC_loss: 7.0988 - KL: -6.0400 - val_loss: 7.2562 - val_REC_loss: 7.3792 - val_KL: -7.8751\n",
      "Epoch 16/30\n",
      "10060/10060 [==============================] - 2s 184us/step - loss: 6.9011 - REC_loss: 6.9848 - KL: -5.3570 - val_loss: 7.1856 - val_REC_loss: 7.2995 - val_KL: -7.2890\n",
      "Epoch 17/30\n",
      "10060/10060 [==============================] - 2s 185us/step - loss: 6.7969 - REC_loss: 6.8712 - KL: -4.7581 - val_loss: 7.1220 - val_REC_loss: 7.2287 - val_KL: -6.8269\n",
      "Epoch 18/30\n",
      "10060/10060 [==============================] - 2s 183us/step - loss: 6.6983 - REC_loss: 6.7653 - KL: -4.2871 - val_loss: 7.0579 - val_REC_loss: 7.1571 - val_KL: -6.3506\n",
      "Epoch 19/30\n",
      "10060/10060 [==============================] - 2s 186us/step - loss: 6.6006 - REC_loss: 6.6610 - KL: -3.8643 - val_loss: 6.9976 - val_REC_loss: 7.0912 - val_KL: -5.9885\n",
      "Epoch 20/30\n",
      "10060/10060 [==============================] - 2s 186us/step - loss: 6.5077 - REC_loss: 6.5626 - KL: -3.5117 - val_loss: 6.9446 - val_REC_loss: 7.0339 - val_KL: -5.7156\n",
      "Epoch 21/30\n",
      "10060/10060 [==============================] - 2s 185us/step - loss: 6.4187 - REC_loss: 6.4687 - KL: -3.1978 - val_loss: 6.8860 - val_REC_loss: 6.9709 - val_KL: -5.4363\n",
      "Epoch 22/30\n",
      "10060/10060 [==============================] - 2s 185us/step - loss: 6.3306 - REC_loss: 6.3764 - KL: -2.9287 - val_loss: 6.8318 - val_REC_loss: 6.9135 - val_KL: -5.2259\n",
      "Epoch 23/30\n",
      "10060/10060 [==============================] - 2s 186us/step - loss: 6.2481 - REC_loss: 6.2902 - KL: -2.6944 - val_loss: 6.7830 - val_REC_loss: 6.8620 - val_KL: -5.0534\n",
      "Epoch 24/30\n",
      "10060/10060 [==============================] - 2s 183us/step - loss: 6.1666 - REC_loss: 6.2056 - KL: -2.4972 - val_loss: 6.7353 - val_REC_loss: 6.8116 - val_KL: -4.8840\n",
      "Epoch 25/30\n",
      "10060/10060 [==============================] - 2s 185us/step - loss: 6.0901 - REC_loss: 6.1264 - KL: -2.3222 - val_loss: 6.6864 - val_REC_loss: 6.7598 - val_KL: -4.7006\n",
      "Epoch 26/30\n",
      "10060/10060 [==============================] - 2s 187us/step - loss: 6.0166 - REC_loss: 6.0503 - KL: -2.1594 - val_loss: 6.6493 - val_REC_loss: 6.7208 - val_KL: -4.5768\n",
      "Epoch 27/30\n",
      "10060/10060 [==============================] - 2s 185us/step - loss: 5.9464 - REC_loss: 5.9778 - KL: -2.0132 - val_loss: 6.6012 - val_REC_loss: 6.6708 - val_KL: -4.4559\n",
      "Epoch 28/30\n",
      "10060/10060 [==============================] - 2s 183us/step - loss: 5.8793 - REC_loss: 5.9086 - KL: -1.8744 - val_loss: 6.5653 - val_REC_loss: 6.6331 - val_KL: -4.3361\n",
      "Epoch 29/30\n",
      "10060/10060 [==============================] - 2s 185us/step - loss: 5.8143 - REC_loss: 5.8420 - KL: -1.7737 - val_loss: 6.5276 - val_REC_loss: 6.5947 - val_KL: -4.2913\n",
      "Epoch 30/30\n",
      "10060/10060 [==============================] - 2s 186us/step - loss: 5.7530 - REC_loss: 5.7791 - KL: -1.6764 - val_loss: 6.4881 - val_REC_loss: 6.5535 - val_KL: -4.1885\n"
     ]
    }
   ],
   "source": [
    "binary_vae,encoder_Bvae,generator_Bvae= binary_VAE(X_train.shape[1],Nb=32,units=500,layers_e=2,layers_d=0,beta=beta_B)\n",
    "\n",
    "hist2 = binary_vae.fit(X_total_input, X_total, epochs=epochs, batch_size=batch_size\n",
    "                           ,validation_data=(X_test_input,X_test) )\n",
    "                       #,callbacks=[Tau_Call(tau)] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## kl increase va a depender netamente del problema, que tan rapido aprende el modelo y como vaariar eso.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another intrinsic measure: *Classification*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#function to define and train model\n",
    "from sklearn.metrics import jaccard_score\n",
    "from utils import define_fit, visualize_probas, visualize_mean, calculate_hash, visualize_probas_byB\n",
    "\n",
    "results = []\n",
    "results_S = []\n",
    "results_B = []\n",
    "results_O_B = [] #original testing on tresholded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#codify input data (binarize -- or aprox)\n",
    "X_train_logits = encoder_Bvae.predict(X_total_input)\n",
    "X_val_logits = encoder_Bvae.predict(X_test_input)\n",
    "\n",
    "#probabilities\n",
    "X_train_Bcode = expit(X_train_logits)\n",
    "X_val_Bcode = expit(X_val_logits)\n",
    "\n",
    "#Z-mean\n",
    "X_train_Tcode = encoder_Tvae.predict(X_total_input)\n",
    "X_val_Tcode = encoder_Tvae.predict(X_test_input)\n",
    "\n",
    "##codify labels\n",
    "labels_aux = np.asarray(labels)\n",
    "def codify_labels(inputs):\n",
    "    matrix_labels = np.zeros((inputs.shape[0],labels_aux.shape[0]))\n",
    "    for i,aux_labels in enumerate(inputs):\n",
    "        if type(aux_labels) == list or type(aux_labels) == np.ndarray :\n",
    "            for aux_label in aux_labels:\n",
    "                idx = np.where(aux_label==labels_aux)[0]\n",
    "                matrix_labels[i,idx] = 1 #various-multiple\n",
    "        else:\n",
    "            idx = np.where(aux_labels==labels_aux)[0]\n",
    "            matrix_labels[i,idx] = 1 #only one\n",
    "    return matrix_labels\n",
    "\n",
    "C_train = codify_labels(labels_total)\n",
    "C_val = codify_labels(labels_test)\n",
    "C_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "aux = [dat_n]\n",
    "multi_label = \"Reuters\" in aux or \"TMC\" in aux\n",
    "\n",
    "model1_O = define_fit(multi_label,X_train_Bcode,C_train)\n",
    "model2_O = define_fit(multi_label,X_train_Tcode,C_train)\n",
    "model3 = define_fit(multi_label,X_total_input,C_train)\n",
    "\n",
    "if not multi_label:\n",
    "    aux.append(model3.evaluate(X_total_input,C_train,verbose=0)[1])\n",
    "    aux.append(model3.evaluate(X_test_input,C_val,verbose=0)[1])\n",
    "\n",
    "    aux.append(model2_O.evaluate(X_train_Tcode,C_train,verbose=0)[1])\n",
    "    aux.append(model2_O.evaluate(X_val_Tcode,C_val,verbose=0)[1])\n",
    "\n",
    "    aux.append(model1_O.evaluate(X_train_Bcode,C_train,verbose=0)[1])\n",
    "    aux.append(model1_O.evaluate(X_val_Bcode,C_val,verbose=0)[1])\n",
    "else:\n",
    "    aux.append(jaccard_score(C_train, (model3.predict(X_total_input)>=0.5)*1, average='micro'))\n",
    "    aux.append(jaccard_score(C_val, (model3.predict(X_test_input)>=0.5)*1, average='micro'))\n",
    "\n",
    "    aux.append(jaccard_score(C_train, (model2_O.predict(X_train_Tcode)>=0.5)*1, average='micro'))\n",
    "    aux.append(jaccard_score(C_val, (model2_O.predict(X_val_Tcode)>=0.5)*1, average='micro'))\n",
    "\n",
    "    aux.append(jaccard_score(C_train, (model1_O.predict(X_train_Bcode)>=0.5)*1, average='micro'))\n",
    "    aux.append(jaccard_score(C_val, (model1_O.predict(X_val_Bcode)>=0.5)*1, average='micro'))\n",
    "    \n",
    "results.append(aux)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on dataset:  20News\n",
      "Raw Input (train-val): 0.999523 - 0.906761\n",
      "Traditional VAE (train-val): 0.849350 - 0.784351\n",
      "Binary VAE (train-val): 0.826057 - 0.768266\n",
      "\n",
      "Accuracy on dataset:  Reuters\n",
      "Raw Input (train-val): 0.990956 - 0.851508\n",
      "Traditional VAE (train-val): 0.797300 - 0.786197\n",
      "Binary VAE (train-val): 0.748453 - 0.728665\n",
      "\n",
      "Accuracy on dataset:  TMC\n",
      "Raw Input (train-val): 0.971480 - 0.430312\n",
      "Traditional VAE (train-val): 0.529945 - 0.458556\n",
      "Binary VAE (train-val): 0.532579 - 0.453345\n",
      "\n",
      "Accuracy on dataset:  Snippets\n",
      "Raw Input (train-val): 0.998907 - 0.698246\n",
      "Traditional VAE (train-val): 0.898907 - 0.701754\n",
      "Binary VAE (train-val): 0.891948 - 0.673246\n"
     ]
    }
   ],
   "source": [
    "for valores in results:\n",
    "    print(\"\\nAccuracy on dataset: \",valores[0])\n",
    "    print(\"Raw Input (train-val): %f - %f\"%(valores[1],valores[2]))\n",
    "    print(\"Traditional VAE (train-val): %f - %f\"%(valores[3],valores[4]))\n",
    "    print(\"Binary VAE (train-val): %f - %f\"%(valores[5],valores[6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora es mejor para reconstruir ya que el modelo VAE al concentrarse mas en la KL (peso mas alto) olvida mejorar la reconstrucción y los patrones de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, ..., 0, 1, 1],\n",
       "       [1, 1, 1, ..., 0, 0, 1],\n",
       "       [1, 0, 0, ..., 1, 1, 1],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       [1, 0, 0, ..., 1, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## binary // treshold it\n",
    "\n",
    "X_train_Tcode_B = calculate_hash(X_train_Tcode, from_probas=False)\n",
    "X_val_Tcode_B = calculate_hash(X_val_Tcode, from_probas=False)\n",
    "\n",
    "X_train_Bcode_B = calculate_hash(X_train_Bcode, from_probas=True, from_logits=False)\n",
    "X_val_Bcode_B = calculate_hash(X_val_Bcode, from_probas=True, from_logits=False)\n",
    "\n",
    "X_train_Tcode_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "### trained models over encoder representation testing on tresholded representation (see how decrease)\n",
    "\n",
    "aux = [dat_n]\n",
    "multi_label = \"Reuters\" in aux or \"TMC\" in aux\n",
    "if not multi_label:\n",
    "    aux.append(model2_O.evaluate(X_train_Tcode_B,C_train,verbose=0)[1])\n",
    "    aux.append(model2_O.evaluate(X_val_Tcode_B,C_val,verbose=0)[1])\n",
    "\n",
    "    aux.append(model1_O.evaluate(X_train_Bcode_B,C_train,verbose=0)[1])\n",
    "    aux.append(model1_O.evaluate(X_val_Bcode_B,C_val,verbose=0)[1])\n",
    "else:\n",
    "    aux.append(jaccard_score(C_train, (model2_O.predict(X_train_Tcode_B)>=0.5)*1, average='micro'))\n",
    "    aux.append(jaccard_score(C_val, (model2_O.predict(X_val_Tcode_B)>=0.5)*1, average='micro'))\n",
    "\n",
    "    aux.append(jaccard_score(C_train, (model1_O.predict(X_train_Bcode_B)>=0.5)*1, average='micro'))\n",
    "    aux.append(jaccard_score(C_val, (model1_O.predict(X_val_Bcode_B)>=0.5)*1, average='micro'))\n",
    "    \n",
    "results_O_B.append(aux)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification on Binary (thresholded) representation\n",
      "\n",
      "Accuracy on dataset:  20News\n",
      "Traditional VAE (train-val): 0.421440 - 0.404308\n",
      "Binary VAE (train-val): 0.785398 - 0.730916\n",
      "\n",
      "Accuracy on dataset:  Reuters\n",
      "Traditional VAE (train-val): 0.467864 - 0.462428\n",
      "Binary VAE (train-val): 0.746070 - 0.723512\n",
      "\n",
      "Accuracy on dataset:  TMC\n",
      "Traditional VAE (train-val): 0.318780 - 0.271784\n",
      "Binary VAE (train-val): 0.529306 - 0.445516\n",
      "\n",
      "Accuracy on dataset:  Snippets\n",
      "Traditional VAE (train-val): 0.504871 - 0.407018\n",
      "Binary VAE (train-val): 0.874950 - 0.647807\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification on Binary (thresholded) representation\")\n",
    "for valores in results_O_B:\n",
    "    print(\"\\nAccuracy on dataset: \",valores[0])\n",
    "    print(\"Traditional VAE (train-val): %f - %f\"%(valores[1],valores[2]))\n",
    "    print(\"Binary VAE (train-val): %f - %f\"%(valores[3],valores[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "aux = [dat_n]\n",
    "multi_label = \"Reuters\" in aux or \"TMC\" in aux\n",
    "\n",
    "model1 = define_fit(multi_label,X_train_Bcode_B,C_train)\n",
    "model2 = define_fit(multi_label,X_train_Tcode_B,C_train)\n",
    "\n",
    "if not multi_label:\n",
    "    aux.append(model2.evaluate(X_train_Tcode_B,C_train,verbose=0)[1])\n",
    "    aux.append(model2.evaluate(X_val_Tcode_B,C_val,verbose=0)[1])\n",
    "\n",
    "    aux.append(model1.evaluate(X_train_Bcode_B,C_train,verbose=0)[1])\n",
    "    aux.append(model1.evaluate(X_val_Bcode_B,C_val,verbose=0)[1])\n",
    "else:\n",
    "    aux.append(jaccard_score(C_train, (model2.predict(X_train_Tcode_B)>=0.5)*1, average='micro'))\n",
    "    aux.append(jaccard_score(C_val, (model2.predict(X_val_Tcode_B)>=0.5)*1, average='micro'))\n",
    "\n",
    "    aux.append(jaccard_score(C_train, (model1.predict(X_train_Bcode_B)>=0.5)*1, average='micro'))\n",
    "    aux.append(jaccard_score(C_val, (model1.predict(X_val_Bcode_B)>=0.5)*1, average='micro'))\n",
    "    \n",
    "results_B.append(aux)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification on Binary (thresholded) representation\n",
      "\n",
      "Accuracy on dataset:  20News\n",
      "Traditional VAE (train-val): 0.749574 - 0.711559\n",
      "Binary VAE (train-val): 0.796636 - 0.735823\n",
      "\n",
      "Accuracy on dataset:  Reuters\n",
      "Traditional VAE (train-val): 0.718972 - 0.709104\n",
      "Binary VAE (train-val): 0.741730 - 0.719477\n",
      "\n",
      "Accuracy on dataset:  TMC\n",
      "Traditional VAE (train-val): 0.498211 - 0.431998\n",
      "Binary VAE (train-val): 0.528525 - 0.444090\n",
      "\n",
      "Accuracy on dataset:  Snippets\n",
      "Traditional VAE (train-val): 0.825547 - 0.612281\n",
      "Binary VAE (train-val): 0.880119 - 0.648246\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification on Binary (thresholded) representation\")\n",
    "for valores in results_B:\n",
    "    print(\"\\nAccuracy on dataset: \",valores[0])\n",
    "    print(\"Traditional VAE (train-val): %f - %f\"%(valores[1],valores[2]))\n",
    "    print(\"Binary VAE (train-val): %f - %f\"%(valores[3],valores[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "---\n",
    "#### Proceso de evaluación (*content-based retrieval*)\n",
    "> *Query*: **Documento**\n",
    "\n",
    "1. Calcular código hashing/binario de cada dato/documento\n",
    "    * Para *VAE* tradicional se utiliza la mediana de cada componente como *treshold*\n",
    "    * Para *VAE* binario se utiliza el *treshold* de 0.5 en la probabilidad\n",
    "2. Recuperar documentos basado en *match* perfecto, *top K* o *ball search* de distancia *hamming* de un documento consulta.\n",
    "    * En el mismo conjunto/*set*\n",
    "    * Con *query* recupero sobre *database*\n",
    "3. Medir *precision* y *recall* en base a algún criterio de relevancia.\n",
    "    * Documento relevante a otro o no -> Si comparten al menos una clase.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import MedianHashing, get_similar, measure_metrics, calculate_hash\n",
    "from utils import MAP_atk, M_P_atk, AP_atk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentando variando el #Bits\n",
    "---\n",
    "A continación se realizan las experimentaciones correspondientes para encontrar la mejor arquitectura y mejor configuración del modelo propuesto en base a las métricas *precision* y *recall* del conjunto de validación en el **top 100** de elementos recuperados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_hashing_DE(train_hash,test_hash,labels_trainn,labels_testt,tipo=\"topK\",eval_tipo='PRatk',K=100):\n",
    "    \"\"\"\n",
    "        Evaluate Hashing correclty: Query and retrieve on a different set\n",
    "    \"\"\"\n",
    "    test_similares_train =  get_similar(test_hash,train_hash,tipo=tipo,K=K)\n",
    "    if eval_tipo==\"MAP\":\n",
    "        return MAP_atk(test_similares_train,labels_query=labels_testt, labels_source=labels_trainn, K=0) #all ranking\n",
    "    elif eval_tipo == \"PRatk\":\n",
    "        return measure_metrics(labels,test_similares_train,labels_testt,labels_source=labels_trainn)\n",
    "    elif eval_tipo == \"Patk\":\n",
    "        return M_P_atk(test_similares_train, labels_query=labels_testt, labels_source=labels_trainn, K=K)\n",
    "\n",
    "def hash_data(model, x_train, x_test, binary=True):\n",
    "    encode_train = model.predict(x_train)\n",
    "    encode_test = model.predict(x_test)\n",
    "    \n",
    "    train_hash = calculate_hash(encode_train, from_probas=binary )\n",
    "    test_hash = calculate_hash(encode_test, from_probas = binary)\n",
    "    return train_hash, test_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "traditional_dat = {\"20news\":{\"p\":[],\"r\":[]},\n",
    "                   \"snippets\":{\"p\":[],\"r\":[]},\n",
    "                   \"reuters\":{\"p\":[],\"r\":[]},\n",
    "                  \"tmc\":{\"p\":[],\"r\":[]}}\n",
    "binary_dat = {\"20news\":{\"p\":[],\"r\":[]},\n",
    "              \"snippets\":{\"p\":[],\"r\":[]},\n",
    "              \"reuters\":{\"p\":[],\"r\":[]},\n",
    "             \"tmc\":{\"p\":[],\"r\":[]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "snippets\n",
      "pre-encoder network:\n",
      "Model: \"pre-encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_17 (Dense)             (None, 500)               5000500   \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 5,255,000\n",
      "Trainable params: 5,253,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "Model: \"generator/decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_19 (Dense)             (None, 10000)             50000     \n",
      "=================================================================\n",
      "Total params: 50,000\n",
      "Trainable params: 50,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "pre-encoder network:\n",
      "Model: \"pre-encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_20 (Dense)             (None, 500)               5000500   \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 5,255,000\n",
      "Trainable params: 5,253,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "Model: \"generator/decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_22 (Dense)             (None, 10000)             50000     \n",
      "=================================================================\n",
      "Total params: 50,000\n",
      "Trainable params: 50,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "pre-encoder network:\n",
      "Model: \"pre-encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 500)               5000500   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 5,255,000\n",
      "Trainable params: 5,253,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "Model: \"generator/decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 10000)             90000     \n",
      "=================================================================\n",
      "Total params: 90,000\n",
      "Trainable params: 90,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "pre-encoder network:\n",
      "Model: \"pre-encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 500)               5000500   \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 5,255,000\n",
      "Trainable params: 5,253,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "Model: \"generator/decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 10000)             90000     \n",
      "=================================================================\n",
      "Total params: 90,000\n",
      "Trainable params: 90,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "pre-encoder network:\n",
      "Model: \"pre-encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 500)               5000500   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 5,255,000\n",
      "Trainable params: 5,253,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "Model: \"generator/decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 10000)             170000    \n",
      "=================================================================\n",
      "Total params: 170,000\n",
      "Trainable params: 170,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "pre-encoder network:\n",
      "Model: \"pre-encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 500)               5000500   \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 5,255,000\n",
      "Trainable params: 5,253,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "Model: \"generator/decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 10000)             170000    \n",
      "=================================================================\n",
      "Total params: 170,000\n",
      "Trainable params: 170,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-encoder network:\n",
      "Model: \"pre-encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 500)               5000500   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 5,255,000\n",
      "Trainable params: 5,253,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "Model: \"generator/decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 10000)             330000    \n",
      "=================================================================\n",
      "Total params: 330,000\n",
      "Trainable params: 330,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "pre-encoder network:\n",
      "Model: \"pre-encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 500)               5000500   \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 5,255,000\n",
      "Trainable params: 5,253,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "Model: \"generator/decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 10000)             330000    \n",
      "=================================================================\n",
      "Total params: 330,000\n",
      "Trainable params: 330,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "pre-encoder network:\n",
      "Model: \"pre-encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 500)               5000500   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 5,255,000\n",
      "Trainable params: 5,253,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "Model: \"generator/decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 10000)             650000    \n",
      "=================================================================\n",
      "Total params: 650,000\n",
      "Trainable params: 650,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "pre-encoder network:\n",
      "Model: \"pre-encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 500)               5000500   \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 5,255,000\n",
      "Trainable params: 5,253,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "Model: \"generator/decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 10000)             650000    \n",
      "=================================================================\n",
      "Total params: 650,000\n",
      "Trainable params: 650,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Nbits = np.asarray([4,8,16,32,64])\n",
    "dataset = dat_n.lower()\n",
    "print(dataset)\n",
    "\n",
    "for Nbit in Nbits:\n",
    "    traditional_vae,encoder_Tvae,generator_Tvae = traditional_VAE(X_train.shape[1],Nb=Nbit,units=500,layers_e=2,layers_d=0,beta=beta_T)\n",
    "    traditional_vae.fit(X_total_input, X_total, epochs=epochs, batch_size=batch_size,verbose=0)\n",
    "    total_hash_VAE, test_hash_VAE = hash_data(encoder_Tvae,X_total_input,X_test_input, binary=False)\n",
    "\n",
    "    p_t,r_t = evaluate_hashing_DE(total_hash_VAE,test_hash_VAE,labels_total,labels_test,tipo=\"topK\")\n",
    "    traditional_dat[dataset][\"p\"].append(p_t) \n",
    "    traditional_dat[dataset][\"r\"].append(r_t) \n",
    "    \n",
    "    binary_vae,encoder_Bvae,generator_Bvae = binary_VAE(X_train.shape[1],Nb=Nbit,units=500,layers_e=2,layers_d=0,beta=beta_B)\n",
    "    binary_vae.fit(X_total_input, X_total, epochs=epochs, batch_size=batch_size,verbose=0 )\n",
    "    total_hash_BVAE, test_hash_BVAE = hash_data(encoder_Bvae,X_total_input,X_test_input)\n",
    "\n",
    "    p_b,r_b = evaluate_hashing_DE(total_hash_BVAE, test_hash_BVAE,labels_total,labels_test,tipo=\"topK\")\n",
    "    binary_dat[dataset][\"p\"].append(p_b) \n",
    "    binary_dat[dataset][\"r\"].append(r_b) \n",
    "    keras.backend.clear_session()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de Precision en validación\n",
      "\n",
      "*** VAE Traditional***\n",
      "                 0         1          2          3          4\n",
      "N bits    4.000000  8.000000  16.000000  32.000000  64.000000\n",
      "20News    0.190144  0.360177   0.478850   0.536148   0.582287\n",
      "Reuters   0.509959  0.681324   0.730910   0.772596   0.799586\n",
      "Snippets  0.205728  0.313623   0.421509   0.479193   0.474447\n",
      "TMC       0.581810  0.650026   0.682196   0.702690   0.722281\n",
      "\n",
      "*** VAE Binary***\n",
      "                 0         1          2          3          4\n",
      "N bits    4.000000  8.000000  16.000000  32.000000  64.000000\n",
      "20News    0.218582  0.353732   0.551379   0.563806   0.540613\n",
      "Reuters   0.455346  0.638511   0.761768   0.803289   0.787260\n",
      "Snippets  0.156048  0.344667   0.482996   0.517167   0.502075\n",
      "TMC       0.543859  0.636349   0.701964   0.723608   0.704914\n",
      "\n",
      "Resultados de Recall en validación\n",
      "\n",
      "*** VAE Traditional***\n",
      "                 0         1          2          3          4\n",
      "N bits    4.000000  8.000000  16.000000  32.000000  64.000000\n",
      "20News    0.025465  0.048278   0.064016   0.071784   0.078025\n",
      "Reuters   0.028231  0.041966   0.053692   0.058279   0.062556\n",
      "Snippets  0.016766  0.024594   0.034459   0.038746   0.038503\n",
      "TMC       0.006776  0.009850   0.011816   0.012548   0.013327\n",
      "\n",
      "*** VAE Binary***\n",
      "                 0         1          2          3          4\n",
      "N bits    4.000000  8.000000  16.000000  32.000000  64.000000\n",
      "20News    0.029299  0.046951   0.073669   0.075519   0.072632\n",
      "Reuters   0.021122  0.036027   0.049211   0.063105   0.066648\n",
      "Snippets  0.011977  0.028601   0.040383   0.044898   0.045673\n",
      "TMC       0.005649  0.008790   0.012554   0.014280   0.013917\n"
     ]
    }
   ],
   "source": [
    "print(\"Resultados de Precision en validación\")\n",
    "t = pd.DataFrame() #Table()\n",
    "t[\"N bits\"] = Nbits\n",
    "t[\"20News\"] = traditional_dat[\"20news\"][\"p\"]\n",
    "t[\"Reuters\"] = traditional_dat[\"reuters\"][\"p\"]\n",
    "t[\"Snippets\"] = traditional_dat[\"snippets\"][\"p\"]\n",
    "t[\"TMC\"] = traditional_dat[\"tmc\"][\"p\"]\n",
    "print(\"\\n*** VAE Traditional***\")\n",
    "print(t.T)\n",
    "\n",
    "t = pd.DataFrame() #Table()\n",
    "t[\"N bits\"] = Nbits\n",
    "t[\"20News\"] = binary_dat[\"20news\"][\"p\"]\n",
    "t[\"Reuters\"] = binary_dat[\"reuters\"][\"p\"]\n",
    "t[\"Snippets\"] = binary_dat[\"snippets\"][\"p\"]\n",
    "t[\"TMC\"] = binary_dat[\"tmc\"][\"p\"]\n",
    "print(\"\\n*** VAE Binary***\")\n",
    "print(t.T)\n",
    "\n",
    "print(\"\\nResultados de Recall en validación\")\n",
    "t = pd.DataFrame() #Table()\n",
    "t[\"N bits\"] = Nbits\n",
    "t[\"20News\"] = traditional_dat[\"20news\"][\"r\"]\n",
    "t[\"Reuters\"] = traditional_dat[\"reuters\"][\"r\"]\n",
    "t[\"Snippets\"] = traditional_dat[\"snippets\"][\"r\"]\n",
    "t[\"TMC\"] = traditional_dat[\"tmc\"][\"r\"]\n",
    "print(\"\\n*** VAE Traditional***\")\n",
    "print(t.T)\n",
    "\n",
    "t = pd.DataFrame() #Table()\n",
    "t[\"N bits\"] = Nbits\n",
    "t[\"20News\"] = binary_dat[\"20news\"][\"r\"]\n",
    "t[\"Reuters\"] = binary_dat[\"reuters\"][\"r\"]\n",
    "t[\"Snippets\"] = binary_dat[\"snippets\"][\"r\"]\n",
    "t[\"TMC\"] = binary_dat[\"tmc\"][\"r\"]\n",
    "print(\"\\n*** VAE Binary***\")\n",
    "print(t.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_map = {}\n",
    "results_p5000 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-encoder network:\n",
      "Model: \"pre-encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 500)               5000500   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 5,255,000\n",
      "Trainable params: 5,253,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "Model: \"generator/decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 10000)             50000     \n",
      "=================================================================\n",
      "Total params: 50,000\n",
      "Trainable params: 50,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "pre-encoder network:\n",
      "Model: \"pre-encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 500)               5000500   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 5,255,000\n",
      "Trainable params: 5,253,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "Model: \"generator/decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 10000)             50000     \n",
      "=================================================================\n",
      "Total params: 50,000\n",
      "Trainable params: 50,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "pre-encoder network:\n",
      "Model: \"pre-encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 500)               5000500   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 5,255,000\n",
      "Trainable params: 5,253,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "Model: \"generator/decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 10000)             90000     \n",
      "=================================================================\n",
      "Total params: 90,000\n",
      "Trainable params: 90,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "pre-encoder network:\n",
      "Model: \"pre-encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 500)               5000500   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 5,255,000\n",
      "Trainable params: 5,253,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "Model: \"generator/decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 10000)             90000     \n",
      "=================================================================\n",
      "Total params: 90,000\n",
      "Trainable params: 90,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "pre-encoder network:\n",
      "Model: \"pre-encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 500)               5000500   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 5,255,000\n",
      "Trainable params: 5,253,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "Model: \"generator/decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 10000)             170000    \n",
      "=================================================================\n",
      "Total params: 170,000\n",
      "Trainable params: 170,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "pre-encoder network:\n",
      "Model: \"pre-encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 500)               5000500   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 5,255,000\n",
      "Trainable params: 5,253,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "Model: \"generator/decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 10000)             170000    \n",
      "=================================================================\n",
      "Total params: 170,000\n",
      "Trainable params: 170,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-encoder network:\n",
      "Model: \"pre-encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 500)               5000500   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 5,255,000\n",
      "Trainable params: 5,253,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "Model: \"generator/decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 10000)             330000    \n",
      "=================================================================\n",
      "Total params: 330,000\n",
      "Trainable params: 330,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "pre-encoder network:\n",
      "Model: \"pre-encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 500)               5000500   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 5,255,000\n",
      "Trainable params: 5,253,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "Model: \"generator/decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 10000)             330000    \n",
      "=================================================================\n",
      "Total params: 330,000\n",
      "Trainable params: 330,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "pre-encoder network:\n",
      "Model: \"pre-encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 500)               5000500   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 5,255,000\n",
      "Trainable params: 5,253,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "Model: \"generator/decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 10000)             650000    \n",
      "=================================================================\n",
      "Total params: 650,000\n",
      "Trainable params: 650,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "pre-encoder network:\n",
      "Model: \"pre-encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 500)               5000500   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 5,255,000\n",
      "Trainable params: 5,253,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "Model: \"generator/decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 10000)             650000    \n",
      "=================================================================\n",
      "Total params: 650,000\n",
      "Trainable params: 650,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Nbits = np.asarray([4,8,16,32,64])\n",
    "dataset = dat_n.lower()\n",
    "\n",
    "results_map[dataset] = {\"vdsh\":[], \"bvae\":[]}\n",
    "results_p5000[dataset] = {\"vdsh\":[], \"bvae\":[]}\n",
    "\n",
    "for Nbit in Nbits:\n",
    "    traditional_vae,encoder_Tvae,generator_Tvae = traditional_VAE(X_total_input.shape[1],Nb=Nbit,units=500,layers_e=2,layers_d=0,beta=beta_T)\n",
    "    traditional_vae.fit(X_total_input, X_total, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    total_hash_VAE, test_hash_VAE = hash_data(encoder_Tvae,X_total_input,X_test_input, binary=False)\n",
    "    \n",
    "    map_t= evaluate_hashing_DE(total_hash_VAE,test_hash_VAE,labels_total,labels_test,eval_tipo=\"MAP\",K=9999999)\n",
    "    results_map[dataset][\"vdsh\"].append(map_t) \n",
    "    p5k_t = evaluate_hashing_DE(total_hash_VAE,test_hash_VAE,labels_total,labels_test,eval_tipo=\"Patk\",K=5000)\n",
    "    results_p5000[dataset][\"vdsh\"].append(p5k_t) \n",
    "    del traditional_vae, encoder_Tvae, generator_Tvae, total_hash_VAE, test_hash_VAE\n",
    "    keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "    \n",
    "    binary_vae,encoder_Bvae,generator_Bvae= binary_VAE(X_total_input.shape[1],Nb=Nbit,units=500,layers_e=2,layers_d=0,beta=beta_B)\n",
    "    binary_vae.fit(X_total_input, X_total, epochs=epochs, batch_size=batch_size, verbose=0 )\n",
    "    total_hash_BVAE, test_hash_BVAE = hash_data(encoder_Bvae,X_total_input,X_test_input)\n",
    "\n",
    "    map_b= evaluate_hashing_DE(total_hash_BVAE, test_hash_BVAE,labels_total,labels_test,eval_tipo=\"MAP\",K=9999999)\n",
    "    results_map[dataset][\"bvae\"].append(map_b) \n",
    "    p5k_b = evaluate_hashing_DE(total_hash_BVAE, test_hash_BVAE,labels_total,labels_test,eval_tipo=\"Patk\",K=5000)\n",
    "    results_p5000[dataset][\"bvae\"].append(p5k_b) \n",
    "    del binary_vae, encoder_Bvae, generator_Bvae, total_hash_BVAE, test_hash_BVAE\n",
    "    keras.backend.clear_session()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados de MAP baseline VDSH\n",
      "                 0         1          2          3          4\n",
      "N bits    4.000000  8.000000  16.000000  32.000000  64.000000\n",
      "20News    0.155396  0.232945   0.305911   0.380244   0.420678\n",
      "Reuters   0.444329  0.575396   0.527764   0.610194   0.638079\n",
      "TMC       0.517761  0.525473   0.542988   0.558376   0.587390\n",
      "Snippets  0.176747  0.243381   0.280135   0.300114   0.335936\n",
      "\n",
      "Resultados de MAP proposal B-VAE\n",
      "                 0         1          2          3          4\n",
      "N bits    4.000000  8.000000  16.000000  32.000000  64.000000\n",
      "20News    0.213947  0.277323   0.392858   0.371965   0.313554\n",
      "Reuters   0.424511  0.508233   0.618551   0.589735   0.536851\n",
      "TMC       0.513850  0.539820   0.559803   0.556420   0.529839\n",
      "Snippets  0.157278  0.271857   0.305895   0.329780   0.297100\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nResultados de MAP baseline VDSH\")\n",
    "t = pd.DataFrame() #Table()\n",
    "t[\"N bits\"] = Nbits\n",
    "t[\"20News\"] = results_map[\"20news\"][\"vdsh\"]\n",
    "t[\"Reuters\"] = results_map[\"reuters\"][\"vdsh\"]\n",
    "t[\"TMC\"] = results_map[\"tmc\"][\"vdsh\"]\n",
    "t[\"Snippets\"] = results_map[\"snippets\"][\"vdsh\"]\n",
    "print(t.T)\n",
    "\n",
    "print(\"\\nResultados de MAP proposal B-VAE\")\n",
    "t = pd.DataFrame() #Table()\n",
    "t[\"N bits\"] = Nbits\n",
    "t[\"20News\"] = results_map[\"20news\"][\"bvae\"]\n",
    "t[\"Reuters\"] = results_map[\"reuters\"][\"bvae\"]\n",
    "t[\"TMC\"] = results_map[\"tmc\"][\"bvae\"]\n",
    "t[\"Snippets\"] = results_map[\"snippets\"][\"bvae\"]\n",
    "print(t.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados de P@5000 baseline VDSH\n",
      "                 0         1          2          3          4\n",
      "N bits    4.000000  8.000000  16.000000  32.000000  64.000000\n",
      "20News    0.103188  0.112450   0.119118   0.127257   0.132508\n",
      "Reuters   0.332040  0.362683   0.348024   0.368383   0.376327\n",
      "TMC       0.541001  0.537547   0.550605   0.567184   0.591222\n",
      "Snippets  0.158501  0.182261   0.186041   0.189241   0.196206\n",
      "\n",
      "Resultados de MAP proposal B-VAE\n",
      "                 0         1          2          3          4\n",
      "N bits    4.000000  8.000000  16.000000  32.000000  64.000000\n",
      "20News    0.120017  0.126814   0.128579   0.124503   0.117453\n",
      "Reuters   0.327887  0.348460   0.359632   0.343994   0.326402\n",
      "TMC       0.529515  0.552976   0.568279   0.559600   0.533851\n",
      "Snippets  0.145197  0.182262   0.189499   0.194021   0.188258\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nResultados de P@5000 baseline VDSH\")\n",
    "t = pd.DataFrame() #Table()\n",
    "t[\"N bits\"] = Nbits\n",
    "t[\"20News\"] = results_p5000[\"20news\"][\"vdsh\"]\n",
    "t[\"Reuters\"] = results_p5000[\"reuters\"][\"vdsh\"]\n",
    "t[\"TMC\"] = results_p5000[\"tmc\"][\"vdsh\"]\n",
    "t[\"Snippets\"] = results_p5000[\"snippets\"][\"vdsh\"]\n",
    "print(t.T)\n",
    "\n",
    "print(\"\\nResultados de MAP proposal B-VAE\")\n",
    "t = pd.DataFrame() #Table()\n",
    "t[\"N bits\"] = Nbits\n",
    "t[\"20News\"] = results_p5000[\"20news\"][\"bvae\"]\n",
    "t[\"Reuters\"] = results_p5000[\"reuters\"][\"bvae\"]\n",
    "t[\"TMC\"] = results_p5000[\"tmc\"][\"bvae\"]\n",
    "t[\"Snippets\"] = results_p5000[\"snippets\"][\"bvae\"]\n",
    "print(t.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:newpy3_tf1]",
   "language": "python",
   "name": "conda-env-newpy3_tf1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
