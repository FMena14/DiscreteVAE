{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"20%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "<h1 align='center'> Binary Variational Semantic Hashing </h1>\n",
    "\n",
    "<H3 align='center'> Extensión trabajo CIARP </H3>\n",
    "<hr style=\"height:2px;border:none\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras,gc, os, time, sys\n",
    "from keras.layers import *\n",
    "from keras.models import Sequential,Model\n",
    "from keras import backend as K\n",
    "from astropy.table import Table\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from scipy.special import expit\n",
    "\n",
    "from base_networks import *\n",
    "from utils import check_availability, load_imgs_mask\n",
    "\n",
    "from utils import get_topK_labels,set_newlabel_list, enmask_data\n",
    "\n",
    "np.random.seed(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access Data/VGG_128: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!ls Data/VGG_128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "### MNIST\n",
    "---\n",
    "\n",
    "Imágenes en blanco y negro de 28x28 píxeles, de números del 0 al 9, que dan orígen a las 10 clases del problema.\n",
    "\n",
    "|Tipo set|Datos|Label|\n",
    "|---|---|---|\n",
    "|Entrenamiento|60000|10|\n",
    "|Pruebas|10000|10|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_dat = \"MNIST-raw\"\n",
    "\n",
    "(X_t, aux_t), (X_test, aux_test) = keras.datasets.mnist.load_data()\n",
    "labels = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
    "labels_t = np.asarray([labels[value] for value in aux_t])\n",
    "labels_test = np.asarray([labels[value] for value in aux_test])\n",
    "labels_t = np.concatenate((labels_t,labels_test),axis=0)\n",
    "\n",
    "X_t = np.concatenate([X_t, X_test], axis=0).reshape(len(labels_t),28*28)\n",
    "X_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 512)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_dat = \"MNIST\"\n",
    "\n",
    "(_, aux_t), (_, aux_test) = keras.datasets.mnist.load_data()\n",
    "labels = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
    "labels_t = np.asarray([labels[value] for value in aux_t])\n",
    "labels_test = np.asarray([labels[value] for value in aux_test])\n",
    "labels_t = np.concatenate((labels_t,labels_test),axis=0)\n",
    "\n",
    "X_t = np.load(\"../AUX/VGG_128/mnist_VGG_avg.npy\")\n",
    "\n",
    "X_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de datos Entrenamiento:  69000\n",
      "Cantidad de datos Pruebas:  1000\n"
     ]
    }
   ],
   "source": [
    "##### 1000 imagenes de prueba... \n",
    "from utils import sample_test_mask\n",
    "mask_train = sample_test_mask(labels_t, N=100, multi_label=False)\n",
    "\n",
    "## creat test como dicen...\n",
    "X_test = X_t[~mask_train]\n",
    "X_t = X_t[mask_train]\n",
    "labels_test = enmask_data(labels_t, ~mask_train)\n",
    "labels_t = enmask_data(labels_t, mask_train)\n",
    "\n",
    "gc.collect()\n",
    "print(\"Cantidad de datos Entrenamiento: \",len(X_t))\n",
    "print(\"Cantidad de datos Pruebas: \",len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### CIFAR-10\n",
    "---\n",
    "Imágenes RGB pequeñas de  32x32 píxeles, de fotos naturales de distintos objetos.\n",
    "\n",
    "|Tipo set|Datos|Label|\n",
    "|---|---|---|\n",
    "|Entrenamiento|50000|10|\n",
    "|Pruebas|10000|10|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 512)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_dat = \"CIFAR-10\"\n",
    "\n",
    "(_, aux_t), (_, aux_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "labels = [\"airplane\", \"automobile\",\"bird\", \"cat\",\"deer\",\"dog\",\"frog\",\"horse\",\"ship\",\"truck\"]\n",
    "labels_t = np.asarray([labels[value[0]] for value in aux_t])\n",
    "labels_test = np.asarray([labels[value[0]] for value in aux_test])\n",
    "labels_t = np.concatenate((labels_t,labels_test),axis=0)\n",
    "\n",
    "\n",
    "X_t = np.load(\"../AUX/VGG_224/cifar10_VGG_avg.npy\") #mejora\n",
    "\n",
    "X_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de datos Entrenamiento:  59000\n",
      "Cantidad de datos Pruebas:  1000\n"
     ]
    }
   ],
   "source": [
    "##### 1000 imagenes de prueba... \n",
    "from utils import sample_test_mask\n",
    "mask_train = sample_test_mask(labels_t, N=100, multi_label=False)\n",
    "\n",
    "## creat test como dicen...\n",
    "X_test = X_t[~mask_train]\n",
    "X_t = X_t[mask_train]\n",
    "\n",
    "labels_test = enmask_data(labels_t, ~mask_train)\n",
    "labels_t = enmask_data(labels_t, mask_train)\n",
    "\n",
    "gc.collect()\n",
    "print(\"Cantidad de datos Entrenamiento: \",len(X_t))\n",
    "print(\"Cantidad de datos Pruebas: \",len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NUSWIDE\n",
    "---\n",
    "* Cantidad de datos totales: 269648\n",
    "* Datos utlizados (con imagenes disponibles a descargar): 169500\n",
    "* Datos top-21 clases: 158383\n",
    "\n",
    "Imágenes de eventos con 81 tópicos asociados, re-dimensionadas a 64x64.\n",
    "\n",
    "|Tipo set|Datos|Label|\n",
    "|---|---|---|\n",
    "|Entrenamiento|xxx|81|\n",
    "|Validación|xxx|81|\n",
    "|Pruebas|xxx|81|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de labels:  81\n",
      "Cantidad de objetos:  169500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['person'], ['person'], ['person'], ['person'], ['person']]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_dat = \"Nus-Wide\"\n",
    "\n",
    "\n",
    "mask_av = np.loadtxt(\"./Data/\"+name_dat+\"_mask_avail.txt\").astype(bool)\n",
    "\n",
    "folder = \"../AUX2/\" #\"../Dataset_NUSWIDE/\"\n",
    "labels = pd.read_csv(folder+'Concepts81.txt',header=None).values.reshape(1,-1)[0]\n",
    "print(\"Cantidad de labels: \",len(labels) )\n",
    "\n",
    "labels_t = [[] for _ in range(269648)]\n",
    "for concept in labels:\n",
    "    aux = pd.read_csv(folder+\"Groundtruth/AllLabels/Labels_\"+concept+\".txt\",header=None)\n",
    "    indexs_true = aux.loc[(aux==1).values[:,0]].index\n",
    "    \n",
    "    for value in indexs_true:\n",
    "        labels_t[value].append(concept)\n",
    "        \n",
    "labels_t = enmask_data(labels_t, mask_av)\n",
    "N_total = len(labels_t)\n",
    "print(\"Cantidad de objetos: \",N_total )\n",
    "\n",
    "labels_t[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get top-K labels data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category with most data (sky) has = 61066, the top-K category (mountain) has = 4232\n",
      "Cantidad de objetos:  158383\n"
     ]
    }
   ],
   "source": [
    "new_labels = get_topK_labels(labels_t, labels, K=21)\n",
    "\n",
    "labels_t = set_newlabel_list(new_labels, labels_t)\n",
    "labels = new_labels\n",
    "# y si quedan datos sin clase?\n",
    "mask_used_t = np.asarray(list(map(len,labels_t))) != 0\n",
    "\n",
    "labels_t = enmask_data(labels_t, mask_used_t)\n",
    "print(\"Cantidad de objetos: \", len(labels_t) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N_used = 80*1000 #o 150k??\n",
    "idx_all = np.arange(0, N_total)\n",
    "mask_used = np.zeros(N_total, dtype=bool)\n",
    "mask_used[np.random.choice(np.arange(0, N_total), size=N_used, replace=False)] = 1\n",
    "mask_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(158383, 512)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_t = np.load(\"../AUX/VGG_224/nuswide_VGG_avg.npy\")\n",
    "\n",
    "X_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de datos Entrenamiento:  156283\n",
      "Cantidad de datos Pruebas:  2100\n"
     ]
    }
   ],
   "source": [
    "from utils import sample_test_mask\n",
    "mask_train = sample_test_mask(labels_t, N=100)\n",
    "\n",
    "## creat test como dicen...\n",
    "X_test = X_t[~mask_train]\n",
    "X_t = X_t[mask_train]\n",
    "labels_test = enmask_data(labels_t, ~mask_train)\n",
    "labels_t = enmask_data(labels_t, mask_train)\n",
    "\n",
    "gc.collect()\n",
    "print(\"Cantidad de datos Entrenamiento: \",len(X_t))\n",
    "print(\"Cantidad de datos Pruebas: \",len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### CelebA\n",
    "---\n",
    "* Cantidad de datos totales: 202599\n",
    "* Datos en plataforma Kaggle:https://www.kaggle.com/jessicali9530/celeba-dataset \n",
    "\n",
    "Imágenes RGB re-dimensionadas a 64x64 píxeles, de fotos naturales de rostros de celebridades. Una muestra es utilizada\n",
    "\n",
    "|Tipo set|Datos|Label|\n",
    "|---|---|---|\n",
    "|Entrenamiento|xxx|40|\n",
    "|Validación|xxx|40|\n",
    "|Pruebas|xxx|40|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mask_av = check_availability(folder+\"small_images/\", labels_t)\n",
    "np.savetxt(\"./Data/+\"name_dat+\"_mask_avail.txt\", mask_av, fmt=\"%1i\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de labels:  40\n",
      "Cantidad de objetos:  202599\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['5_o_Clock_Shadow',\n",
       " 'Arched_Eyebrows',\n",
       " 'Attractive',\n",
       " 'Bags_Under_Eyes',\n",
       " 'Bald']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_dat = \"CelebA\"\n",
    "mask_av = np.loadtxt(\"./Data/\"+name_dat+\"_mask_avail.txt\").astype(bool)\n",
    "\n",
    "folder = \"../CelebA/\"\n",
    "\n",
    "part = pd.read_csv(folder+\"list_eval_partition.csv\")\n",
    "mask_train = (part[\"partition\"].values == 0)[mask_av]\n",
    "mask_val = (part[\"partition\"].values == 1)[mask_av]\n",
    "mask_test = (part[\"partition\"].values == 2)[mask_av]\n",
    "\n",
    "df_atrr = pd.read_csv(folder+\"list_attr_celeba.csv\")[mask_av]\n",
    "img_names = df_atrr[\"image_id\"].values\n",
    "labels = list(df_atrr.columns[1:])\n",
    "print(\"Cantidad de labels: \",len(labels) )\n",
    "\n",
    "N_total = len(df_atrr)\n",
    "print(\"Cantidad de objetos: \",N_total )\n",
    "aux = (df_atrr == 1).values\n",
    "labels_t = np.asarray([list(df_atrr.columns[aux[value]]) for value in range(N_total)])\n",
    "labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(202599, 512)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_t = np.load(\"./Data/VGG_128/celeba_VGG_avg.npy\")\n",
    "X_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de datos Entrenamiento:  198599\n",
      "Cantidad de datos Pruebas:  4000\n"
     ]
    }
   ],
   "source": [
    "from utils import sample_test_mask\n",
    "mask_train = sample_test_mask(labels_t, N=100)\n",
    "\n",
    "## creat test como dicen...\n",
    "X_test = X_t[~mask_train]\n",
    "X_t = X_t[mask_train]\n",
    "labels_test = enmask_data(labels_t, ~mask_train)\n",
    "labels_t = enmask_data(labels_t, mask_train)\n",
    "\n",
    "gc.collect()\n",
    "print(\"Cantidad de datos Entrenamiento: \",len(X_t))\n",
    "print(\"Cantidad de datos Pruebas: \",len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation creation\n",
    "\n",
    "Pre-process: División por 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_t = X_t.astype(\"float32\")/255\n",
    "X_test = X_test.astype(\"float32\")/255\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.23920606, -0.07962036, -0.5254031 , ...,  2.0260806 ,\n",
       "        -0.7929327 , -0.28251556],\n",
       "       [-0.4696024 , -0.42504   , -0.5254031 , ..., -0.36204025,\n",
       "        -0.7727085 ,  0.654433  ],\n",
       "       [-0.4696024 ,  0.70346487, -0.08988021, ...,  1.8024794 ,\n",
       "        -0.87285894, -0.39981654],\n",
       "       ...,\n",
       "       [-0.19953102, -0.0663664 , -0.47528175, ..., -0.16238125,\n",
       "        -0.8415185 , -0.37152454],\n",
       "       [ 0.60592276, -0.42423177,  1.9266406 , ...,  0.31726354,\n",
       "        -0.44633317, -0.3987648 ],\n",
       "       [ 6.880586  , -0.08244617, -0.26303512, ..., -0.43013588,\n",
       "        -0.24529743, -0.03925205]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "std = StandardScaler(with_mean=True, with_std=True)\n",
    "std.fit(X_t)\n",
    "\n",
    "X_t = std.transform(X_t)\n",
    "X_test = std.transform(X_test)\n",
    "X_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de datos Entrenamiento:  154183\n",
      "Cantidad de datos Validación:  2100\n",
      "Cantidad de datos Pruebas:  2100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, labels_train, labels_val  = train_test_split(X_t, labels_t, random_state=20,test_size=len(X_test))\n",
    "\n",
    "print(\"Cantidad de datos Entrenamiento: \",len(X_train))\n",
    "print(\"Cantidad de datos Validación: \",len(X_val))\n",
    "print(\"Cantidad de datos Pruebas: \",len(X_test))\n",
    "del X_t, labels_t\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_input = X_train\n",
    "X_val_input = X_val\n",
    "X_test_input = X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "---\n",
    "CNN\n",
    "https://github.com/rtflynn/Cifar-Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_BKL_loss(logits_b):\n",
    "    ep = K.epsilon()\n",
    "    p_b_x = keras.activations.sigmoid(logits_b) #B_j = Q(b_j|x) probability of b_j\n",
    "    p_b = K.mean(p_b_x, axis=0, keepdims=True) #marginalize over minibatch\n",
    "    Nb = K.int_shape(p_b)[1]\n",
    "    def infoKL(y_true, y_pred):\n",
    "        return Nb*np.log(2) + K.sum( p_b*K.log(p_b + ep) + (1-p_b)* K.log(1-p_b +ep),axis=1) #mean\n",
    "    return infoKL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def REC_loss(x_true, x_pred):\n",
    "    if raw:\n",
    "        return K.mean( K.binary_crossentropy(x_true, x_pred), axis=-1)\n",
    "    else:\n",
    "        return K.mean( (x_true- x_pred)**2 ,axis=-1)  #it should be VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "def sample_gumbel(shape,eps=K.epsilon()):\n",
    "    \"\"\"Inverse Sample function from Gumbel(0, 1)\"\"\"\n",
    "    U = K.random_uniform(shape, 0, 1)\n",
    "    return K.log(U + eps)- K.log(1-U + eps)\n",
    "\n",
    "def binary_VAE(data_dim,Nb,units,layers_e,layers_d,opt='adam',BN=True,beta=0., lamb=0., summ=True):\n",
    "    tau = K.variable(0.67, name=\"temperature\") #o tau fijo en 0.67=2/3\n",
    "    \n",
    "    pre_encoder = define_pre_encoder(data_dim, layers=layers_e,units=units,BN=BN)\n",
    "    if raw:\n",
    "        generator = define_generator(Nb,data_dim,layers=layers_d,units=units,BN=BN, out_type='sigmoid')\n",
    "    else:\n",
    "        generator = define_generator(Nb,data_dim,layers=layers_d,units=units,BN=BN, out_type='linear')\n",
    "    if summ:\n",
    "        print(\"pre-encoder network:\")\n",
    "        pre_encoder.summary()\n",
    "        print(\"generator network:\")\n",
    "        generator.summary()\n",
    "\n",
    "    x = Input(shape=(data_dim,))\n",
    "    hidden = pre_encoder(x)\n",
    "    logits_b  = Dense(Nb, activation='linear', name='logits-b')(hidden) #log(B_j/1-B_j)\n",
    "    #proba = np.exp(logits_b)/(1+np.exp(logits_b)) = sigmoidal(logits_b) <<<<<<<<<< recupera probabilidad\n",
    "    #dist = Dense(Nb, activation='sigmoid')(hidden) #p(b) #otra forma de modelarlo\n",
    "    encoder = Model(x, logits_b)\n",
    "\n",
    "    def sampling(logits_b):\n",
    "        #logits_b = K.log(aux/(1-aux) + K.epsilon() )\n",
    "        b = logits_b + sample_gumbel(K.shape(logits_b)) # logits + gumbel noise\n",
    "        return keras.activations.sigmoid( b/tau )\n",
    "\n",
    "    b_sampled = Lambda(sampling, output_shape=(Nb,), name='sampled')(logits_b)\n",
    "    output = generator(b_sampled)\n",
    "        \n",
    "    Recon_loss = REC_loss\n",
    "    kl_loss = BKL_loss(logits_b)\n",
    "    info_kl_loss = info_BKL_loss(logits_b) #over minibatch\n",
    "    def BVAE_loss(y_true, y_pred): \n",
    "        return Recon_loss(y_true, y_pred) + beta*kl_loss(y_true, y_pred) + lamb*info_kl_loss(y_true,y_pred)\n",
    "\n",
    "    binary_vae = Model(x, output)\n",
    "    binary_vae.compile(optimizer=opt, loss=BVAE_loss, metrics = [Recon_loss,kl_loss,info_kl_loss])\n",
    "    return binary_vae, encoder,generator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train details\n",
    "---\n",
    "\n",
    "* 30* epochs* \n",
    "* *batch size* de 200\n",
    "* optimizador Adam\n",
    "* Inicializador de Glorot (para los pesos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import  compare_hist_train, add_hist_plot\n",
    "\n",
    "batch_size = 100*2 #ya que son datasets mas grandes\n",
    "epochs = 30 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1702: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "***************************************\n",
      "*********** SUMMARY RESULTS ***********\n",
      "***************************************\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lambda</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.8323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>0.8339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.8287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.8317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>0.8295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>0.8371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.000000e-01</td>\n",
       "      <td>0.8315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.8198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>0.7975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.000000e+02</td>\n",
       "      <td>0.7293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.000000e+03</td>\n",
       "      <td>0.6239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.000000e+04</td>\n",
       "      <td>0.5592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.000000e+05</td>\n",
       "      <td>0.5943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.000000e+06</td>\n",
       "      <td>0.5880</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          lambda   score\n",
       "0   1.000000e-07  0.8323\n",
       "1   1.000000e-06  0.8339\n",
       "2   1.000000e-05  0.8287\n",
       "3   1.000000e-04  0.8317\n",
       "4   1.000000e-03  0.8295\n",
       "5   1.000000e-02  0.8371\n",
       "6   1.000000e-01  0.8315\n",
       "7   1.000000e+00  0.8198\n",
       "8   1.000000e+01  0.7975\n",
       "9   1.000000e+02  0.7293\n",
       "10  1.000000e+03  0.6239\n",
       "11  1.000000e+04  0.5592\n",
       "12  1.000000e+05  0.5943\n",
       "13  1.000000e+06  0.5880"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best value is 0.8371 with lambda 0.010000\n",
      "Worst value is 0.5593 with lambda 10000.000000\n",
      "***************************************\n"
     ]
    }
   ],
   "source": [
    "#from utils import find_lambda\n",
    "\n",
    "def create_model_B(lambda_V):\n",
    "    return binary_VAE(X_train_input.shape[1],Nb=32,units=500,layers_e=2,layers_d=0\n",
    "                                                                  ,beta=0, lamb=lambda_V, summ=False)\n",
    "\n",
    "lambda_B = find_lambda(create_model_B, X_train_input, X_train, X_val_input,labels_train,labels_val,\n",
    "                   binary=True, BS=batch_size)\n",
    "#mnist -- raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_V_found = {\"mnist-raw\":1e-2, \"mnist\":1e-2, \"cifar-10\":1e1, \"nus-wide\":1e1} #values for sum\n",
    "\n",
    "lambda_B = lambda_V_found[name_dat.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_V_found = {\"mnist-raw\":1e-5, \"mnist\":1, \"cifar-10\":1e2, \"nus-wide\":1e3} #values for mean\n",
    "\n",
    "lambda_B = lambda_V_found[name_dat.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-encoder network:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               256500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 511,000\n",
      "Trainable params: 509,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               16896     \n",
      "=================================================================\n",
      "Total params: 16,896\n",
      "Trainable params: 16,896\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Tensor(\"loss/generator/decoder_loss/add_3:0\", shape=(1,), dtype=float32)\n",
      "Tensor(\"loss/generator/decoder_loss/Mean_1:0\", shape=(?,), dtype=float32)\n",
      "Tensor(\"loss/generator/decoder_loss/add_7:0\", shape=(?,), dtype=float32)\n",
      "Train on 58000 samples, validate on 1000 samples\n",
      "Epoch 1/30\n",
      "58000/58000 [==============================] - 4s 62us/step - loss: 0.8182 - REC_loss: 0.8176 - KL: 0.5050 - infoKL: 5.1800e-04 - val_loss: 0.7469 - val_REC_loss: 0.7446 - val_KL: 0.5701 - val_infoKL: 0.0023\n",
      "Epoch 2/30\n",
      "58000/58000 [==============================] - 3s 49us/step - loss: 0.7127 - REC_loss: 0.7120 - KL: 0.5503 - infoKL: 6.8815e-04 - val_loss: 0.7049 - val_REC_loss: 0.7027 - val_KL: 0.5627 - val_infoKL: 0.0022\n",
      "Epoch 3/30\n",
      "58000/58000 [==============================] - 3s 48us/step - loss: 0.6874 - REC_loss: 0.6866 - KL: 0.5503 - infoKL: 7.6859e-04 - val_loss: 0.6898 - val_REC_loss: 0.6878 - val_KL: 0.5595 - val_infoKL: 0.0020\n",
      "Epoch 4/30\n",
      "58000/58000 [==============================] - 3s 51us/step - loss: 0.6769 - REC_loss: 0.6761 - KL: 0.5498 - infoKL: 8.0327e-04 - val_loss: 0.6825 - val_REC_loss: 0.6806 - val_KL: 0.5563 - val_infoKL: 0.0020\n",
      "Epoch 5/30\n",
      "58000/58000 [==============================] - 3s 51us/step - loss: 0.6715 - REC_loss: 0.6707 - KL: 0.5497 - infoKL: 8.0756e-04 - val_loss: 0.6800 - val_REC_loss: 0.6779 - val_KL: 0.5521 - val_infoKL: 0.0021\n",
      "Epoch 6/30\n",
      "58000/58000 [==============================] - 3s 50us/step - loss: 0.6685 - REC_loss: 0.6677 - KL: 0.5485 - infoKL: 8.0897e-04 - val_loss: 0.6776 - val_REC_loss: 0.6753 - val_KL: 0.5495 - val_infoKL: 0.0023\n",
      "Epoch 7/30\n",
      "58000/58000 [==============================] - 3s 49us/step - loss: 0.6664 - REC_loss: 0.6656 - KL: 0.5485 - infoKL: 8.0676e-04 - val_loss: 0.6759 - val_REC_loss: 0.6734 - val_KL: 0.5498 - val_infoKL: 0.0025\n",
      "Epoch 8/30\n",
      "58000/58000 [==============================] - 3s 50us/step - loss: 0.6649 - REC_loss: 0.6640 - KL: 0.5482 - infoKL: 8.2278e-04 - val_loss: 0.6745 - val_REC_loss: 0.6721 - val_KL: 0.5483 - val_infoKL: 0.0024\n",
      "Epoch 9/30\n",
      "58000/58000 [==============================] - 3s 51us/step - loss: 0.6639 - REC_loss: 0.6630 - KL: 0.5477 - infoKL: 8.0889e-04 - val_loss: 0.6730 - val_REC_loss: 0.6708 - val_KL: 0.5487 - val_infoKL: 0.0022\n",
      "Epoch 10/30\n",
      "58000/58000 [==============================] - 3s 51us/step - loss: 0.6630 - REC_loss: 0.6622 - KL: 0.5481 - infoKL: 8.1705e-04 - val_loss: 0.6722 - val_REC_loss: 0.6699 - val_KL: 0.5485 - val_infoKL: 0.0022\n",
      "Epoch 11/30\n",
      "58000/58000 [==============================] - 3s 52us/step - loss: 0.6621 - REC_loss: 0.6613 - KL: 0.5482 - infoKL: 7.9473e-04 - val_loss: 0.6708 - val_REC_loss: 0.6686 - val_KL: 0.5488 - val_infoKL: 0.0022\n",
      "Epoch 12/30\n",
      "58000/58000 [==============================] - 3s 51us/step - loss: 0.6615 - REC_loss: 0.6607 - KL: 0.5486 - infoKL: 8.0690e-04 - val_loss: 0.6716 - val_REC_loss: 0.6692 - val_KL: 0.5478 - val_infoKL: 0.0024\n",
      "Epoch 13/30\n",
      "58000/58000 [==============================] - 3s 49us/step - loss: 0.6607 - REC_loss: 0.6599 - KL: 0.5490 - infoKL: 8.0174e-04 - val_loss: 0.6704 - val_REC_loss: 0.6680 - val_KL: 0.5488 - val_infoKL: 0.0023\n",
      "Epoch 14/30\n",
      "58000/58000 [==============================] - 3s 50us/step - loss: 0.6601 - REC_loss: 0.6593 - KL: 0.5496 - infoKL: 7.9693e-04 - val_loss: 0.6700 - val_REC_loss: 0.6675 - val_KL: 0.5495 - val_infoKL: 0.0025\n",
      "Epoch 15/30\n",
      "58000/58000 [==============================] - 3s 50us/step - loss: 0.6595 - REC_loss: 0.6587 - KL: 0.5499 - infoKL: 8.0015e-04 - val_loss: 0.6685 - val_REC_loss: 0.6662 - val_KL: 0.5494 - val_infoKL: 0.0023\n",
      "Epoch 16/30\n",
      "58000/58000 [==============================] - 3s 50us/step - loss: 0.6591 - REC_loss: 0.6583 - KL: 0.5505 - infoKL: 7.9685e-04 - val_loss: 0.6689 - val_REC_loss: 0.6665 - val_KL: 0.5505 - val_infoKL: 0.0024\n",
      "Epoch 17/30\n",
      "58000/58000 [==============================] - 3s 49us/step - loss: 0.6586 - REC_loss: 0.6578 - KL: 0.5507 - infoKL: 7.9585e-04 - val_loss: 0.6680 - val_REC_loss: 0.6658 - val_KL: 0.5505 - val_infoKL: 0.0022\n",
      "Epoch 18/30\n",
      "58000/58000 [==============================] - 3s 51us/step - loss: 0.6580 - REC_loss: 0.6572 - KL: 0.5511 - infoKL: 7.8321e-04 - val_loss: 0.6686 - val_REC_loss: 0.6665 - val_KL: 0.5510 - val_infoKL: 0.0022\n",
      "Epoch 19/30\n",
      "58000/58000 [==============================] - 3s 50us/step - loss: 0.6576 - REC_loss: 0.6569 - KL: 0.5515 - infoKL: 7.7876e-04 - val_loss: 0.6681 - val_REC_loss: 0.6657 - val_KL: 0.5515 - val_infoKL: 0.0024\n",
      "Epoch 20/30\n",
      "58000/58000 [==============================] - 3s 51us/step - loss: 0.6573 - REC_loss: 0.6565 - KL: 0.5517 - infoKL: 7.9837e-04 - val_loss: 0.6677 - val_REC_loss: 0.6654 - val_KL: 0.5509 - val_infoKL: 0.0022\n",
      "Epoch 21/30\n",
      "58000/58000 [==============================] - 3s 50us/step - loss: 0.6569 - REC_loss: 0.6561 - KL: 0.5522 - infoKL: 8.1736e-04 - val_loss: 0.6669 - val_REC_loss: 0.6647 - val_KL: 0.5522 - val_infoKL: 0.0022\n",
      "Epoch 22/30\n",
      "58000/58000 [==============================] - 3s 50us/step - loss: 0.6565 - REC_loss: 0.6557 - KL: 0.5531 - infoKL: 7.9290e-04 - val_loss: 0.6663 - val_REC_loss: 0.6640 - val_KL: 0.5524 - val_infoKL: 0.0023\n",
      "Epoch 23/30\n",
      "58000/58000 [==============================] - 3s 50us/step - loss: 0.6561 - REC_loss: 0.6553 - KL: 0.5540 - infoKL: 7.9402e-04 - val_loss: 0.6664 - val_REC_loss: 0.6641 - val_KL: 0.5519 - val_infoKL: 0.0023\n",
      "Epoch 24/30\n",
      "58000/58000 [==============================] - 3s 50us/step - loss: 0.6558 - REC_loss: 0.6550 - KL: 0.5541 - infoKL: 7.7736e-04 - val_loss: 0.6660 - val_REC_loss: 0.6638 - val_KL: 0.5520 - val_infoKL: 0.0022\n",
      "Epoch 25/30\n",
      "58000/58000 [==============================] - 3s 50us/step - loss: 0.6555 - REC_loss: 0.6547 - KL: 0.5547 - infoKL: 7.8019e-04 - val_loss: 0.6657 - val_REC_loss: 0.6633 - val_KL: 0.5530 - val_infoKL: 0.0023\n",
      "Epoch 26/30\n",
      "58000/58000 [==============================] - 3s 50us/step - loss: 0.6553 - REC_loss: 0.6545 - KL: 0.5554 - infoKL: 7.9121e-04 - val_loss: 0.6654 - val_REC_loss: 0.6632 - val_KL: 0.5533 - val_infoKL: 0.0022\n",
      "Epoch 27/30\n",
      "58000/58000 [==============================] - 3s 49us/step - loss: 0.6548 - REC_loss: 0.6541 - KL: 0.5557 - infoKL: 7.7852e-04 - val_loss: 0.6650 - val_REC_loss: 0.6628 - val_KL: 0.5536 - val_infoKL: 0.0022\n",
      "Epoch 28/30\n",
      "58000/58000 [==============================] - 3s 49us/step - loss: 0.6546 - REC_loss: 0.6538 - KL: 0.5566 - infoKL: 7.9023e-04 - val_loss: 0.6641 - val_REC_loss: 0.6620 - val_KL: 0.5542 - val_infoKL: 0.0021\n",
      "Epoch 29/30\n",
      "58000/58000 [==============================] - 3s 48us/step - loss: 0.6543 - REC_loss: 0.6535 - KL: 0.5572 - infoKL: 7.8628e-04 - val_loss: 0.6642 - val_REC_loss: 0.6620 - val_KL: 0.5548 - val_infoKL: 0.0022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30\n",
      "58000/58000 [==============================] - 3s 48us/step - loss: 0.6540 - REC_loss: 0.6532 - KL: 0.5576 - infoKL: 7.9168e-04 - val_loss: 0.6654 - val_REC_loss: 0.6632 - val_KL: 0.5562 - val_infoKL: 0.0022\n"
     ]
    }
   ],
   "source": [
    "binary_vae,encoder_Bvae,generator_Bvae= binary_VAE(X_train.shape[1],Nb=32,units=500,layers_e=2,layers_d=0,beta=beta_B)\n",
    "\n",
    "hist2 = binary_vae.fit(X_train_input, X_train, epochs=epochs, batch_size=batch_size\n",
    "                           ,validation_data=(X_val_input,X_val) )\n",
    "                       #,callbacks=[Tau_Call(tau)] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another intrinsic measure: *Classification*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#function to define and train model\n",
    "from sklearn.metrics import jaccard_score\n",
    "from utils import define_fit, MedianHashing, visualize_probas, visualize_mean, calculate_hash,visualize_probas_byB\n",
    "\n",
    "results = []\n",
    "results_S = []\n",
    "results_B = []\n",
    "results_O_B = [] #original testing on tresholded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 1., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#codify input data (binarize -- or aprox)\n",
    "X_train_logits = encoder_Bvae.predict(X_train)\n",
    "X_val_logits = encoder_Bvae.predict(X_test)\n",
    "\n",
    "#probabilities\n",
    "X_train_Bcode = expit(X_train_logits)\n",
    "X_val_Bcode = expit(X_val_logits)\n",
    "\n",
    "##codify labels\n",
    "labels_aux = np.asarray(labels)\n",
    "def codify_labels(inputs):\n",
    "    inputs = np.asarray(inputs)\n",
    "    matrix_labels = np.zeros((inputs.shape[0],labels_aux.shape[0]))\n",
    "    for i,aux_labels in enumerate(inputs):\n",
    "        if type(aux_labels) == list or type(aux_labels) == np.ndarray :\n",
    "            for aux_label in aux_labels:\n",
    "                idx = np.where(aux_label==labels_aux)[0]\n",
    "                matrix_labels[i,idx] = 1 #various-multiple\n",
    "        else:\n",
    "            idx = np.where(aux_labels==labels_aux)[0]\n",
    "            matrix_labels[i,idx] = 1 #only one\n",
    "    return matrix_labels\n",
    "\n",
    "C_train = codify_labels(labels_train)\n",
    "#C_val = codify_labels(labels_val)\n",
    "C_val = codify_labels(labels_test)\n",
    "C_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(name_dat)\n",
    "visualize_probas(X_train_logits, X_train_Bcode)\n",
    "\n",
    "visualize_probas_byB(X_train_Bcode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "aux = [name_dat.lower()]\n",
    "multi_label = \"celeba\" in aux or \"nus-wide\" in aux\n",
    "\n",
    "model1_O = define_fit(multi_label,X_train_Bcode,C_train)\n",
    "\n",
    "if not multi_label:\n",
    "    aux.append(model1_O.evaluate(X_train_Bcode,C_train,verbose=0)[1])\n",
    "    aux.append(model1_O.evaluate(X_val_Bcode,C_val,verbose=0)[1])\n",
    "else:\n",
    "    aux.append(jaccard_score(C_train, (model1_O.predict(X_train_Bcode)>=0.5)*1, average='micro'))\n",
    "    aux.append(jaccard_score(C_val, (model1_O.predict(X_val_Bcode)>=0.5)*1, average='micro'))\n",
    "    \n",
    "results.append(aux)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for valores in results:\n",
    "    print(\"\\nAccuracy on dataset: \",valores[0])\n",
    "    print(\"Binary VAE (train-val): %f - %f\"%(valores[1],valores[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, ..., 0, 1, 0],\n",
       "       [0, 1, 0, ..., 1, 0, 1],\n",
       "       [0, 1, 0, ..., 0, 0, 1],\n",
       "       ...,\n",
       "       [0, 1, 1, ..., 0, 0, 1],\n",
       "       [0, 1, 1, ..., 1, 0, 1],\n",
       "       [0, 1, 0, ..., 1, 1, 1]], dtype=int32)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## binary // treshold it\n",
    "X_train_Bcode_B = calculate_hash(X_train_Bcode, from_probas=True, from_logits=False)\n",
    "X_val_Bcode_B = calculate_hash(X_val_Bcode, from_probas=True, from_logits=False)\n",
    "\n",
    "X_train_Tcode_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "### trained models over encoder representation testing on tresholded representation (see how decrease)\n",
    "aux = [name_dat.lower()]\n",
    "multi_label = \"celeba\" in aux or \"nus-wide\" in aux\n",
    "\n",
    "if not multi_label:\n",
    "    aux.append(model1_O.evaluate(X_train_Bcode_B,C_train,verbose=0)[1])\n",
    "    aux.append(model1_O.evaluate(X_val_Bcode_B,C_val,verbose=0)[1])\n",
    "else:\n",
    "    aux.append(jaccard_score(C_train, (model1_O.predict(X_train_Bcode_B)>=0.5)*1, average='micro'))\n",
    "    aux.append(jaccard_score(C_val, (model1_O.predict(X_val_Bcode_B)>=0.5)*1, average='micro'))\n",
    "    \n",
    "results_O_B.append(aux)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification on Binary (thresholded) representation\")\n",
    "for valores in results_O_B:\n",
    "    print(\"\\nAccuracy on dataset: \",valores[0])\n",
    "    print(\"Binary VAE (train-val): %f - %f\"%(valores[1],valores[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "aux = [name_dat.lower()]\n",
    "multi_label = \"celeba\" in aux or \"nus-wide\" in aux\n",
    "\n",
    "model1 = define_fit(multi_label,X_train_Bcode_B,C_train)\n",
    "\n",
    "if not multi_label:\n",
    "    aux.append(model1.evaluate(X_train_Bcode_B,C_train,verbose=0)[1])\n",
    "    aux.append(model1.evaluate(X_val_Bcode_B,C_val,verbose=0)[1])\n",
    "else:\n",
    "    aux.append(jaccard_score(C_train, (model1.predict(X_train_Bcode_B)>=0.5)*1, average='micro'))\n",
    "    aux.append(jaccard_score(C_val, (model1.predict(X_val_Bcode_B)>=0.5)*1, average='micro'))\n",
    "    \n",
    "results_B.append(aux)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classification on Binary representation\")\n",
    "for valores in results_B:\n",
    "    print(\"\\nAccuracy on dataset: \",valores[0])\n",
    "    print(\"Binary VAE (train-val): %f - %f\"%(valores[1],valores[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Results\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils import MedianHashing, get_similar, measure_metrics, calculate_hash\n",
    "from utils import MAP_atk, M_P_atk, AP_atk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentación\n",
    "---\n",
    "A continación se realizan las experimentaciones correspondientes para encontrar la mejor arquitectura y mejor configuración del modelo propuesto en base a las métricas *precision* y *recall* del conjunto de validación en el **top 100** de elementos recuperados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_hashingB(encoder,train,val,labels_train,labels_val,traditional=True,tipo=\"topK\",K=100):\n",
    "    encode_train = encoder.predict(train)\n",
    "    encode_val = encoder.predict(val)\n",
    "    \n",
    "    train_hash = calculate_hash(encode_train, from_probas=~traditional )\n",
    "    val_hash = calculate_hash(encode_val, from_probas=~traditional)\n",
    "\n",
    "    val_similares_train =  get_similar(val_hash, train_hash, tipo=tipo,K=K) \n",
    "\n",
    "    return measure_metrics(labels,val_similares_train,labels_query=labels_val,labels_source=labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Experimentando variando el #Bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_dat = {\"mnist\":{\"p\":[],\"r\":[]},\"cifar-10\":{\"p\":[],\"r\":[]},\n",
    "              \"celeba\":{\"p\":[],\"r\":[]},'nus-wide':{\"p\":[],\"r\":[]},\n",
    "               \"mnist-raw\":{\"p\":[],\"r\":[]}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Best models facing test set*\n",
    "Luego de la selección de la mejor configuración de los modelos se evalua de diferentes formas sobre el conjunto de pruebas de los distintos *datasets*, los cambios realizados son:\n",
    "* Se entrenan sobre todos los datos disponibles (*train+val*)\n",
    "* Se aumentó el número de *epochs* a 50.\n",
    "\n",
    "El mejor modelo:\n",
    "* Simétrica para *Binary VAE* y Base para *Traditional VAE*\n",
    "* 32 Bits en la codificación (número de variables latentes)\n",
    "* Representación sobre las top 10 mil *tokens* más frecuentes y *Term Frequency*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import calculate_hash\n",
    "def evaluate_hashing_DE(train_hash,test_hash,labels_trainn,labels_testt,tipo=\"topK\",eval_tipo='PRatk',K=100):\n",
    "    \"\"\"\n",
    "        Evaluate Hashing correclty: Query and retrieve on a different set\n",
    "    \"\"\"\n",
    "    test_similares_train =  get_similar(test_hash,train_hash,tipo=tipo,K=K)\n",
    "    if eval_tipo==\"MAP\":\n",
    "        return MAP_atk(test_similares_train,labels_query=labels_testt, labels_source=labels_trainn, K=0) #all ranking\n",
    "    elif eval_tipo == \"PRatk\":\n",
    "        return measure_metrics(labels,test_similares_train,labels_testt,labels_source=labels_trainn)\n",
    "    elif eval_tipo == \"Patk\":\n",
    "        return M_P_atk(test_similares_train, labels_query=labels_testt, labels_source=labels_trainn, K=K)\n",
    "\n",
    "def hash_data(model, x_train, x_test, binary=True):\n",
    "    encode_train = model.predict(x_train)\n",
    "    encode_test = model.predict(x_test)\n",
    "    \n",
    "    train_hash = calculate_hash(encode_train, from_probas=binary )\n",
    "    test_hash = calculate_hash(encode_test, from_probas = binary)\n",
    "    return train_hash, test_hash\n",
    "\n",
    "#to save results\n",
    "results_dat = {\"mnist\":{},\"mnist-raw\":{},\"cifar-10\":{},\"celeba\":{}, \"nus-wide\":{}} \n",
    "\n",
    "BITS_S = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_total = np.concatenate((X_train,X_val),axis=0)\n",
    "X_total_input = X_total\n",
    "labels_total = np.concatenate((labels_train,labels_val),axis=0)\n",
    "del X_train, X_train_input, X_val, X_val_input, labels_train,labels_val\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-encoder network:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 500)               256500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 511,000\n",
      "Trainable params: 509,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 512)               16896     \n",
      "=================================================================\n",
      "Total params: 16,896\n",
      "Trainable params: 16,896\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "pre-encoder network:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 500)               256500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 511,000\n",
      "Trainable params: 509,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 512)               16896     \n",
      "=================================================================\n",
      "Total params: 16,896\n",
      "Trainable params: 16,896\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Tensor(\"loss_3/generator/decoder_loss/add_3:0\", shape=(1,), dtype=float32)\n",
      "Tensor(\"loss_3/generator/decoder_loss/Mean_1:0\", shape=(?,), dtype=float32)\n",
      "Tensor(\"loss_3/generator/decoder_loss/add_7:0\", shape=(?,), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2adc0a94eef0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_vae,encoder_Bvae,generator_Bvae = binary_VAE(X_total.shape[1],Nb=BITS_S,units=500,layers_e=2,layers_d=0,beta=beta_B)\n",
    "binary_vae.fit(X_total_input, X_total, epochs=epochs, batch_size=batch_size,verbose=0)\n",
    "#save model\n",
    "encoder_Bvae.save(\"saved_models/\"+name_dat+\"_BVAE_\"+str(BITS_S)+\"b_E_VGG_L?avg.h5\")\n",
    "generator_Bvae.save_weights(\"saved_models/\"+name_dat+\"_BVAE_\"+str(BITS_S)+\"b_D_VGG_L?avg_w.h5\")\n",
    "\n",
    "keras.backend.clear_session()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##load model\n",
    "encoder_Bvae = keras.models.load_model(\"saved_models/\"+name_dat+\"_BVAE_\"+str(BITS_S)+\"b_E_VGG_L?avg.h5\")\n",
    "#generator_Bvae = keras.models.load_model(\"saved_models/\"+name_dat+\"_BVAE_\"+str(BITS_S)+\"b_D_defaultCNN.h5\")\n",
    "encoder_Bvae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_hash_BVAE, test_hash_BVAE = hash_data(encoder_Bvae,X_total_input,X_test_input)\n",
    "\n",
    "del X_total_input, X_test_input, X_test, X_total\n",
    "gc.collect()\n",
    "\n",
    "keras.backend.clear_session()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Realizado.\n"
     ]
    }
   ],
   "source": [
    "dataset = name_dat.lower()\n",
    "\n",
    "p_b,r_b = evaluate_hashing_DE(total_hash_BVAE, test_hash_BVAE,labels_total,labels_test,tipo=\"topK\")\n",
    "results_dat[dataset][\"p\"].append(p_b)\n",
    "results_dat[dataset][\"r\"].append(r_b)\n",
    "p5k_b = evaluate_hashing_DE(total_hash_BVAE, test_hash_BVAE,labels_total,labels_test,eval_tipo=\"Patk\",K=5000)\n",
    "results_dat[dataset][\"p5k\"].append(p5k_b)\n",
    "\n",
    "print(\"Realizado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"*************Top 100 most similar retrieve*************\")\n",
    "\n",
    "print(\"\\nResultados de Precision en pruebas\")\n",
    "t = pd.DataFrame() #Table()\n",
    "t[\"Modelo\"] = [\"Binario\"]\n",
    "t[\"MNIST-R\"] = results_dat[\"mnist-raw\"][\"p\"]\n",
    "t[\"MNIST\"] = results_dat[\"mnist\"][\"p\"]\n",
    "t[\"CIFAR-10\"] = results_dat[\"cifar-10\"][\"p\"]\n",
    "#t[\"CelebA\"] = results_dat[\"celeba\"][\"p\"]\n",
    "t[\"Nus-Wide\"] = results_dat[\"nus-wide\"][\"p\"]\n",
    "print(t)\n",
    "\n",
    "print(\"\\n\\nResultados de Recall en pruebas\")\n",
    "t = pd.DataFrame() #Table()\n",
    "t[\"Modelo\"] = [\"Binario\"]\n",
    "t[\"MNIST-R\"] = results_dat[\"mnist-raw\"][\"r\"]\n",
    "t[\"MNIST\"] = results_dat[\"mnist\"][\"r\"]\n",
    "t[\"CIFAR-10\"] = results_dat[\"cifar-10\"][\"r\"]\n",
    "#t[\"CelebA\"] = results_dat[\"celeba\"][\"r\"]\n",
    "t[\"Nus-Wide\"] = results_dat[\"nus-wide\"][\"r\"]\n",
    "print(t)\n",
    "\n",
    "print(\"\\n\\nResultados de Precision en pruebas (top-5k)\")\n",
    "t = pd.DataFrame() #Table()\n",
    "t[\"Modelo\"] = [\"Binario\"]\n",
    "t[\"MNIST-R\"] = results_dat[\"mnist-raw\"][\"p5k\"]\n",
    "t[\"MNIST\"] = results_dat[\"mnist\"][\"p5k\"]\n",
    "t[\"CIFAR-10\"] = results_dat[\"cifar-10\"][\"p5k\"]\n",
    "#t[\"CelebA\"] = results_dat[\"celeba\"][\"p5k\"]\n",
    "t[\"Nus-Wide\"] = results_dat[\"nus-wide\"][\"p5k\"]\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP on MNIST-raw dataset\n",
      "Traditional VAE =  0.30261327832994833\n",
      "Binary VAE =  0.3810399300812874\n"
     ]
    }
   ],
   "source": [
    "print(\"MAP on %s dataset\"%name_dat)\n",
    "map_t= evaluate_hashing_DE(total_hash_VAE,test_hash_VAE,labels_total,labels_test,eval_tipo=\"MAP\",K=9999999)\n",
    "print(\"Traditional VAE = \",map_t)\n",
    "\n",
    "map_b= evaluate_hashing_DE(total_hash_BVAE, test_hash_BVAE,labels_total,labels_test,eval_tipo=\"MAP\",K=9999999)\n",
    "print(\"Binary VAE = \",map_b)\n",
    "#raw estilo binary.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP on MNIST dataset\n",
      "Traditional VAE =  0.43409273017681177\n",
      "Binary VAE =  0.4929304844746192\n"
     ]
    }
   ],
   "source": [
    "print(\"MAP on %s dataset\"%name_dat)\n",
    "map_t= evaluate_hashing_DE(total_hash_VAE,test_hash_VAE,labels_total,labels_test,eval_tipo=\"MAP\",K=9999999)\n",
    "print(\"Traditional VAE = \",map_t)\n",
    "\n",
    "map_b= evaluate_hashing_DE(total_hash_BVAE, test_hash_BVAE,labels_total,labels_test,eval_tipo=\"MAP\",K=9999999)\n",
    "print(\"Binary VAE = \",map_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP on CIFAR-10 dataset\n",
      "Traditional VAE =  0.24808571513939143\n",
      "Binary VAE =  0.26827994302416186\n"
     ]
    }
   ],
   "source": [
    "print(\"MAP on %s dataset\"%name_dat)\n",
    "map_t= evaluate_hashing_DE(total_hash_VAE,test_hash_VAE,labels_total,labels_test,eval_tipo=\"MAP\",K=9999999)\n",
    "print(\"Traditional VAE = \",map_t)\n",
    "\n",
    "map_b= evaluate_hashing_DE(total_hash_BVAE, test_hash_BVAE,labels_total,labels_test,eval_tipo=\"MAP\",K=9999999)\n",
    "print(\"Binary VAE = \",map_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP on CIFAR-10 dataset\n",
      "Traditional VAE =  0.24747785137194664\n",
      "Binary VAE =  0.27358479284808834\n"
     ]
    }
   ],
   "source": [
    "print(\"MAP on %s dataset\"%name_dat)\n",
    "map_t= evaluate_hashing_DE(total_hash_VAE,test_hash_VAE,labels_total,labels_test,eval_tipo=\"MAP\",K=9999999)\n",
    "print(\"Traditional VAE = \",map_t)\n",
    "\n",
    "map_b= evaluate_hashing_DE(total_hash_BVAE, test_hash_BVAE,labels_total,labels_test,eval_tipo=\"MAP\",K=9999999)\n",
    "print(\"Binary VAE = \",map_b) #vgg 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP on CIFAR-10 dataset\n",
      "Traditional VAE =  0.2490984142621118\n",
      "Binary VAE =  0.29134353422855586\n"
     ]
    }
   ],
   "source": [
    "print(\"MAP on %s dataset\"%name_dat)\n",
    "map_t= evaluate_hashing_DE(total_hash_VAE,test_hash_VAE,labels_total,labels_test,eval_tipo=\"MAP\",K=9999999)\n",
    "print(\"Traditional VAE = \",map_t)\n",
    "\n",
    "map_b= evaluate_hashing_DE(total_hash_BVAE, test_hash_BVAE,labels_total,labels_test,eval_tipo=\"MAP\",K=9999999)\n",
    "print(\"Binary VAE = \",map_b) #vgg 224 -- infovae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP@5000 on Nus-Wide dataset\n",
      "Traditional VAE =  0.7155371284080504\n",
      "Binary VAE =  0.7319403102242767\n"
     ]
    }
   ],
   "source": [
    "print(\"MAP@5000 on %s dataset\"%name_dat)\n",
    "map_t= evaluate_hashing_DE(total_hash_VAE,test_hash_VAE,labels_total,labels_test,eval_tipo=\"MAP\",K=5000)\n",
    "print(\"Traditional VAE = \",map_t)\n",
    "\n",
    "map_b= evaluate_hashing_DE(total_hash_BVAE, test_hash_BVAE,labels_total,labels_test,eval_tipo=\"MAP\",K=5000)\n",
    "print(\"Binary VAE = \",map_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP on Nus-Wide dataset\n",
      "Traditional VAE =  0.5253504847565769\n",
      "Binary VAE =  0.5498500603806175\n"
     ]
    }
   ],
   "source": [
    "print(\"MAP on %s dataset\"%name_dat)\n",
    "map_t= evaluate_hashing_DE(total_hash_VAE,test_hash_VAE,labels_total,labels_test,eval_tipo=\"MAP\",K=9999999)\n",
    "print(\"Traditional VAE = \",map_t)\n",
    "\n",
    "map_b= evaluate_hashing_DE(total_hash_BVAE, test_hash_BVAE,labels_total,labels_test,eval_tipo=\"MAP\",K=9999999)\n",
    "print(\"Binary VAE = \",map_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## en otro jupyter agregar varios.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La propuesta presentada de *VAE* binario sobrepasa al *VAE* tradicional en todos los resultados presentados.\n",
    "### Experimentos con el *ball search*\n",
    "A continuación se muestra una leve experimentación cuando se trabaja con la búsqueda a través de un radio en el espacio de *Haming*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_similar_hammD_based,get_hammD\n",
    "ball_radius = np.arange(0,25) #ball of radius graphic\n",
    "\n",
    "traditional_dat_BS = {\"mnist\":{},\"cifar-10\":{},\"celeba\":{},'nus-wide':{},\"mnist-raw\":{}}\n",
    "binary_dat_BS = {\"mnist\":{},\"cifar-10\":{},\"celeba\":{},'nus-wide':{},\"mnist-raw\":{}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dat_show = \"MNIST\"\n",
    "print(\"DAtaset \",dat_show)\n",
    "print(\"B-VAE P@ball=2 equals=\",binary_dat_BS[dat_show.lower()][\"p\"][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dat_show = \"CIFAR-10\"\n",
    "print(\"DAtaset \",dat_show)\n",
    "print(\"B-VAE P@ball=2 equals=\",binary_dat_BS[dat_show.lower()][\"p\"][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dat_show = \"NUS-WIDE\"\n",
    "print(\"DAtaset \",dat_show)\n",
    "print(\"B-VAE P@ball=2 equals=\",binary_dat_BS[dat_show.lower()][\"p\"][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Termino\n",
      "CPU times: user 1h 55min 16s, sys: 2min 51s, total: 1h 58min 8s\n",
      "Wall time: 1h 58min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dataset = name_dat.lower()\n",
    "\n",
    "binary_dat_BS[dataset][\"p\"] = []\n",
    "binary_dat_BS[dataset][\"r\"] = []\n",
    "\n",
    "hammD_scores = get_hammD(test_hash_BVAE,total_hash_BVAE) #all hamming ranking\n",
    "for ball_r in ball_radius:\n",
    "    test_similares_train = get_similar_hammD_based(hammD_scores,tipo=\"ball\", ball=ball_r)\n",
    "\n",
    "    p_b,r_b  = measure_metrics(labels,test_similares_train,labels_query=labels_test,labels_source=labels_total)\n",
    "    binary_dat_BS[dataset][\"p\"].append(p_b)\n",
    "    binary_dat_BS[dataset][\"r\"].append(r_b)\n",
    "\n",
    "\n",
    "del hammD_scores, test_similares_train\n",
    "gc.collect()\n",
    "    \n",
    "print(\"Termino\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choosed_data = [\"MNIST-RAW\", \"MNIST\", \"CIFAR-10\", \"NUS-WIDE\"]\n",
    "f, axx = plt.subplots(1, len(choosed_data), figsize=(20,5), sharey=False)\n",
    "\n",
    "b_aux = 16\n",
    "for z in range(len(choosed_data)):\n",
    "    dat = choosed_data[z].lower()\n",
    "\n",
    "    axx[z].plot(ball_radius[:b_aux],binary_dat_BS[dat][\"p\"][:b_aux],'go-',label=\"precision (Binary VAE)\")\n",
    "    axx[z].plot(ball_radius[:b_aux],binary_dat_BS[dat][\"r\"][:b_aux],'gv-', label='recall' ) \n",
    "    axx[z].set_title(choosed_data[z])\n",
    "    axx[z].set_xticks(ball_radius)\n",
    "    axx[z].set_xlabel(\"Ball radius\")\n",
    "    axx[z].set_xlim(-0.5, len(ball_radius[:b_aux])-.5)\n",
    "    #axx[z].set_ylim(-0.05)\n",
    "    \n",
    "    if z == 0:\n",
    "        axx[z].set_ylabel(\"Score\")\n",
    "        axx[z].legend(bbox_to_anchor=(2, -0.1))\n",
    "        \n",
    "f.suptitle(\"Comparison of ball search\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### donde beta esta calculado guardad..."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:newpy3_tf1]",
   "language": "python",
   "name": "conda-env-newpy3_tf1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
