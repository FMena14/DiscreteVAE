{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"20%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "<h1 align='center'> Binary Variational Semantic Hashing </h1>\n",
    "\n",
    "<H3 align='center'> Extensión trabajo CIARP </H3>\n",
    "<hr style=\"height:2px;border:none\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fmena/anaconda3/envs/tesis/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/fmena/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:469: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/fmena/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:470: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/fmena/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:471: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/fmena/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:472: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/fmena/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:473: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/fmena/.local/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:476: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras,gc, os, time, sys\n",
    "from keras.layers import *\n",
    "from keras.models import Sequential,Model\n",
    "from keras import backend as K\n",
    "from astropy.table import Table\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from scipy.special import expit\n",
    "\n",
    "from base_networks import *\n",
    "from utils import check_availability, load_imgs_mask\n",
    "\n",
    "from utils import get_topK_labels,set_newlabel_list, enmask_data\n",
    "\n",
    "np.random.seed(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access Data/VGG_128: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!ls Data/VGG_128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "### MNIST\n",
    "---\n",
    "\n",
    "Imágenes en blanco y negro de 28x28 píxeles, de números del 0 al 9, que dan orígen a las 10 clases del problema.\n",
    "\n",
    "|Tipo set|Datos|Label|\n",
    "|---|---|---|\n",
    "|Entrenamiento|60000|10|\n",
    "|Pruebas|10000|10|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_dat = \"MNIST-raw\"\n",
    "\n",
    "(X_t, aux_t), (X_test, aux_test) = keras.datasets.mnist.load_data()\n",
    "labels = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
    "labels_t = np.asarray([labels[value] for value in aux_t])\n",
    "labels_test = np.asarray([labels[value] for value in aux_test])\n",
    "labels_t = np.concatenate((labels_t,labels_test),axis=0)\n",
    "\n",
    "X_t = np.concatenate([X_t, X_test], axis=0).reshape(len(labels_t),28*28)\n",
    "X_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 512)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_dat = \"MNIST\"\n",
    "\n",
    "(_, aux_t), (_, aux_test) = keras.datasets.mnist.load_data()\n",
    "labels = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
    "labels_t = np.asarray([labels[value] for value in aux_t])\n",
    "labels_test = np.asarray([labels[value] for value in aux_test])\n",
    "labels_t = np.concatenate((labels_t,labels_test),axis=0)\n",
    "\n",
    "#X_t = np.load(\"./Data/VGG_CNN/mnist_VGG_avg.npy\")\n",
    "X_t = np.load(\"../AUX/VGG_128/mnist_VGG_avg.npy\")\n",
    "\n",
    "X_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de datos Entrenamiento:  69000\n",
      "Cantidad de datos Pruebas:  1000\n"
     ]
    }
   ],
   "source": [
    "##### 1000 imagenes de prueba... \n",
    "from utils import sample_test_mask\n",
    "mask_train = sample_test_mask(labels_t, N=100, multi_label=False)\n",
    "\n",
    "## creat test como dicen...\n",
    "X_test = X_t[~mask_train]\n",
    "X_t = X_t[mask_train]\n",
    "labels_test = enmask_data(labels_t, ~mask_train)\n",
    "labels_t = enmask_data(labels_t, mask_train)\n",
    "\n",
    "gc.collect()\n",
    "print(\"Cantidad de datos Entrenamiento: \",len(X_t))\n",
    "print(\"Cantidad de datos Pruebas: \",len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### CIFAR-10\n",
    "---\n",
    "Imágenes RGB pequeñas de  32x32 píxeles, de fotos naturales de distintos objetos.\n",
    "\n",
    "|Tipo set|Datos|Label|\n",
    "|---|---|---|\n",
    "|Entrenamiento|50000|10|\n",
    "|Pruebas|10000|10|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 512)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_dat = \"CIFAR-10\"\n",
    "\n",
    "(_, aux_t), (_, aux_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "labels = [\"airplane\", \"automobile\",\"bird\", \"cat\",\"deer\",\"dog\",\"frog\",\"horse\",\"ship\",\"truck\"]\n",
    "labels_t = np.asarray([labels[value[0]] for value in aux_t])\n",
    "labels_test = np.asarray([labels[value[0]] for value in aux_test])\n",
    "labels_t = np.concatenate((labels_t,labels_test),axis=0)\n",
    "\n",
    "#X_t = np.load(\"./Data/VGG_CNN/cifar10_VGG_avg.npy\")\n",
    "#X_t = np.load(\"./Data/VGG_64/cifar10_VGG_avg.npy\") #mejora\n",
    "\n",
    "#X_t = np.load(\"../AUX/VGG_128/cifar10_VGG_avg.npy\") #mejora\n",
    "X_t = np.load(\"../AUX/VGG_224/cifar10_VGG_avg.npy\") #mejora\n",
    "\n",
    "X_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de datos Entrenamiento:  59000\n",
      "Cantidad de datos Pruebas:  1000\n"
     ]
    }
   ],
   "source": [
    "##### 1000 imagenes de prueba... \n",
    "from utils import sample_test_mask\n",
    "mask_train = sample_test_mask(labels_t, N=100, multi_label=False)\n",
    "\n",
    "## creat test como dicen...\n",
    "X_test = X_t[~mask_train]\n",
    "X_t = X_t[mask_train]\n",
    "labels_test = enmask_data(labels_t, ~mask_train)\n",
    "labels_t = enmask_data(labels_t, mask_train)\n",
    "\n",
    "gc.collect()\n",
    "print(\"Cantidad de datos Entrenamiento: \",len(X_t))\n",
    "print(\"Cantidad de datos Pruebas: \",len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NUSWIDE\n",
    "---\n",
    "* Cantidad de datos totales: 269648\n",
    "* Datos utlizados (con imagenes disponibles a descargar): 169500\n",
    "* Datos top-21 clases: 158383\n",
    "\n",
    "Imágenes de eventos con 81 tópicos asociados, re-dimensionadas a 64x64.\n",
    "\n",
    "|Tipo set|Datos|Label|\n",
    "|---|---|---|\n",
    "|Entrenamiento|xxx|81|\n",
    "|Validación|xxx|81|\n",
    "|Pruebas|xxx|81|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de labels:  81\n",
      "Cantidad de objetos:  169500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['person'], ['person'], ['person'], ['person'], ['person']]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_dat = \"Nus-Wide\"\n",
    "\n",
    "folder = \"../Dataset_NUSWIDE/\"\n",
    "\n",
    "mask_av = np.loadtxt(\"./Data/\"+name_dat+\"_mask_avail.txt\").astype(bool)\n",
    "\n",
    "labels = pd.read_csv(folder+'Concepts81.txt',header=None).values.reshape(1,-1)[0]\n",
    "print(\"Cantidad de labels: \",len(labels) )\n",
    "\n",
    "labels_t = [[] for _ in range(269648)]\n",
    "for concept in labels:\n",
    "    aux = pd.read_csv(folder+\"Groundtruth/AllLabels/Labels_\"+concept+\".txt\",header=None)\n",
    "    indexs_true = aux.loc[(aux==1).values[:,0]].index\n",
    "    \n",
    "    for value in indexs_true:\n",
    "        labels_t[value].append(concept)\n",
    "        \n",
    "labels_t = enmask_data(labels_t, mask_av)\n",
    "N_total = len(labels_t)\n",
    "print(\"Cantidad de objetos: \",N_total )\n",
    "\n",
    "labels_t[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get top-K labels data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category with most data (sky) has = 61066, the top-K category (mountain) has = 4232\n",
      "Cantidad de objetos:  158383\n"
     ]
    }
   ],
   "source": [
    "new_labels = get_topK_labels(labels_t, labels, K=21)\n",
    "\n",
    "labels_t = set_newlabel_list(new_labels, labels_t)\n",
    "labels = new_labels\n",
    "# y si quedan datos sin clase?\n",
    "mask_used_t = np.asarray(list(map(len,labels_t))) != 0\n",
    "\n",
    "labels_t = enmask_data(labels_t, mask_used_t)\n",
    "print(\"Cantidad de objetos: \", len(labels_t) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N_used = 80*1000 #o 150k??\n",
    "idx_all = np.arange(0, N_total)\n",
    "mask_used = np.zeros(N_total, dtype=bool)\n",
    "mask_used[np.random.choice(np.arange(0, N_total), size=N_used, replace=False)] = 1\n",
    "mask_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(158383, 512)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_t = np.load(\"../AUX/VGG_224/nuswide_VGG_avg.npy\")\n",
    "X_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de datos Entrenamiento:  156283\n",
      "Cantidad de datos Pruebas:  2100\n"
     ]
    }
   ],
   "source": [
    "from utils import sample_test_mask\n",
    "mask_train = sample_test_mask(labels_t, N=100)\n",
    "\n",
    "## creat test como dicen...\n",
    "X_test = X_t[~mask_train]\n",
    "X_t = X_t[mask_train]\n",
    "labels_test = enmask_data(labels_t, ~mask_train)\n",
    "labels_t = enmask_data(labels_t, mask_train)\n",
    "\n",
    "gc.collect()\n",
    "print(\"Cantidad de datos Entrenamiento: \",len(X_t))\n",
    "print(\"Cantidad de datos Pruebas: \",len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation creation\n",
    "\n",
    "Pre-process: División por 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_t = X_t.astype(\"float32\")/255\n",
    "X_test = X_test.astype(\"float32\")/255\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.20899148, -0.17588514, -0.38836133, ..., -0.44424307,\n",
       "        -0.80652124,  0.78794557],\n",
       "       [-0.20899148, -0.17588514,  4.3881245 , ..., -0.5044999 ,\n",
       "        -0.9952953 , -0.48293716],\n",
       "       [-0.20899148, -0.17588514, -0.38836133, ..., -0.5044999 ,\n",
       "        -1.1586936 , -0.48293716],\n",
       "       ...,\n",
       "       [-0.20899148, -0.17588514, -0.13085562, ..., -0.5044999 ,\n",
       "        -0.5196408 ,  1.589052  ],\n",
       "       [-0.01284289, -0.17588514,  0.4630181 , ..., -0.5044999 ,\n",
       "        -0.26463884, -0.48293716],\n",
       "       [-0.20899148, -0.17588514, -0.38836133, ...,  1.6912041 ,\n",
       "        -0.02900592, -0.31895235]], dtype=float32)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "std = StandardScaler(with_mean=True, with_std=True)\n",
    "std.fit(X_t)\n",
    "\n",
    "X_t = std.transform(X_t)\n",
    "X_test = std.transform(X_test)\n",
    "X_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de datos Entrenamiento:  68000\n",
      "Cantidad de datos Validación:  1000\n",
      "Cantidad de datos Pruebas:  1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, labels_train, labels_val  = train_test_split(X_t, labels_t, random_state=20,test_size=len(X_test))\n",
    "\n",
    "print(\"Cantidad de datos Entrenamiento: \",len(X_train))\n",
    "print(\"Cantidad de datos Validación: \",len(X_val))\n",
    "print(\"Cantidad de datos Pruebas: \",len(X_test))\n",
    "del X_t, labels_t\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_input = X_train\n",
    "X_val_input = X_val\n",
    "X_test_input = X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "---\n",
    "CNN\n",
    "https://github.com/rtflynn/Cifar-Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def REC_loss(x_true, x_pred):\n",
    "    if raw:\n",
    "        return K.mean( K.binary_crossentropy(x_true, x_pred), axis=-1)\n",
    "    else:\n",
    "        return K.mean( (x_true- x_pred)**2 ,axis=-1)  #it should be VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def traditional_VAE(data_dim,Nb,units,layers_e,layers_d,opt='adam',BN=True, beta=0, summ=True):\n",
    "    pre_encoder = define_pre_encoder(data_dim, layers=layers_e,units=units,BN=BN)\n",
    "    if raw:\n",
    "        generator = define_generator(Nb,data_dim,layers=layers_d,units=units,BN=BN, out_type='sigmoid')\n",
    "    else:\n",
    "        generator = define_generator(Nb,data_dim,layers=layers_d,units=units,BN=BN, out_type='linear')    \n",
    "    if summ:\n",
    "        print(\"pre-encoder network:\")\n",
    "        pre_encoder.summary()\n",
    "        print(\"generator network:\")\n",
    "        generator.summary()\n",
    "    ## Encoder\n",
    "    x = Input(shape=(data_dim,))\n",
    "    hidden = pre_encoder(x)\n",
    "    z_mean = Dense(Nb,activation='linear', name='z-mean')(hidden)\n",
    "    z_log_var = Dense(Nb,activation='linear',name = 'z-log_var')(hidden)\n",
    "    encoder = Model(x, z_mean) # build a model to project inputs on the latent space\n",
    "\n",
    "    def sampling(args):\n",
    "        epsilon_std = 1.0\n",
    "        z_mean, z_log_var = args\n",
    "        epsilon = K.random_normal(shape=(K.shape(z_mean)[0], Nb),mean=0., stddev=epsilon_std)\n",
    "        return z_mean + K.exp(0.5*z_log_var) * epsilon #+sigma (desvest)\n",
    "    \n",
    "    ## Decoder\n",
    "    z_sampled = Lambda(sampling, output_shape=(Nb,), name='sampled')([z_mean, z_log_var])\n",
    "    output = generator(z_sampled)\n",
    "    \n",
    "    Recon_loss = REC_loss\n",
    "    kl_loss = KL_loss(z_mean,z_log_var)\n",
    "    def VAE_loss(y_true, y_pred): \n",
    "        return Recon_loss(y_true, y_pred) + beta*kl_loss(y_true, y_pred)\n",
    "\n",
    "    traditional_vae = Model(x, output)\n",
    "    traditional_vae.compile(optimizer=opt, loss=VAE_loss, metrics = [Recon_loss,kl_loss])\n",
    "    \n",
    "    return traditional_vae, encoder,generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "def sample_gumbel(shape,eps=K.epsilon()):\n",
    "    \"\"\"Inverse Sample function from Gumbel(0, 1)\"\"\"\n",
    "    U = K.random_uniform(shape, 0, 1)\n",
    "    return K.log(U + eps)- K.log(1-U + eps)\n",
    "\n",
    "def binary_VAE(data_dim,Nb,units,layers_e,layers_d,opt='adam',BN=True,beta=0., summ=True):\n",
    "    tau = K.variable(0.67, name=\"temperature\") #o tau fijo en 0.67=2/3\n",
    "    \n",
    "    pre_encoder = define_pre_encoder(data_dim, layers=layers_e,units=units,BN=BN)\n",
    "    if raw:\n",
    "        generator = define_generator(Nb,data_dim,layers=layers_d,units=units,BN=BN, out_type='sigmoid')\n",
    "    else:\n",
    "        generator = define_generator(Nb,data_dim,layers=layers_d,units=units,BN=BN, out_type='linear')\n",
    "    if summ:\n",
    "        print(\"pre-encoder network:\")\n",
    "        pre_encoder.summary()\n",
    "        print(\"generator network:\")\n",
    "        generator.summary()\n",
    "\n",
    "    x = Input(shape=(data_dim,))\n",
    "    hidden = pre_encoder(x)\n",
    "    logits_b  = Dense(Nb, activation='linear', name='logits-b')(hidden) #log(B_j/1-B_j)\n",
    "    #proba = np.exp(logits_b)/(1+np.exp(logits_b)) = sigmoidal(logits_b) <<<<<<<<<< recupera probabilidad\n",
    "    #dist = Dense(Nb, activation='sigmoid')(hidden) #p(b) #otra forma de modelarlo\n",
    "    encoder = Model(x, logits_b)\n",
    "\n",
    "    def sampling(logits_b):\n",
    "        #logits_b = K.log(aux/(1-aux) + K.epsilon() )\n",
    "        b = logits_b + sample_gumbel(K.shape(logits_b)) # logits + gumbel noise\n",
    "        return keras.activations.sigmoid( b/tau )\n",
    "\n",
    "    b_sampled = Lambda(sampling, output_shape=(Nb,), name='sampled')(logits_b)\n",
    "    output = generator(b_sampled)\n",
    "        \n",
    "    Recon_loss = REC_loss\n",
    "    kl_loss = BKL_loss(logits_b)\n",
    "    def BVAE_loss(y_true, y_pred): \n",
    "        return Recon_loss(y_true, y_pred) + beta*kl_loss(y_true, y_pred)\n",
    "\n",
    "    binary_vae = Model(x, output)\n",
    "    binary_vae.compile(optimizer=opt, loss=BVAE_loss, metrics = [Recon_loss,kl_loss])\n",
    "    return binary_vae, encoder,generator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train details\n",
    "---\n",
    "\n",
    "* 30* epochs* \n",
    "* *batch size* de 200\n",
    "* optimizador Adam\n",
    "* Inicializador de Glorot (para los pesos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import  compare_hist_train, add_hist_plot\n",
    "\n",
    "batch_size = 100*2 #ya que son datasets mas grandes\n",
    "epochs = 30 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import find_beta\n",
    "\n",
    "def create_model_T(beta_V):\n",
    "    return traditional_VAE(X_train_input.shape[1],Nb=32,units=500,layers_e=2,layers_d=0\n",
    "                                                                  ,beta=beta_V, summ=False)\n",
    "def create_model_B(beta_V):\n",
    "    return binary_VAE(X_train_input.shape[1],Nb=32,units=500,layers_e=2,layers_d=0\n",
    "                                                                  ,beta=beta_V, summ=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************************\n",
      "*********** SUMMARY RESULTS ***********\n",
      "***************************************\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beta</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.4453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.4477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.4980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.6048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.6385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.6847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.7094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.7439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.7614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.7635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.7643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000488</td>\n",
       "      <td>0.7687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000244</td>\n",
       "      <td>0.7647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.7697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.7708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.7692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.7728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.7786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.7776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.7689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        beta   score\n",
       "0   1.000000  0.4453\n",
       "1   0.500000  0.4477\n",
       "2   0.250000  0.4980\n",
       "3   0.125000  0.6048\n",
       "4   0.062500  0.6385\n",
       "5   0.031250  0.6847\n",
       "6   0.015625  0.7094\n",
       "7   0.007812  0.7439\n",
       "8   0.003906  0.7614\n",
       "9   0.001953  0.7635\n",
       "10  0.000977  0.7643\n",
       "11  0.000488  0.7687\n",
       "12  0.000244  0.7647\n",
       "13  0.000122  0.7697\n",
       "14  0.000061  0.7708\n",
       "15  0.000031  0.7692\n",
       "16  0.000015  0.7728\n",
       "17  0.000008  0.7786\n",
       "18  0.000004  0.7776\n",
       "19  0.000002  0.7689"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best value is 0.7786 with beta 0.000008\n",
      "Worst value is 0.4453 with beta 1.000000\n",
      "***************************************\n",
      "***************************************\n",
      "*********** SUMMARY RESULTS ***********\n",
      "***************************************\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beta</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.2422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.3660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.4665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.5928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.4887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.6490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.7261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.7920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.8151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.8279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.8284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000488</td>\n",
       "      <td>0.8232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000244</td>\n",
       "      <td>0.8251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.8285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.8249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.8254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.8310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.8250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.8301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.8258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        beta   score\n",
       "0   1.000000  0.2422\n",
       "1   0.500000  0.3660\n",
       "2   0.250000  0.4665\n",
       "3   0.125000  0.5928\n",
       "4   0.062500  0.4887\n",
       "5   0.031250  0.6490\n",
       "6   0.015625  0.7261\n",
       "7   0.007812  0.7920\n",
       "8   0.003906  0.8151\n",
       "9   0.001953  0.8279\n",
       "10  0.000977  0.8284\n",
       "11  0.000488  0.8232\n",
       "12  0.000244  0.8251\n",
       "13  0.000122  0.8285\n",
       "14  0.000061  0.8249\n",
       "15  0.000031  0.8254\n",
       "16  0.000015  0.8310\n",
       "17  0.000008  0.8250\n",
       "18  0.000004  0.8301\n",
       "19  0.000002  0.8258"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best value is 0.8309 with beta 0.000015\n",
      "Worst value is 0.2422 with beta 1.000000\n",
      "***************************************\n"
     ]
    }
   ],
   "source": [
    "beta_T = find_beta(create_model_T, X_train_input, X_train, X_val_input,labels_train,labels_val,\n",
    "                   binary=False, BS=batch_size)\n",
    "beta_B = find_beta(create_model_B, X_train_input, X_train, X_val_input,labels_train,labels_val,\n",
    "                   binary=True, BS=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## para mnist raw\n",
    "beta_T = 0.000015\n",
    "beta_B = 0.000122"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### PARA MNIST-vGG!!\n",
    "beta_T = 0.000008\n",
    "beta_B = 0.000015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### PARA CIFAR VGG224!!\n",
    "beta_T = 0.000008\n",
    "beta_B = 0.003906"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### PARA NUSWIDE!!\n",
    "beta_T = 0.000008\n",
    "beta_B = 0.001953"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### falta celebA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_total = np.concatenate((X_train,X_val),axis=0)\n",
    "X_total_input = X_total\n",
    "labels_total = np.concatenate((labels_train,labels_val),axis=0)\n",
    "del X_train, X_train_input, X_val, X_val_input, labels_train,labels_val\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-encoder network:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 500)               256500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 511,000\n",
      "Trainable params: 509,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 512)               16896     \n",
      "=================================================================\n",
      "Total params: 16,896\n",
      "Trainable params: 16,896\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 156283 samples, validate on 2100 samples\n",
      "Epoch 1/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.7068 - REC_loss: 0.7068 - KL: 6.5171 - val_loss: 0.4535 - val_REC_loss: 0.4534 - val_KL: 8.0752\n",
      "Epoch 2/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.5846 - REC_loss: 0.5845 - KL: 9.4558 - val_loss: 0.4385 - val_REC_loss: 0.4385 - val_KL: 7.6242\n",
      "Epoch 3/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.5730 - REC_loss: 0.5729 - KL: 8.4606 - val_loss: 0.4330 - val_REC_loss: 0.4330 - val_KL: 6.9632\n",
      "Epoch 4/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.5687 - REC_loss: 0.5686 - KL: 7.6793 - val_loss: 0.4327 - val_REC_loss: 0.4326 - val_KL: 6.6215\n",
      "Epoch 5/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.5664 - REC_loss: 0.5663 - KL: 7.1466 - val_loss: 0.4300 - val_REC_loss: 0.4299 - val_KL: 6.0842\n",
      "Epoch 6/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.5648 - REC_loss: 0.5647 - KL: 6.7519 - val_loss: 0.4295 - val_REC_loss: 0.4295 - val_KL: 5.9594\n",
      "Epoch 7/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.5639 - REC_loss: 0.5638 - KL: 6.4502 - val_loss: 0.4291 - val_REC_loss: 0.4291 - val_KL: 5.7179\n",
      "Epoch 8/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.5631 - REC_loss: 0.5630 - KL: 6.2187 - val_loss: 0.4279 - val_REC_loss: 0.4279 - val_KL: 5.5870\n",
      "Epoch 9/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.5624 - REC_loss: 0.5623 - KL: 6.0655 - val_loss: 0.4275 - val_REC_loss: 0.4275 - val_KL: 5.5636\n",
      "Epoch 10/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.5620 - REC_loss: 0.5619 - KL: 5.9436 - val_loss: 0.4274 - val_REC_loss: 0.4274 - val_KL: 5.4688\n",
      "Epoch 11/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.5616 - REC_loss: 0.5615 - KL: 5.8733 - val_loss: 0.4265 - val_REC_loss: 0.4265 - val_KL: 5.4825\n",
      "Epoch 12/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.5613 - REC_loss: 0.5612 - KL: 5.8245 - val_loss: 0.4272 - val_REC_loss: 0.4272 - val_KL: 5.5003\n",
      "Epoch 13/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.5610 - REC_loss: 0.5609 - KL: 5.8111 - val_loss: 0.4279 - val_REC_loss: 0.4278 - val_KL: 5.5487\n",
      "Epoch 14/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.5607 - REC_loss: 0.5607 - KL: 5.8229 - val_loss: 0.4269 - val_REC_loss: 0.4269 - val_KL: 5.5492\n",
      "Epoch 15/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.5605 - REC_loss: 0.5604 - KL: 5.8284 - val_loss: 0.4261 - val_REC_loss: 0.4261 - val_KL: 5.5626\n",
      "Epoch 16/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.5603 - REC_loss: 0.5602 - KL: 5.8267 - val_loss: 0.4261 - val_REC_loss: 0.4261 - val_KL: 5.5749\n",
      "Epoch 17/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.5602 - REC_loss: 0.5601 - KL: 5.8227 - val_loss: 0.4262 - val_REC_loss: 0.4262 - val_KL: 5.5530\n",
      "Epoch 18/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.5600 - REC_loss: 0.5600 - KL: 5.8091 - val_loss: 0.4261 - val_REC_loss: 0.4261 - val_KL: 5.5319\n",
      "Epoch 19/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.5598 - REC_loss: 0.5597 - KL: 5.8183 - val_loss: 0.4263 - val_REC_loss: 0.4263 - val_KL: 5.5972\n",
      "Epoch 20/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.5597 - REC_loss: 0.5597 - KL: 5.7911 - val_loss: 0.4261 - val_REC_loss: 0.4260 - val_KL: 5.5928\n",
      "Epoch 21/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.5596 - REC_loss: 0.5595 - KL: 5.7913 - val_loss: 0.4264 - val_REC_loss: 0.4264 - val_KL: 5.6061\n",
      "Epoch 22/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.5594 - REC_loss: 0.5594 - KL: 5.7636 - val_loss: 0.4261 - val_REC_loss: 0.4260 - val_KL: 5.5940\n",
      "Epoch 23/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.5594 - REC_loss: 0.5593 - KL: 5.7438 - val_loss: 0.4258 - val_REC_loss: 0.4258 - val_KL: 5.5660\n",
      "Epoch 24/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.5594 - REC_loss: 0.5593 - KL: 5.7141 - val_loss: 0.4258 - val_REC_loss: 0.4257 - val_KL: 5.5433\n",
      "Epoch 25/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.5592 - REC_loss: 0.5591 - KL: 5.7314 - val_loss: 0.4258 - val_REC_loss: 0.4257 - val_KL: 5.5168\n",
      "Epoch 26/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.5591 - REC_loss: 0.5591 - KL: 5.7023 - val_loss: 0.4255 - val_REC_loss: 0.4255 - val_KL: 5.5251\n",
      "Epoch 27/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.5590 - REC_loss: 0.5589 - KL: 5.6716 - val_loss: 0.4259 - val_REC_loss: 0.4258 - val_KL: 5.4933\n",
      "Epoch 28/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.5589 - REC_loss: 0.5588 - KL: 5.6372 - val_loss: 0.4255 - val_REC_loss: 0.4254 - val_KL: 5.4374\n",
      "Epoch 29/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.5589 - REC_loss: 0.5588 - KL: 5.6003 - val_loss: 0.4257 - val_REC_loss: 0.4256 - val_KL: 5.4599\n",
      "Epoch 30/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.5588 - REC_loss: 0.5588 - KL: 5.5881 - val_loss: 0.4260 - val_REC_loss: 0.4259 - val_KL: 5.4529\n"
     ]
    }
   ],
   "source": [
    "traditional_vae,encoder_Tvae,generator_Tvae = traditional_VAE(X_total_input.shape[1],Nb=32,units=500,layers_e=2,layers_d=0,beta=beta_T)\n",
    "\n",
    "hist1 = traditional_vae.fit(X_total_input, X_total, epochs=epochs, batch_size=batch_size\n",
    "                           ,validation_data=(X_test_input,X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-encoder network:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_14 (InputLayer)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 500)               256500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 511,000\n",
      "Trainable params: 509,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_15 (InputLayer)        (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 512)               16896     \n",
      "=================================================================\n",
      "Total params: 16,896\n",
      "Trainable params: 16,896\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 156283 samples, validate on 2100 samples\n",
      "Epoch 1/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.7726 - REC_loss: 0.7716 - KL: 0.5316 - val_loss: 0.5399 - val_REC_loss: 0.5388 - val_KL: 0.5641\n",
      "Epoch 2/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.7064 - REC_loss: 0.7053 - KL: 0.5611 - val_loss: 0.5315 - val_REC_loss: 0.5304 - val_KL: 0.5673\n",
      "Epoch 3/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.6976 - REC_loss: 0.6965 - KL: 0.5649 - val_loss: 0.5278 - val_REC_loss: 0.5267 - val_KL: 0.5713\n",
      "Epoch 4/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.6937 - REC_loss: 0.6926 - KL: 0.5661 - val_loss: 0.5266 - val_REC_loss: 0.5254 - val_KL: 0.5740\n",
      "Epoch 5/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.6914 - REC_loss: 0.6903 - KL: 0.5671 - val_loss: 0.5252 - val_REC_loss: 0.5241 - val_KL: 0.5755\n",
      "Epoch 6/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.6896 - REC_loss: 0.6885 - KL: 0.5684 - val_loss: 0.5243 - val_REC_loss: 0.5232 - val_KL: 0.5782\n",
      "Epoch 7/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.6881 - REC_loss: 0.6870 - KL: 0.5699 - val_loss: 0.5234 - val_REC_loss: 0.5222 - val_KL: 0.5797\n",
      "Epoch 8/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.6870 - REC_loss: 0.6858 - KL: 0.5710 - val_loss: 0.5228 - val_REC_loss: 0.5216 - val_KL: 0.5811\n",
      "Epoch 9/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.6859 - REC_loss: 0.6848 - KL: 0.5719 - val_loss: 0.5222 - val_REC_loss: 0.5211 - val_KL: 0.5801\n",
      "Epoch 10/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.6851 - REC_loss: 0.6840 - KL: 0.5727 - val_loss: 0.5210 - val_REC_loss: 0.5198 - val_KL: 0.5840\n",
      "Epoch 11/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.6843 - REC_loss: 0.6832 - KL: 0.5732 - val_loss: 0.5213 - val_REC_loss: 0.5202 - val_KL: 0.5837\n",
      "Epoch 12/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.6836 - REC_loss: 0.6825 - KL: 0.5742 - val_loss: 0.5209 - val_REC_loss: 0.5198 - val_KL: 0.5817\n",
      "Epoch 13/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.6830 - REC_loss: 0.6819 - KL: 0.5748 - val_loss: 0.5197 - val_REC_loss: 0.5185 - val_KL: 0.5835\n",
      "Epoch 14/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.6824 - REC_loss: 0.6813 - KL: 0.5757 - val_loss: 0.5196 - val_REC_loss: 0.5184 - val_KL: 0.5831\n",
      "Epoch 15/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.6821 - REC_loss: 0.6809 - KL: 0.5761 - val_loss: 0.5194 - val_REC_loss: 0.5182 - val_KL: 0.5851\n",
      "Epoch 16/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.6816 - REC_loss: 0.6804 - KL: 0.5772 - val_loss: 0.5189 - val_REC_loss: 0.5177 - val_KL: 0.5862\n",
      "Epoch 17/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.6811 - REC_loss: 0.6800 - KL: 0.5774 - val_loss: 0.5186 - val_REC_loss: 0.5174 - val_KL: 0.5857\n",
      "Epoch 18/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.6807 - REC_loss: 0.6796 - KL: 0.5781 - val_loss: 0.5183 - val_REC_loss: 0.5172 - val_KL: 0.5872\n",
      "Epoch 19/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.6804 - REC_loss: 0.6792 - KL: 0.5791 - val_loss: 0.5177 - val_REC_loss: 0.5165 - val_KL: 0.5882\n",
      "Epoch 20/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.6800 - REC_loss: 0.6789 - KL: 0.5794 - val_loss: 0.5182 - val_REC_loss: 0.5171 - val_KL: 0.5890\n",
      "Epoch 21/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.6796 - REC_loss: 0.6785 - KL: 0.5799 - val_loss: 0.5171 - val_REC_loss: 0.5159 - val_KL: 0.5885\n",
      "Epoch 22/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.6795 - REC_loss: 0.6783 - KL: 0.5808 - val_loss: 0.5173 - val_REC_loss: 0.5161 - val_KL: 0.5883\n",
      "Epoch 23/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.6793 - REC_loss: 0.6781 - KL: 0.5811 - val_loss: 0.5173 - val_REC_loss: 0.5162 - val_KL: 0.5895\n",
      "Epoch 24/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.6790 - REC_loss: 0.6778 - KL: 0.5818 - val_loss: 0.5172 - val_REC_loss: 0.5161 - val_KL: 0.5909\n",
      "Epoch 25/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.6787 - REC_loss: 0.6775 - KL: 0.5820 - val_loss: 0.5170 - val_REC_loss: 0.5159 - val_KL: 0.5893\n",
      "Epoch 26/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.6785 - REC_loss: 0.6774 - KL: 0.5822 - val_loss: 0.5169 - val_REC_loss: 0.5157 - val_KL: 0.5902\n",
      "Epoch 27/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.6783 - REC_loss: 0.6771 - KL: 0.5828 - val_loss: 0.5165 - val_REC_loss: 0.5154 - val_KL: 0.5902\n",
      "Epoch 28/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.6781 - REC_loss: 0.6770 - KL: 0.5834 - val_loss: 0.5165 - val_REC_loss: 0.5154 - val_KL: 0.5908\n",
      "Epoch 29/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.6779 - REC_loss: 0.6767 - KL: 0.5835 - val_loss: 0.5171 - val_REC_loss: 0.5160 - val_KL: 0.5918\n",
      "Epoch 30/30\n",
      "156283/156283 [==============================] - 2s - loss: 0.6778 - REC_loss: 0.6767 - KL: 0.5836 - val_loss: 0.5162 - val_REC_loss: 0.5150 - val_KL: 0.5906\n"
     ]
    }
   ],
   "source": [
    "binary_vae,encoder_Bvae,generator_Bvae= binary_VAE(X_total_input.shape[1],Nb=32,units=500,layers_e=2,layers_d=0,beta=beta_B)\n",
    "\n",
    "hist2 = binary_vae.fit(X_total_input, X_total, epochs=epochs, batch_size=batch_size\n",
    "                           ,validation_data=(X_test_input,X_test) )\n",
    "                       #,callbacks=[Tau_Call(tau)] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another intrinsic measure: *Classification*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#function to define and train model\n",
    "from sklearn.metrics import jaccard_score\n",
    "from utils import define_fit, MedianHashing, visualize_probas, visualize_mean, calculate_hash, visualize_probas_byB\n",
    "\n",
    "results = []\n",
    "results_S = []\n",
    "results_B = []\n",
    "results_O_B = [] #original testing on tresholded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#codify input data (binarize -- or aprox)\n",
    "X_train_logits = encoder_Bvae.predict(X_total_input)\n",
    "X_val_logits = encoder_Bvae.predict(X_test_input)\n",
    "\n",
    "#probabilities\n",
    "X_train_Bcode = expit(X_train_logits)\n",
    "X_val_Bcode = expit(X_val_logits)\n",
    "\n",
    "#Z-mean\n",
    "X_train_Tcode = encoder_Tvae.predict(X_total_input)\n",
    "X_val_Tcode = encoder_Tvae.predict(X_test_input)\n",
    "\n",
    "##codify labels\n",
    "labels_aux = np.asarray(labels)\n",
    "def codify_labels(inputs):\n",
    "    inputs = np.asarray(inputs)\n",
    "    matrix_labels = np.zeros((inputs.shape[0],labels_aux.shape[0]))\n",
    "    for i,aux_labels in enumerate(inputs):\n",
    "        if type(aux_labels) == list or type(aux_labels) == np.ndarray :\n",
    "            for aux_label in aux_labels:\n",
    "                idx = np.where(aux_label==labels_aux)[0]\n",
    "                matrix_labels[i,idx] = 1 #various-multiple\n",
    "        else:\n",
    "            idx = np.where(aux_labels==labels_aux)[0]\n",
    "            matrix_labels[i,idx] = 1 #only one\n",
    "    return matrix_labels\n",
    "\n",
    "C_train = codify_labels(labels_total)\n",
    "#C_val = codify_labels(labels_val)\n",
    "C_val = codify_labels(labels_test)\n",
    "C_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "aux = [name_dat.lower()]\n",
    "multi_label = \"celeba\" in aux or \"nus-wide\" in aux\n",
    "\n",
    "model1_O = define_fit(multi_label,X_train_Bcode,C_train)\n",
    "model2_O = define_fit(multi_label,X_train_Tcode,C_train)\n",
    "\n",
    "if not multi_label:\n",
    "    aux.append(model2_O.evaluate(X_train_Tcode,C_train,verbose=0)[1])\n",
    "    aux.append(model2_O.evaluate(X_val_Tcode,C_val,verbose=0)[1])\n",
    "\n",
    "    aux.append(model1_O.evaluate(X_train_Bcode,C_train,verbose=0)[1])\n",
    "    aux.append(model1_O.evaluate(X_val_Bcode,C_val,verbose=0)[1])\n",
    "else:\n",
    "    aux.append(jaccard_score(C_train, (model2_O.predict(X_train_Tcode)>=0.5)*1, average='micro'))\n",
    "    aux.append(jaccard_score(C_val, (model2_O.predict(X_val_Tcode)>=0.5)*1, average='micro'))\n",
    "\n",
    "    aux.append(jaccard_score(C_train, (model1_O.predict(X_train_Bcode)>=0.5)*1, average='micro'))\n",
    "    aux.append(jaccard_score(C_val, (model1_O.predict(X_val_Bcode)>=0.5)*1, average='micro'))\n",
    "    \n",
    "results.append(aux)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on dataset:  mnist-raw\n",
      "Traditional VAE (train-val): 0.995377 - 0.978000\n",
      "Binary VAE (train-val): 0.967406 - 0.967000\n",
      "\n",
      "Accuracy on dataset:  mnist\n",
      "Traditional VAE (train-val): 0.979420 - 0.954000\n",
      "Binary VAE (train-val): 0.936638 - 0.925000\n",
      "\n",
      "Accuracy on dataset:  cifar-10\n",
      "Traditional VAE (train-val): 0.876458 - 0.849000\n",
      "Binary VAE (train-val): 0.788780 - 0.771000\n",
      "\n",
      "Accuracy on dataset:  nus-wide\n",
      "Traditional VAE (train-val): 0.535764 - 0.490194\n",
      "Binary VAE (train-val): 0.482383 - 0.416436\n"
     ]
    }
   ],
   "source": [
    "for valores in results:\n",
    "    print(\"\\nAccuracy on dataset: \",valores[0])\n",
    "    print(\"Traditional VAE (train-val): %f - %f\"%(valores[1],valores[2]))\n",
    "    print(\"Binary VAE (train-val): %f - %f\"%(valores[3],valores[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, ..., 0, 1, 1],\n",
       "       [1, 0, 0, ..., 1, 1, 0],\n",
       "       [1, 1, 1, ..., 1, 1, 0],\n",
       "       ...,\n",
       "       [0, 1, 0, ..., 0, 1, 1],\n",
       "       [1, 1, 1, ..., 1, 1, 1],\n",
       "       [1, 0, 1, ..., 0, 1, 1]], dtype=int32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## binary // treshold it\n",
    "\n",
    "X_train_Tcode_B = calculate_hash(X_train_Tcode, from_probas=False)\n",
    "X_val_Tcode_B = calculate_hash(X_val_Tcode, from_probas=False)\n",
    "\n",
    "X_train_Bcode_B = calculate_hash(X_train_Bcode, from_probas=True, from_logits=False)\n",
    "X_val_Bcode_B = calculate_hash(X_val_Bcode, from_probas=True, from_logits=False)\n",
    "\n",
    "X_train_Tcode_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "### trained models over encoder representation testing on tresholded representation (see how decrease)\n",
    "aux = [name_dat.lower()]\n",
    "multi_label = \"celeba\" in aux or \"nus-wide\" in aux\n",
    "\n",
    "if not multi_label:\n",
    "    aux.append(model2_O.evaluate(X_train_Tcode_B,C_train,verbose=0)[1])\n",
    "    aux.append(model2_O.evaluate(X_val_Tcode_B,C_val,verbose=0)[1])\n",
    "\n",
    "    aux.append(model1_O.evaluate(X_train_Bcode_B,C_train,verbose=0)[1])\n",
    "    aux.append(model1_O.evaluate(X_val_Bcode_B,C_val,verbose=0)[1])\n",
    "else:\n",
    "    aux.append(jaccard_score(C_train, (model2_O.predict(X_train_Tcode_B)>=0.5)*1, average='micro'))\n",
    "    aux.append(jaccard_score(C_val, (model2_O.predict(X_val_Tcode_B)>=0.5)*1, average='micro'))\n",
    "\n",
    "    aux.append(jaccard_score(C_train, (model1_O.predict(X_train_Bcode_B)>=0.5)*1, average='micro'))\n",
    "    aux.append(jaccard_score(C_val, (model1_O.predict(X_val_Bcode_B)>=0.5)*1, average='micro'))\n",
    "    \n",
    "results_O_B.append(aux)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification on Binary (thresholded) representation\n",
      "\n",
      "Accuracy on dataset:  mnist-raw\n",
      "Traditional VAE (train-val): 0.328899 - 0.309000\n",
      "Binary VAE (train-val): 0.942159 - 0.952000\n",
      "\n",
      "Accuracy on dataset:  mnist\n",
      "Traditional VAE (train-val): 0.499493 - 0.515000\n",
      "Binary VAE (train-val): 0.907072 - 0.905000\n",
      "\n",
      "Accuracy on dataset:  cifar-10\n",
      "Traditional VAE (train-val): 0.440576 - 0.428000\n",
      "Binary VAE (train-val): 0.743051 - 0.738000\n",
      "\n",
      "Accuracy on dataset:  nus-wide\n",
      "Traditional VAE (train-val): 0.305500 - 0.282347\n",
      "Binary VAE (train-val): 0.452593 - 0.387423\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification on Binary (thresholded) representation\")\n",
    "for valores in results_O_B:\n",
    "    print(\"\\nAccuracy on dataset: \",valores[0])\n",
    "    print(\"Traditional VAE (train-val): %f - %f\"%(valores[1],valores[2]))\n",
    "    print(\"Binary VAE (train-val): %f - %f\"%(valores[3],valores[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "aux = [name_dat.lower()]\n",
    "multi_label = \"celeba\" in aux or \"nus-wide\" in aux\n",
    "\n",
    "model1 = define_fit(multi_label,X_train_Bcode_B,C_train)\n",
    "model2 = define_fit(multi_label,X_train_Tcode_B,C_train)\n",
    "\n",
    "if not multi_label:\n",
    "    aux.append(model2.evaluate(X_train_Tcode_B,C_train,verbose=0)[1])\n",
    "    aux.append(model2.evaluate(X_val_Tcode_B,C_val,verbose=0)[1])\n",
    "\n",
    "    aux.append(model1.evaluate(X_train_Bcode_B,C_train,verbose=0)[1])\n",
    "    aux.append(model1.evaluate(X_val_Bcode_B,C_val,verbose=0)[1])\n",
    "else:\n",
    "    aux.append(jaccard_score(C_train, (model2.predict(X_train_Tcode_B)>=0.5)*1, average='micro')) \n",
    "    aux.append(jaccard_score(C_val, (model2.predict(X_val_Tcode_B)>=0.5)*1, average='micro'))\n",
    "\n",
    "    aux.append(jaccard_score(C_train, (model1.predict(X_train_Bcode_B)>=0.5)*1, average='micro'))\n",
    "    aux.append(jaccard_score(C_val, (model1.predict(X_val_Bcode_B)>=0.5)*1, average='micro'))\n",
    "    \n",
    "results_B.append(aux)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification on Binary representation\n",
      "\n",
      "Accuracy on dataset:  mnist-raw\n",
      "Traditional VAE (train-val): 0.903551 - 0.895000\n",
      "Binary VAE (train-val): 0.950710 - 0.953000\n",
      "\n",
      "Accuracy on dataset:  mnist\n",
      "Traditional VAE (train-val): 0.906261 - 0.883000\n",
      "Binary VAE (train-val): 0.914072 - 0.911000\n",
      "\n",
      "Accuracy on dataset:  cifar-10\n",
      "Traditional VAE (train-val): 0.741797 - 0.730000\n",
      "Binary VAE (train-val): 0.753797 - 0.743000\n",
      "\n",
      "Accuracy on dataset:  nus-wide\n",
      "Traditional VAE (train-val): 0.447286 - 0.386121\n",
      "Binary VAE (train-val): 0.446280 - 0.377084\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification on Binary representation\")\n",
    "for valores in results_B:\n",
    "    print(\"\\nAccuracy on dataset: \",valores[0])\n",
    "    print(\"Traditional VAE (train-val): %f - %f\"%(valores[1],valores[2]))\n",
    "    print(\"Binary VAE (train-val): %f - %f\"%(valores[3],valores[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Results\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils import MedianHashing, get_similar, measure_metrics, calculate_hash\n",
    "from utils import MAP_atk, M_P_atk, AP_atk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentación\n",
    "---\n",
    "A continación se realizan las experimentaciones correspondientes para encontrar la mejor arquitectura y mejor configuración del modelo propuesto en base a las métricas *precision* y *recall* del conjunto de validación en el **top 100** de elementos recuperados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_hashing_DE(train_hash,test_hash,labels_trainn,labels_testt,tipo=\"topK\",eval_tipo='PRatk',K=100):\n",
    "    \"\"\"\n",
    "        Evaluate Hashing correclty: Query and retrieve on a different set\n",
    "    \"\"\"\n",
    "    test_similares_train =  get_similar(test_hash,train_hash,tipo=tipo,K=K)\n",
    "    if eval_tipo==\"MAP\":\n",
    "        return MAP_atk(test_similares_train,labels_query=labels_testt, labels_source=labels_trainn, K=0) #all ranking\n",
    "    elif eval_tipo == \"PRatk\":\n",
    "        return measure_metrics(labels,test_similares_train,labels_testt,labels_source=labels_trainn)\n",
    "    elif eval_tipo == \"Patk\":\n",
    "        return M_P_atk(test_similares_train, labels_query=labels_testt, labels_source=labels_trainn, K=K)\n",
    "\n",
    "def hash_data(model, x_train, x_test, binary=True):\n",
    "    encode_train = model.predict(x_train)\n",
    "    encode_test = model.predict(x_test)\n",
    "    \n",
    "    train_hash = calculate_hash(encode_train, from_probas=binary )\n",
    "    test_hash = calculate_hash(encode_test, from_probas = binary)\n",
    "    return train_hash, test_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Experimentando variando el #Bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "traditional_dat = {\"mnist\":{\"p\":[],\"r\":[]},\"cifar-10\":{\"p\":[],\"r\":[]},\n",
    "                   \"celeba\":{\"p\":[],\"r\":[]},'nus-wide':{\"p\":[],\"r\":[]},\n",
    "                  \"mnist-raw\":{\"p\":[],\"r\":[]}}\n",
    "binary_dat = {\"mnist\":{\"p\":[],\"r\":[]},\"cifar-10\":{\"p\":[],\"r\":[]},\n",
    "              \"celeba\":{\"p\":[],\"r\":[]},'nus-wide':{\"p\":[],\"r\":[]},\n",
    "               \"mnist-raw\":{\"p\":[],\"r\":[]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-encoder network:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               256500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 511,000\n",
      "Trainable params: 509,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               2560      \n",
      "=================================================================\n",
      "Total params: 2,560\n",
      "Trainable params: 2,560\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "pre-encoder network:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               256500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 511,000\n",
      "Trainable params: 509,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               2560      \n",
      "=================================================================\n",
      "Total params: 2,560\n",
      "Trainable params: 2,560\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "pre-encoder network:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               256500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 511,000\n",
      "Trainable params: 509,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               4608      \n",
      "=================================================================\n",
      "Total params: 4,608\n",
      "Trainable params: 4,608\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "pre-encoder network:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               256500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 511,000\n",
      "Trainable params: 509,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               4608      \n",
      "=================================================================\n",
      "Total params: 4,608\n",
      "Trainable params: 4,608\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-39810423c7ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mbinary_vae\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoder_Bvae\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgenerator_Bvae\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mbinary_VAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_total_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNbit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlayers_e\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlayers_d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbeta_B\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mhist2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinary_vae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_total_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_total\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mtotal_hash_BVAE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_hash_BVAE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhash_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_Bvae\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_total_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fmena/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1596\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1598\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m     def evaluate(self, x, y,\n",
      "\u001b[0;32m/home/fmena/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fmena/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2273\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fmena/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fmena/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fmena/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fmena/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/fmena/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Nbits = np.asarray([4,8,16,32,64])\n",
    "dataset = name_dat.lower()\n",
    "\n",
    "for Nbit in Nbits:\n",
    "    traditional_vae,encoder_Tvae,generator_Tvae = traditional_VAE(X_total_input.shape[1],Nb=Nbit,units=500,layers_e=2,layers_d=0,beta=beta_T)\n",
    "    hist1 = traditional_vae.fit(X_total_input, X_total, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    total_hash_VAE, test_hash_VAE = hash_data(encoder_Tvae,X_total_input,X_test_input, binary=False)\n",
    "    \n",
    "    p_t,r_t = evaluate_hashing_DE(total_hash_VAE,test_hash_VAE,labels_total,labels_test,tipo=\"topK\")\n",
    "    traditional_dat[dataset][\"p\"].append(p_t) \n",
    "    traditional_dat[dataset][\"r\"].append(r_t) \n",
    "    del traditional_vae, encoder_Tvae, generator_Tvae\n",
    "    keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "    \n",
    "    binary_vae,encoder_Bvae,generator_Bvae= binary_VAE(X_total_input.shape[1],Nb=Nbit,units=500,layers_e=2,layers_d=0,beta=beta_B)\n",
    "    hist2 = binary_vae.fit(X_total_input, X_total, epochs=epochs, batch_size=batch_size, verbose=0 )\n",
    "    total_hash_BVAE, test_hash_BVAE = hash_data(encoder_Bvae,X_total_input,X_test_input)\n",
    "\n",
    "    p_b,r_b = evaluate_hashing_DE(total_hash_BVAE, test_hash_BVAE,labels_total,labels_test,tipo=\"topK\")\n",
    "    binary_dat[dataset][\"p\"].append(p_b) \n",
    "    binary_dat[dataset][\"r\"].append(r_b) \n",
    "    del binary_vae, encoder_Bvae, generator_Bvae\n",
    "    keras.backend.clear_session()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de Precision en validación\n",
      "\n",
      "*** VAE Traditional***\n",
      "                 0         1          2          3          4\n",
      "N bits    4.000000  8.000000  16.000000  32.000000  64.000000\n",
      "MNIST-R   0.198970  0.443070   0.637860   0.716720   0.780210\n",
      "MNIST     0.321310  0.548460   0.683030   0.758330   0.792810\n",
      "CIFAR-10  0.289850  0.393530   0.470420   0.505590   0.539710\n",
      "Nus-Wide  0.606495  0.662643   0.714748   0.758576   0.789552\n",
      "\n",
      "*** VAE Binary***\n",
      "                0         1          2          3          4\n",
      "N bits    4.00000  8.000000  16.000000  32.000000  64.000000\n",
      "MNIST-R   0.33748  0.615050   0.743430   0.837780   0.876550\n",
      "MNIST     0.36021  0.654090   0.728460   0.821650   0.836790\n",
      "CIFAR-10  0.29685  0.439340   0.485970   0.536220   0.575620\n",
      "Nus-Wide  0.60010  0.675962   0.714095   0.760886   0.793352\n",
      "\n",
      "Resultados de Recall en validación\n",
      "\n",
      "*** VAE Traditional***\n",
      "                 0         1          2          3          4\n",
      "N bits    4.000000  8.000000  16.000000  32.000000  64.000000\n",
      "MNIST-R   0.002837  0.006363   0.009215   0.010358   0.011283\n",
      "MNIST     0.004614  0.007894   0.009858   0.010972   0.011479\n",
      "CIFAR-10  0.004913  0.006670   0.007973   0.008569   0.009148\n",
      "Nus-Wide  0.000971  0.001195   0.001322   0.001518   0.001625\n",
      "\n",
      "*** VAE Binary***\n",
      "                 0         1          2          3          4\n",
      "N bits    4.000000  8.000000  16.000000  32.000000  64.000000\n",
      "MNIST-R   0.004812  0.008853   0.010741   0.012134   0.012707\n",
      "MNIST     0.005189  0.009428   0.010522   0.011921   0.012146\n",
      "CIFAR-10  0.005031  0.007446   0.008237   0.009088   0.009756\n",
      "Nus-Wide  0.000959  0.001258   0.001391   0.001511   0.001642\n"
     ]
    }
   ],
   "source": [
    "print(\"Resultados de Precision en validación\")\n",
    "t = pd.DataFrame() #Table()\n",
    "t[\"N bits\"] = Nbits\n",
    "t[\"MNIST-R\"] = traditional_dat[\"mnist-raw\"][\"p\"]\n",
    "t[\"MNIST\"] = traditional_dat[\"mnist\"][\"p\"]\n",
    "t[\"CIFAR-10\"] = traditional_dat[\"cifar-10\"][\"p\"]\n",
    "t[\"Nus-Wide\"] = traditional_dat[\"nus-wide\"][\"p\"]\n",
    "print(\"\\n*** VAE Traditional***\")\n",
    "print(t.T)\n",
    "\n",
    "t = pd.DataFrame() #Table()\n",
    "t[\"N bits\"] = Nbits\n",
    "t[\"MNIST-R\"] = binary_dat[\"mnist-raw\"][\"p\"]\n",
    "t[\"MNIST\"] = binary_dat[\"mnist\"][\"p\"]\n",
    "t[\"CIFAR-10\"] = binary_dat[\"cifar-10\"][\"p\"]\n",
    "t[\"Nus-Wide\"] = binary_dat[\"nus-wide\"][\"p\"]\n",
    "print(\"\\n*** VAE Binary***\")\n",
    "print(t.T)\n",
    "\n",
    "print(\"\\nResultados de Recall en validación\")\n",
    "t = pd.DataFrame() #Table()\n",
    "t[\"N bits\"] = Nbits\n",
    "t[\"MNIST-R\"] = traditional_dat[\"mnist-raw\"][\"r\"]\n",
    "t[\"MNIST\"] = traditional_dat[\"mnist\"][\"r\"]\n",
    "t[\"CIFAR-10\"] = traditional_dat[\"cifar-10\"][\"r\"]\n",
    "t[\"Nus-Wide\"] = traditional_dat[\"nus-wide\"][\"r\"]\n",
    "print(\"\\n*** VAE Traditional***\")\n",
    "print(t.T)\n",
    "\n",
    "t = pd.DataFrame() #Table()\n",
    "t[\"N bits\"] = Nbits\n",
    "t[\"MNIST-R\"] = binary_dat[\"mnist-raw\"][\"r\"]\n",
    "t[\"MNIST\"] = binary_dat[\"mnist\"][\"r\"]\n",
    "t[\"CIFAR-10\"] = binary_dat[\"cifar-10\"][\"r\"]\n",
    "t[\"Nus-Wide\"] = binary_dat[\"nus-wide\"][\"r\"]\n",
    "print(\"\\n*** VAE Binary***\")\n",
    "print(t.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_map = {}\n",
    "results_p5000 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-encoder network:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               256500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 511,000\n",
      "Trainable params: 509,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               2560      \n",
      "=================================================================\n",
      "Total params: 2,560\n",
      "Trainable params: 2,560\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "pre-encoder network:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               256500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 511,000\n",
      "Trainable params: 509,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               2560      \n",
      "=================================================================\n",
      "Total params: 2,560\n",
      "Trainable params: 2,560\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "pre-encoder network:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               256500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 511,000\n",
      "Trainable params: 509,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               4608      \n",
      "=================================================================\n",
      "Total params: 4,608\n",
      "Trainable params: 4,608\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "pre-encoder network:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               256500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 511,000\n",
      "Trainable params: 509,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               4608      \n",
      "=================================================================\n",
      "Total params: 4,608\n",
      "Trainable params: 4,608\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "pre-encoder network:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               256500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 511,000\n",
      "Trainable params: 509,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               8704      \n",
      "=================================================================\n",
      "Total params: 8,704\n",
      "Trainable params: 8,704\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "pre-encoder network:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               256500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 511,000\n",
      "Trainable params: 509,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               8704      \n",
      "=================================================================\n",
      "Total params: 8,704\n",
      "Trainable params: 8,704\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "pre-encoder network:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               256500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 511,000\n",
      "Trainable params: 509,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               16896     \n",
      "=================================================================\n",
      "Total params: 16,896\n",
      "Trainable params: 16,896\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "pre-encoder network:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               256500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 511,000\n",
      "Trainable params: 509,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               16896     \n",
      "=================================================================\n",
      "Total params: 16,896\n",
      "Trainable params: 16,896\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "pre-encoder network:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               256500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 511,000\n",
      "Trainable params: 509,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               33280     \n",
      "=================================================================\n",
      "Total params: 33,280\n",
      "Trainable params: 33,280\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "pre-encoder network:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               256500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 511,000\n",
      "Trainable params: 509,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               33280     \n",
      "=================================================================\n",
      "Total params: 33,280\n",
      "Trainable params: 33,280\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Nbits = np.asarray([4,8,16,32,64])\n",
    "dataset = name_dat.lower()\n",
    "\n",
    "results_map[dataset] = {\"vdsh\":[], \"bvae\":[]}\n",
    "results_p5000[dataset] = {\"vdsh\":[], \"bvae\":[]}\n",
    "\n",
    "for Nbit in Nbits:\n",
    "    traditional_vae,encoder_Tvae,generator_Tvae = traditional_VAE(X_total_input.shape[1],Nb=Nbit,units=500,layers_e=2,layers_d=0,beta=beta_T)\n",
    "    traditional_vae.fit(X_total_input, X_total, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    total_hash_VAE, test_hash_VAE = hash_data(encoder_Tvae,X_total_input,X_test_input, binary=False)\n",
    "    \n",
    "    map_t= evaluate_hashing_DE(total_hash_VAE,test_hash_VAE,labels_total,labels_test,eval_tipo=\"MAP\",K=9999999)\n",
    "    results_map[dataset][\"vdsh\"].append(map_t) \n",
    "    p5k_t = evaluate_hashing_DE(total_hash_VAE,test_hash_VAE,labels_total,labels_test,eval_tipo=\"Patk\",K=5000)\n",
    "    results_p5000[dataset][\"vdsh\"].append(p5k_t) \n",
    "    del traditional_vae, encoder_Tvae, generator_Tvae, total_hash_VAE, test_hash_VAE\n",
    "    keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "    \n",
    "    binary_vae,encoder_Bvae,generator_Bvae= binary_VAE(X_total_input.shape[1],Nb=Nbit,units=500,layers_e=2,layers_d=0,beta=beta_B)\n",
    "    binary_vae.fit(X_total_input, X_total, epochs=epochs, batch_size=batch_size, verbose=0 )\n",
    "    total_hash_BVAE, test_hash_BVAE = hash_data(encoder_Bvae,X_total_input,X_test_input)\n",
    "\n",
    "    map_b= evaluate_hashing_DE(total_hash_BVAE, test_hash_BVAE,labels_total,labels_test,eval_tipo=\"MAP\",K=9999999)\n",
    "    results_map[dataset][\"bvae\"].append(map_b) \n",
    "    p5k_b = evaluate_hashing_DE(total_hash_BVAE, test_hash_BVAE,labels_total,labels_test,eval_tipo=\"Patk\",K=5000)\n",
    "    results_p5000[dataset][\"bvae\"].append(p5k_b) \n",
    "    del binary_vae, encoder_Bvae, generator_Bvae, total_hash_BVAE, test_hash_BVAE\n",
    "    keras.backend.clear_session()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados de MAP baseline VDSH\n",
      "                 0         1          2          3          4\n",
      "N bits    4.000000  8.000000  16.000000  32.000000  64.000000\n",
      "MNIST     0.265900  0.361785   0.422593   0.431858   0.447272\n",
      "CIFAR10   0.221654  0.257115   0.242381   0.250736   0.251333\n",
      "Nus-Wide  0.542514  0.491740   0.518685   0.514311   0.516516\n",
      "\n",
      "Resultados de MAP proposal B-VAE\n",
      "                 0         1          2          3          4\n",
      "N bits    4.000000  8.000000  16.000000  32.000000  64.000000\n",
      "MNIST     0.335850  0.413858   0.501958   0.492641   0.470495\n",
      "CIFAR10   0.244769  0.303432   0.279257   0.276726   0.276349\n",
      "Nus-Wide  0.524674  0.542123   0.553858   0.556836   0.551559\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nResultados de MAP baseline VDSH\")\n",
    "t = pd.DataFrame() #Table()\n",
    "t[\"N bits\"] = Nbits\n",
    "t[\"MNIST\"] = results_map[\"mnist\"][\"vdsh\"]\n",
    "t[\"CIFAR10\"] = results_map[\"cifar-10\"][\"vdsh\"]\n",
    "t[\"Nus-Wide\"] = results_map[\"nus-wide\"][\"vdsh\"]\n",
    "print(t.T)\n",
    "\n",
    "print(\"\\nResultados de MAP proposal B-VAE\")\n",
    "t = pd.DataFrame() #Table()\n",
    "t[\"N bits\"] = Nbits\n",
    "t[\"MNIST\"] = results_map[\"mnist\"][\"bvae\"]\n",
    "t[\"CIFAR10\"] = results_map[\"cifar-10\"][\"bvae\"]\n",
    "t[\"Nus-Wide\"] = results_map[\"nus-wide\"][\"bvae\"]\n",
    "print(t.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados de P@5000 baseline VDSH\n",
      "                 0         1          2          3          4\n",
      "N bits    4.000000  8.000000  16.000000  32.000000  64.000000\n",
      "MNIST     0.311460  0.411270   0.474862   0.488247   0.505516\n",
      "CIFAR10   0.256120  0.288222   0.279407   0.289607   0.292260\n",
      "Nus-Wide  0.626808  0.607734   0.657148   0.674433   0.681941\n",
      "\n",
      "Resultados de P@5000 proposal B-VAE\n",
      "                 0         1          2          3          4\n",
      "N bits    4.000000  8.000000  16.000000  32.000000  64.000000\n",
      "MNIST     0.385569  0.463136   0.551670   0.547747   0.530775\n",
      "CIFAR10   0.284461  0.333407   0.313728   0.312793   0.315043\n",
      "Nus-Wide  0.591853  0.652089   0.683086   0.703288   0.699462\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nResultados de P@5000 baseline VDSH\")\n",
    "t = pd.DataFrame() #Table()\n",
    "t[\"N bits\"] = Nbits\n",
    "t[\"MNIST\"] = results_p5000[\"mnist\"][\"vdsh\"]\n",
    "t[\"CIFAR10\"] = results_p5000[\"cifar-10\"][\"vdsh\"]\n",
    "t[\"Nus-Wide\"] = results_p5000[\"nus-wide\"][\"vdsh\"]\n",
    "print(t.T)\n",
    "\n",
    "print(\"\\nResultados de P@5000 proposal B-VAE\")\n",
    "t = pd.DataFrame() #Table()\n",
    "t[\"N bits\"] = Nbits\n",
    "t[\"MNIST\"] = results_p5000[\"mnist\"][\"bvae\"]\n",
    "t[\"CIFAR10\"] = results_p5000[\"cifar-10\"][\"bvae\"]\n",
    "t[\"Nus-Wide\"] = results_p5000[\"nus-wide\"][\"bvae\"]\n",
    "print(t.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tesis]",
   "language": "python",
   "name": "conda-env-tesis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
