{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"20%\" height=\"20%\" />\n",
    "\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "<h1 align='center'> Binary Variational Semantic Hashing </h1>\n",
    "\n",
    "<H3 align='center'> Extensión trabajo CIARP </H3>\n",
    "<hr style=\"height:2px;border:none\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/casapanshop/anaconda2/envs/newpy3_tf1/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras,gc, os, time, sys\n",
    "from keras.layers import *\n",
    "from keras.models import Sequential,Model\n",
    "from keras import backend as K\n",
    "from astropy.table import Table\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from scipy.special import expit\n",
    "\n",
    "from base_networks import *\n",
    "from utils import check_availability, load_imgs_mask\n",
    "\n",
    "from utils import get_topK_labels,set_newlabel_list, enmask_data\n",
    "\n",
    "np.random.seed(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access Data/VGG_128: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!ls Data/VGG_128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "### MNIST\n",
    "---\n",
    "\n",
    "Imágenes en blanco y negro de 28x28 píxeles, de números del 0 al 9, que dan orígen a las 10 clases del problema.\n",
    "\n",
    "|Tipo set|Datos|Label|\n",
    "|---|---|---|\n",
    "|Entrenamiento|60000|10|\n",
    "|Pruebas|10000|10|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 784)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_dat = \"MNIST-raw\"\n",
    "\n",
    "(X_t, aux_t), (X_test, aux_test) = keras.datasets.mnist.load_data()\n",
    "labels = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
    "labels_t = np.asarray([labels[value] for value in aux_t])\n",
    "labels_test = np.asarray([labels[value] for value in aux_test])\n",
    "labels_t = np.concatenate((labels_t,labels_test),axis=0)\n",
    "\n",
    "X_t = np.concatenate([X_t, X_test], axis=0).reshape(len(labels_t),28*28)\n",
    "X_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 512)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_dat = \"MNIST\"\n",
    "\n",
    "(_, aux_t), (_, aux_test) = keras.datasets.mnist.load_data()\n",
    "labels = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
    "labels_t = np.asarray([labels[value] for value in aux_t])\n",
    "labels_test = np.asarray([labels[value] for value in aux_test])\n",
    "labels_t = np.concatenate((labels_t,labels_test),axis=0)\n",
    "\n",
    "#X_t = np.load(\"./Data/VGG_CNN/mnist_VGG_avg.npy\")\n",
    "X_t = np.load(\"../AUX/VGG_128/mnist_VGG_avg.npy\")\n",
    "\n",
    "X_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de datos Entrenamiento:  69000\n",
      "Cantidad de datos Pruebas:  1000\n"
     ]
    }
   ],
   "source": [
    "##### 1000 imagenes de prueba... \n",
    "from utils import sample_test_mask\n",
    "mask_train = sample_test_mask(labels_t, N=100, multi_label=False)\n",
    "\n",
    "## creat test como dicen...\n",
    "X_test = X_t[~mask_train]\n",
    "X_t = X_t[mask_train]\n",
    "labels_test = enmask_data(labels_t, ~mask_train)\n",
    "labels_t = enmask_data(labels_t, mask_train)\n",
    "\n",
    "gc.collect()\n",
    "print(\"Cantidad de datos Entrenamiento: \",len(X_t))\n",
    "print(\"Cantidad de datos Pruebas: \",len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### CIFAR-10\n",
    "---\n",
    "Imágenes RGB pequeñas de  32x32 píxeles, de fotos naturales de distintos objetos.\n",
    "\n",
    "|Tipo set|Datos|Label|\n",
    "|---|---|---|\n",
    "|Entrenamiento|50000|10|\n",
    "|Pruebas|10000|10|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 512)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_dat = \"CIFAR-10\"\n",
    "\n",
    "(_, aux_t), (_, aux_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "labels = [\"airplane\", \"automobile\",\"bird\", \"cat\",\"deer\",\"dog\",\"frog\",\"horse\",\"ship\",\"truck\"]\n",
    "labels_t = np.asarray([labels[value[0]] for value in aux_t])\n",
    "labels_test = np.asarray([labels[value[0]] for value in aux_test])\n",
    "labels_t = np.concatenate((labels_t,labels_test),axis=0)\n",
    "\n",
    "#X_t = np.load(\"./Data/VGG_CNN/cifar10_VGG_avg.npy\")\n",
    "#X_t = np.load(\"./Data/VGG_64/cifar10_VGG_avg.npy\") #mejora\n",
    "\n",
    "#X_t = np.load(\"../AUX/VGG_128/cifar10_VGG_avg.npy\") #mejora\n",
    "X_t = np.load(\"../AUX/VGG_224/cifar10_VGG_avg.npy\") #mejora\n",
    "\n",
    "X_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de datos Entrenamiento:  59000\n",
      "Cantidad de datos Pruebas:  1000\n"
     ]
    }
   ],
   "source": [
    "##### 1000 imagenes de prueba... \n",
    "from utils import sample_test_mask\n",
    "mask_train = sample_test_mask(labels_t, N=100, multi_label=False)\n",
    "\n",
    "## creat test como dicen...\n",
    "X_test = X_t[~mask_train]\n",
    "X_t = X_t[mask_train]\n",
    "labels_test = enmask_data(labels_t, ~mask_train)\n",
    "labels_t = enmask_data(labels_t, mask_train)\n",
    "\n",
    "gc.collect()\n",
    "print(\"Cantidad de datos Entrenamiento: \",len(X_t))\n",
    "print(\"Cantidad de datos Pruebas: \",len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NUSWIDE\n",
    "---\n",
    "* Cantidad de datos totales: 269648\n",
    "* Datos utlizados (con imagenes disponibles a descargar): 169500\n",
    "* Datos top-21 clases: 158383\n",
    "\n",
    "Imágenes de eventos con 81 tópicos asociados, re-dimensionadas a 64x64.\n",
    "\n",
    "|Tipo set|Datos|Label|\n",
    "|---|---|---|\n",
    "|Entrenamiento|xxx|81|\n",
    "|Validación|xxx|81|\n",
    "|Pruebas|xxx|81|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de labels:  81\n",
      "Cantidad de objetos:  169500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['person'], ['person'], ['person'], ['person'], ['person']]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_dat = \"Nus-Wide\"\n",
    "\n",
    "#folder = \"../Dataset_NUSWIDE/\"\n",
    "folder = \"../AUX/\"\n",
    "\n",
    "mask_av = np.loadtxt(\"./Data/\"+name_dat+\"_mask_avail.txt\").astype(bool)\n",
    "\n",
    "labels = pd.read_csv(folder+'Concepts81.txt',header=None).values.reshape(1,-1)[0]\n",
    "print(\"Cantidad de labels: \",len(labels) )\n",
    "\n",
    "labels_t = [[] for _ in range(269648)]\n",
    "for concept in labels:\n",
    "    aux = pd.read_csv(folder+\"Groundtruth/AllLabels/Labels_\"+concept+\".txt\",header=None)\n",
    "    indexs_true = aux.loc[(aux==1).values[:,0]].index\n",
    "    \n",
    "    for value in indexs_true:\n",
    "        labels_t[value].append(concept)\n",
    "        \n",
    "labels_t = enmask_data(labels_t, mask_av)\n",
    "N_total = len(labels_t)\n",
    "print(\"Cantidad de objetos: \",N_total )\n",
    "\n",
    "labels_t[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get top-K labels data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category with most data (sky) has = 61066, the top-K category (mountain) has = 4232\n",
      "Cantidad de objetos:  158383\n"
     ]
    }
   ],
   "source": [
    "new_labels = get_topK_labels(labels_t, labels, K=21)\n",
    "\n",
    "labels_t = set_newlabel_list(new_labels, labels_t)\n",
    "labels = new_labels\n",
    "# y si quedan datos sin clase?\n",
    "mask_used_t = np.asarray(list(map(len,labels_t))) != 0\n",
    "\n",
    "labels_t = enmask_data(labels_t, mask_used_t)\n",
    "print(\"Cantidad de objetos: \", len(labels_t) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N_used = 80*1000 #o 150k??\n",
    "idx_all = np.arange(0, N_total)\n",
    "mask_used = np.zeros(N_total, dtype=bool)\n",
    "mask_used[np.random.choice(np.arange(0, N_total), size=N_used, replace=False)] = 1\n",
    "mask_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(158383, 512)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_t = np.load(\"../AUX/VGG_224/nuswide_VGG_avg.npy\")\n",
    "X_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de datos Entrenamiento:  156283\n",
      "Cantidad de datos Pruebas:  2100\n"
     ]
    }
   ],
   "source": [
    "from utils import sample_test_mask\n",
    "mask_train = sample_test_mask(labels_t, N=100)\n",
    "\n",
    "## creat test como dicen...\n",
    "X_test = X_t[~mask_train]\n",
    "X_t = X_t[mask_train]\n",
    "labels_test = enmask_data(labels_t, ~mask_train)\n",
    "labels_t = enmask_data(labels_t, mask_train)\n",
    "\n",
    "gc.collect()\n",
    "print(\"Cantidad de datos Entrenamiento: \",len(X_t))\n",
    "print(\"Cantidad de datos Pruebas: \",len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation creation\n",
    "\n",
    "Pre-process: División por 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_t = X_t.astype(\"float32\")/255\n",
    "X_test = X_test.astype(\"float32\")/255\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.634587  ,  0.924737  ,  1.8872904 , ..., -0.44784054,\n",
       "         0.0208998 , -0.37210664],\n",
       "       [ 1.0547302 ,  1.1650215 , -0.6296415 , ..., -0.58369744,\n",
       "         2.1071095 ,  0.9391476 ],\n",
       "       [-0.72126764, -0.5263616 , -0.6296415 , ...,  0.02269623,\n",
       "        -0.11105944, -0.5083225 ],\n",
       "       ...,\n",
       "       [ 2.475187  , -0.6954247 ,  0.6455564 , ...,  0.05694862,\n",
       "        -0.33613503,  0.48908427],\n",
       "       [-0.824497  ,  1.7996023 , -0.6296415 , ..., -0.04331863,\n",
       "        -0.27467495, -0.39849097],\n",
       "       [-0.43451124, -0.74836296, -0.5754748 , ...,  0.94223475,\n",
       "        -0.8602258 , -0.31113762]], dtype=float32)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "std = StandardScaler(with_mean=True, with_std=True)\n",
    "std.fit(X_t)\n",
    "\n",
    "X_t = std.transform(X_t)\n",
    "X_test = std.transform(X_test)\n",
    "X_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de datos Entrenamiento:  68000\n",
      "Cantidad de datos Validación:  1000\n",
      "Cantidad de datos Pruebas:  1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, labels_train, labels_val  = train_test_split(X_t, labels_t, random_state=20,test_size=len(X_test))\n",
    "\n",
    "print(\"Cantidad de datos Entrenamiento: \",len(X_train))\n",
    "print(\"Cantidad de datos Validación: \",len(X_val))\n",
    "print(\"Cantidad de datos Pruebas: \",len(X_test))\n",
    "del X_t, labels_t\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_input = X_train\n",
    "X_val_input = X_val\n",
    "X_test_input = X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "---\n",
    "CNN\n",
    "https://github.com/rtflynn/Cifar-Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def REC_loss(x_true, x_pred):\n",
    "    if raw:\n",
    "        return K.mean( K.binary_crossentropy(x_true, x_pred), axis=-1)\n",
    "    else:\n",
    "        return K.mean( (x_true- x_pred)**2 ,axis=-1)  #it should be VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def traditional_VAE(data_dim,Nb,units,layers_e,layers_d,opt='adam',BN=True, beta=0, summ=True):\n",
    "    pre_encoder = define_pre_encoder(data_dim, layers=layers_e,units=units,BN=BN)\n",
    "    if raw:\n",
    "        generator = define_generator(Nb,data_dim,layers=layers_d,units=units,BN=BN, out_type='sigmoid')\n",
    "    else:\n",
    "        generator = define_generator(Nb,data_dim,layers=layers_d,units=units,BN=BN, out_type='linear')    \n",
    "    if summ:\n",
    "        print(\"pre-encoder network:\")\n",
    "        pre_encoder.summary()\n",
    "        print(\"generator network:\")\n",
    "        generator.summary()\n",
    "    ## Encoder\n",
    "    x = Input(shape=(data_dim,))\n",
    "    hidden = pre_encoder(x)\n",
    "    z_mean = Dense(Nb,activation='linear', name='z-mean')(hidden)\n",
    "    z_log_var = Dense(Nb,activation='linear',name = 'z-log_var')(hidden)\n",
    "    encoder = Model(x, z_mean) # build a model to project inputs on the latent space\n",
    "\n",
    "    def sampling(args):\n",
    "        epsilon_std = 1.0\n",
    "        z_mean, z_log_var = args\n",
    "        epsilon = K.random_normal(shape=(K.shape(z_mean)[0], Nb),mean=0., stddev=epsilon_std)\n",
    "        return z_mean + K.exp(0.5*z_log_var) * epsilon #+sigma (desvest)\n",
    "    \n",
    "    ## Decoder\n",
    "    z_sampled = Lambda(sampling, output_shape=(Nb,), name='sampled')([z_mean, z_log_var])\n",
    "    output = generator(z_sampled)\n",
    "    \n",
    "    Recon_loss = REC_loss\n",
    "    kl_loss = KL_loss(z_mean,z_log_var)\n",
    "    def VAE_loss(y_true, y_pred): \n",
    "        return Recon_loss(y_true, y_pred) + beta*kl_loss(y_true, y_pred)\n",
    "\n",
    "    traditional_vae = Model(x, output)\n",
    "    traditional_vae.compile(optimizer=opt, loss=VAE_loss, metrics = [Recon_loss,kl_loss])\n",
    "    \n",
    "    return traditional_vae, encoder,generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "def sample_gumbel(shape,eps=K.epsilon()):\n",
    "    \"\"\"Inverse Sample function from Gumbel(0, 1)\"\"\"\n",
    "    U = K.random_uniform(shape, 0, 1)\n",
    "    return K.log(U + eps)- K.log(1-U + eps)\n",
    "\n",
    "def binary_VAE(data_dim,Nb,units,layers_e,layers_d,opt='adam',BN=True,beta=0., summ=True):\n",
    "    tau = K.variable(0.67, name=\"temperature\") #o tau fijo en 0.67=2/3\n",
    "    \n",
    "    pre_encoder = define_pre_encoder(data_dim, layers=layers_e,units=units,BN=BN)\n",
    "    if raw:\n",
    "        generator = define_generator(Nb,data_dim,layers=layers_d,units=units,BN=BN, out_type='sigmoid')\n",
    "    else:\n",
    "        generator = define_generator(Nb,data_dim,layers=layers_d,units=units,BN=BN, out_type='linear')\n",
    "    if summ:\n",
    "        print(\"pre-encoder network:\")\n",
    "        pre_encoder.summary()\n",
    "        print(\"generator network:\")\n",
    "        generator.summary()\n",
    "\n",
    "    x = Input(shape=(data_dim,))\n",
    "    hidden = pre_encoder(x)\n",
    "    logits_b  = Dense(Nb, activation='linear', name='logits-b')(hidden) #log(B_j/1-B_j)\n",
    "    #proba = np.exp(logits_b)/(1+np.exp(logits_b)) = sigmoidal(logits_b) <<<<<<<<<< recupera probabilidad\n",
    "    #dist = Dense(Nb, activation='sigmoid')(hidden) #p(b) #otra forma de modelarlo\n",
    "    encoder = Model(x, logits_b)\n",
    "\n",
    "    def sampling(logits_b):\n",
    "        #logits_b = K.log(aux/(1-aux) + K.epsilon() )\n",
    "        b = logits_b + sample_gumbel(K.shape(logits_b)) # logits + gumbel noise\n",
    "        return keras.activations.sigmoid( b/tau )\n",
    "\n",
    "    b_sampled = Lambda(sampling, output_shape=(Nb,), name='sampled')(logits_b)\n",
    "    output = generator(b_sampled)\n",
    "        \n",
    "    Recon_loss = REC_loss\n",
    "    kl_loss = BKL_loss(logits_b)\n",
    "    def BVAE_loss(y_true, y_pred): \n",
    "        return Recon_loss(y_true, y_pred) + beta*kl_loss(y_true, y_pred)\n",
    "\n",
    "    binary_vae = Model(x, output)\n",
    "    binary_vae.compile(optimizer=opt, loss=BVAE_loss, metrics = [Recon_loss,kl_loss])\n",
    "    return binary_vae, encoder,generator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train details\n",
    "---\n",
    "\n",
    "* 30* epochs* \n",
    "* *batch size* de 200\n",
    "* optimizador Adam\n",
    "* Inicializador de Glorot (para los pesos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import  compare_hist_train, add_hist_plot\n",
    "\n",
    "batch_size = 100*2 #ya que son datasets mas grandes\n",
    "epochs = 30 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import find_beta\n",
    "\n",
    "def create_model_T(beta_V):\n",
    "    return traditional_VAE(X_train_input.shape[1],Nb=32,units=500,layers_e=2,layers_d=0\n",
    "                                                                  ,beta=beta_V, summ=False)\n",
    "def create_model_B(beta_V):\n",
    "    return binary_VAE(X_train_input.shape[1],Nb=32,units=500,layers_e=2,layers_d=0\n",
    "                                                                  ,beta=beta_V, summ=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************************************\n",
      "*********** SUMMARY RESULTS ***********\n",
      "***************************************\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beta</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.4453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.4477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.4980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.6048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.6385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.6847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.7094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.7439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.7614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.7635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.7643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000488</td>\n",
       "      <td>0.7687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000244</td>\n",
       "      <td>0.7647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.7697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.7708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.7692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.7728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.7786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.7776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.7689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        beta   score\n",
       "0   1.000000  0.4453\n",
       "1   0.500000  0.4477\n",
       "2   0.250000  0.4980\n",
       "3   0.125000  0.6048\n",
       "4   0.062500  0.6385\n",
       "5   0.031250  0.6847\n",
       "6   0.015625  0.7094\n",
       "7   0.007812  0.7439\n",
       "8   0.003906  0.7614\n",
       "9   0.001953  0.7635\n",
       "10  0.000977  0.7643\n",
       "11  0.000488  0.7687\n",
       "12  0.000244  0.7647\n",
       "13  0.000122  0.7697\n",
       "14  0.000061  0.7708\n",
       "15  0.000031  0.7692\n",
       "16  0.000015  0.7728\n",
       "17  0.000008  0.7786\n",
       "18  0.000004  0.7776\n",
       "19  0.000002  0.7689"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best value is 0.7786 with beta 0.000008\n",
      "Worst value is 0.4453 with beta 1.000000\n",
      "***************************************\n",
      "***************************************\n",
      "*********** SUMMARY RESULTS ***********\n",
      "***************************************\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beta</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.2422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.3660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.4665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.5928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.4887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.6490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.7261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.7920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.8151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.8279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.8284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.000488</td>\n",
       "      <td>0.8232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000244</td>\n",
       "      <td>0.8251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.8285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.8249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.8254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.8310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.8250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.8301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.8258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        beta   score\n",
       "0   1.000000  0.2422\n",
       "1   0.500000  0.3660\n",
       "2   0.250000  0.4665\n",
       "3   0.125000  0.5928\n",
       "4   0.062500  0.4887\n",
       "5   0.031250  0.6490\n",
       "6   0.015625  0.7261\n",
       "7   0.007812  0.7920\n",
       "8   0.003906  0.8151\n",
       "9   0.001953  0.8279\n",
       "10  0.000977  0.8284\n",
       "11  0.000488  0.8232\n",
       "12  0.000244  0.8251\n",
       "13  0.000122  0.8285\n",
       "14  0.000061  0.8249\n",
       "15  0.000031  0.8254\n",
       "16  0.000015  0.8310\n",
       "17  0.000008  0.8250\n",
       "18  0.000004  0.8301\n",
       "19  0.000002  0.8258"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best value is 0.8309 with beta 0.000015\n",
      "Worst value is 0.2422 with beta 1.000000\n",
      "***************************************\n"
     ]
    }
   ],
   "source": [
    "beta_T = find_beta(create_model_T, X_train_input, X_train, X_val_input,labels_train,labels_val,\n",
    "                   binary=False, BS=batch_size)\n",
    "beta_B = find_beta(create_model_B, X_train_input, X_train, X_val_input,labels_train,labels_val,\n",
    "                   binary=True, BS=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "## para mnist raw\n",
    "beta_T = 0.000015\n",
    "beta_B = 0.000122"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PARA MNIST-vGG!!\n",
    "beta_T = 0.000008\n",
    "beta_B = 0.000015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PARA CIFAR VGG224!!\n",
    "beta_T = 0.000008\n",
    "beta_B = 0.003906"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PARA NUSWIDE!!\n",
    "beta_T = 0.000008\n",
    "beta_B = 0.001953"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### falta celebA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_total = np.concatenate((X_train,X_val),axis=0)\n",
    "X_total_input = X_total\n",
    "labels_total = np.concatenate((labels_train,labels_val),axis=0)\n",
    "del X_train, X_train_input, X_val, X_val_input, labels_train,labels_val\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-encoder network:\n",
      "Model: \"pre-encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 500)               256500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 511,000\n",
      "Trainable params: 509,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "Model: \"generator/decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 512)               16896     \n",
      "=================================================================\n",
      "Total params: 16,896\n",
      "Trainable params: 16,896\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 156283 samples, validate on 2100 samples\n",
      "Epoch 1/30\n",
      "156283/156283 [==============================] - 6s 38us/step - loss: 0.6591 - REC_loss: 0.6577 - KL: 174.0118 - val_loss: 0.4566 - val_REC_loss: 0.4550 - val_KL: 204.4708\n",
      "Epoch 2/30\n",
      "156283/156283 [==============================] - 5s 32us/step - loss: 0.5702 - REC_loss: 0.5685 - KL: 208.1885 - val_loss: 0.4513 - val_REC_loss: 0.4499 - val_KL: 181.9644\n",
      "Epoch 3/30\n",
      "156283/156283 [==============================] - 5s 32us/step - loss: 0.5664 - REC_loss: 0.5649 - KL: 188.1172 - val_loss: 0.4496 - val_REC_loss: 0.4483 - val_KL: 165.9877\n",
      "Epoch 4/30\n",
      "156283/156283 [==============================] - 5s 32us/step - loss: 0.5647 - REC_loss: 0.5633 - KL: 172.5495 - val_loss: 0.4497 - val_REC_loss: 0.4485 - val_KL: 154.2610\n",
      "Epoch 5/30\n",
      "156283/156283 [==============================] - 5s 33us/step - loss: 0.5638 - REC_loss: 0.5625 - KL: 161.4454 - val_loss: 0.4480 - val_REC_loss: 0.4469 - val_KL: 146.7433\n",
      "Epoch 6/30\n",
      "156283/156283 [==============================] - 5s 32us/step - loss: 0.5631 - REC_loss: 0.5619 - KL: 153.4496 - val_loss: 0.4476 - val_REC_loss: 0.4465 - val_KL: 140.5643\n",
      "Epoch 7/30\n",
      "156283/156283 [==============================] - 5s 32us/step - loss: 0.5626 - REC_loss: 0.5615 - KL: 147.9486 - val_loss: 0.4474 - val_REC_loss: 0.4463 - val_KL: 136.6232\n",
      "Epoch 8/30\n",
      "156283/156283 [==============================] - 5s 32us/step - loss: 0.5623 - REC_loss: 0.5612 - KL: 143.7097 - val_loss: 0.4468 - val_REC_loss: 0.4457 - val_KL: 133.8146\n",
      "Epoch 9/30\n",
      "156283/156283 [==============================] - 5s 32us/step - loss: 0.5620 - REC_loss: 0.5609 - KL: 140.3390 - val_loss: 0.4465 - val_REC_loss: 0.4454 - val_KL: 130.7945\n",
      "Epoch 10/30\n",
      "156283/156283 [==============================] - 5s 32us/step - loss: 0.5618 - REC_loss: 0.5607 - KL: 137.5129 - val_loss: 0.4469 - val_REC_loss: 0.4459 - val_KL: 129.7700\n",
      "Epoch 11/30\n",
      "156283/156283 [==============================] - 5s 32us/step - loss: 0.5616 - REC_loss: 0.5605 - KL: 135.4761 - val_loss: 0.4468 - val_REC_loss: 0.4458 - val_KL: 128.3458\n",
      "Epoch 12/30\n",
      "156283/156283 [==============================] - 5s 32us/step - loss: 0.5613 - REC_loss: 0.5603 - KL: 134.2952 - val_loss: 0.4462 - val_REC_loss: 0.4452 - val_KL: 127.1604\n",
      "Epoch 13/30\n",
      "156283/156283 [==============================] - 5s 32us/step - loss: 0.5612 - REC_loss: 0.5602 - KL: 132.8982 - val_loss: 0.4463 - val_REC_loss: 0.4453 - val_KL: 125.0757\n",
      "Epoch 14/30\n",
      "156283/156283 [==============================] - 5s 33us/step - loss: 0.5610 - REC_loss: 0.5600 - KL: 132.1648 - val_loss: 0.4459 - val_REC_loss: 0.4449 - val_KL: 126.8301\n",
      "Epoch 15/30\n",
      "156283/156283 [==============================] - 5s 32us/step - loss: 0.5609 - REC_loss: 0.5599 - KL: 131.0844 - val_loss: 0.4460 - val_REC_loss: 0.4450 - val_KL: 126.8629\n",
      "Epoch 16/30\n",
      "156283/156283 [==============================] - 5s 32us/step - loss: 0.5608 - REC_loss: 0.5598 - KL: 130.5366 - val_loss: 0.4457 - val_REC_loss: 0.4447 - val_KL: 125.0417\n",
      "Epoch 17/30\n",
      "156283/156283 [==============================] - 5s 32us/step - loss: 0.5608 - REC_loss: 0.5597 - KL: 130.0410 - val_loss: 0.4457 - val_REC_loss: 0.4447 - val_KL: 125.3365\n",
      "Epoch 18/30\n",
      "156283/156283 [==============================] - 5s 32us/step - loss: 0.5606 - REC_loss: 0.5596 - KL: 129.6335 - val_loss: 0.4461 - val_REC_loss: 0.4451 - val_KL: 124.7222\n",
      "Epoch 19/30\n",
      "156283/156283 [==============================] - 5s 32us/step - loss: 0.5604 - REC_loss: 0.5594 - KL: 129.2617 - val_loss: 0.4459 - val_REC_loss: 0.4449 - val_KL: 125.8126\n",
      "Epoch 20/30\n",
      "156283/156283 [==============================] - 5s 32us/step - loss: 0.5604 - REC_loss: 0.5594 - KL: 128.7676 - val_loss: 0.4453 - val_REC_loss: 0.4443 - val_KL: 123.2167\n",
      "Epoch 21/30\n",
      "156283/156283 [==============================] - 5s 32us/step - loss: 0.5603 - REC_loss: 0.5592 - KL: 128.3293 - val_loss: 0.4455 - val_REC_loss: 0.4445 - val_KL: 123.9981\n",
      "Epoch 22/30\n",
      "156283/156283 [==============================] - 5s 32us/step - loss: 0.5602 - REC_loss: 0.5591 - KL: 128.2547 - val_loss: 0.4458 - val_REC_loss: 0.4449 - val_KL: 123.4186\n",
      "Epoch 23/30\n",
      "156283/156283 [==============================] - 5s 32us/step - loss: 0.5602 - REC_loss: 0.5592 - KL: 128.0694 - val_loss: 0.4458 - val_REC_loss: 0.4448 - val_KL: 122.2906\n",
      "Epoch 24/30\n",
      "156283/156283 [==============================] - 5s 31us/step - loss: 0.5601 - REC_loss: 0.5590 - KL: 127.7884 - val_loss: 0.4457 - val_REC_loss: 0.4446 - val_KL: 125.2687\n",
      "Epoch 25/30\n",
      "156283/156283 [==============================] - 5s 32us/step - loss: 0.5601 - REC_loss: 0.5591 - KL: 127.7467 - val_loss: 0.4453 - val_REC_loss: 0.4443 - val_KL: 124.6941\n",
      "Epoch 26/30\n",
      "156283/156283 [==============================] - 5s 32us/step - loss: 0.5599 - REC_loss: 0.5589 - KL: 127.8374 - val_loss: 0.4452 - val_REC_loss: 0.4442 - val_KL: 125.6647\n",
      "Epoch 27/30\n",
      "156283/156283 [==============================] - 5s 32us/step - loss: 0.5599 - REC_loss: 0.5589 - KL: 127.5238 - val_loss: 0.4449 - val_REC_loss: 0.4439 - val_KL: 124.2448\n",
      "Epoch 28/30\n",
      "156283/156283 [==============================] - 5s 31us/step - loss: 0.5600 - REC_loss: 0.5590 - KL: 127.5442 - val_loss: 0.4449 - val_REC_loss: 0.4440 - val_KL: 122.4354\n",
      "Epoch 29/30\n",
      "156283/156283 [==============================] - 5s 32us/step - loss: 0.5598 - REC_loss: 0.5588 - KL: 127.5818 - val_loss: 0.4456 - val_REC_loss: 0.4446 - val_KL: 123.9897\n",
      "Epoch 30/30\n",
      "156283/156283 [==============================] - 5s 32us/step - loss: 0.5598 - REC_loss: 0.5588 - KL: 127.5497 - val_loss: 0.4448 - val_REC_loss: 0.4438 - val_KL: 125.1465\n"
     ]
    }
   ],
   "source": [
    "traditional_vae,encoder_Tvae,generator_Tvae = traditional_VAE(X_total_input.shape[1],Nb=32,units=500,layers_e=2,layers_d=0,beta=beta_T)\n",
    "\n",
    "hist1 = traditional_vae.fit(X_total_input, X_total, epochs=epochs, batch_size=batch_size\n",
    "                           ,validation_data=(X_test_input,X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-encoder network:\n",
      "Model: \"pre-encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 500)               256500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 511,000\n",
      "Trainable params: 509,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "Model: \"generator/decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 512)               16896     \n",
      "=================================================================\n",
      "Total params: 16,896\n",
      "Trainable params: 16,896\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 156283 samples, validate on 2100 samples\n",
      "Epoch 1/30\n",
      "156283/156283 [==============================] - 6s 37us/step - loss: 0.7613 - REC_loss: 0.7749 - KL: -6.9596 - val_loss: 0.5576 - val_REC_loss: 0.5679 - val_KL: -5.2907\n",
      "Epoch 2/30\n",
      "156283/156283 [==============================] - 5s 32us/step - loss: 0.6963 - REC_loss: 0.7066 - KL: -5.2761 - val_loss: 0.5463 - val_REC_loss: 0.5556 - val_KL: -4.7521\n",
      "Epoch 3/30\n",
      "156283/156283 [==============================] - 5s 32us/step - loss: 0.6876 - REC_loss: 0.6972 - KL: -4.9306 - val_loss: 0.5446 - val_REC_loss: 0.5536 - val_KL: -4.6080\n",
      "Epoch 4/30\n",
      "156283/156283 [==============================] - 5s 32us/step - loss: 0.6843 - REC_loss: 0.6937 - KL: -4.8200 - val_loss: 0.5428 - val_REC_loss: 0.5515 - val_KL: -4.4607\n",
      "Epoch 5/30\n",
      "156283/156283 [==============================] - 5s 32us/step - loss: 0.6824 - REC_loss: 0.6916 - KL: -4.7285 - val_loss: 0.5407 - val_REC_loss: 0.5491 - val_KL: -4.2956\n",
      "Epoch 6/30\n",
      "156283/156283 [==============================] - 5s 31us/step - loss: 0.6809 - REC_loss: 0.6900 - KL: -4.6753 - val_loss: 0.5405 - val_REC_loss: 0.5491 - val_KL: -4.4210\n",
      "Epoch 7/30\n",
      "156283/156283 [==============================] - 5s 32us/step - loss: 0.6797 - REC_loss: 0.6887 - KL: -4.6417 - val_loss: 0.5388 - val_REC_loss: 0.5472 - val_KL: -4.3026\n",
      "Epoch 8/30\n",
      "156283/156283 [==============================] - 5s 31us/step - loss: 0.6787 - REC_loss: 0.6877 - KL: -4.5896 - val_loss: 0.5387 - val_REC_loss: 0.5470 - val_KL: -4.2516\n",
      "Epoch 9/30\n",
      "156283/156283 [==============================] - 5s 32us/step - loss: 0.6779 - REC_loss: 0.6868 - KL: -4.5491 - val_loss: 0.5384 - val_REC_loss: 0.5467 - val_KL: -4.2581\n",
      "Epoch 10/30\n",
      "156283/156283 [==============================] - 5s 30us/step - loss: 0.6770 - REC_loss: 0.6858 - KL: -4.4994 - val_loss: 0.5376 - val_REC_loss: 0.5458 - val_KL: -4.2100\n",
      "Epoch 11/30\n",
      "156283/156283 [==============================] - 5s 31us/step - loss: 0.6764 - REC_loss: 0.6852 - KL: -4.4667 - val_loss: 0.5382 - val_REC_loss: 0.5464 - val_KL: -4.1907\n",
      "Epoch 12/30\n",
      "156283/156283 [==============================] - 5s 30us/step - loss: 0.6757 - REC_loss: 0.6844 - KL: -4.4662 - val_loss: 0.5371 - val_REC_loss: 0.5453 - val_KL: -4.1952\n",
      "Epoch 13/30\n",
      "156283/156283 [==============================] - 5s 30us/step - loss: 0.6752 - REC_loss: 0.6839 - KL: -4.4433 - val_loss: 0.5369 - val_REC_loss: 0.5451 - val_KL: -4.1987\n",
      "Epoch 14/30\n",
      "156283/156283 [==============================] - 5s 31us/step - loss: 0.6747 - REC_loss: 0.6834 - KL: -4.4140 - val_loss: 0.5367 - val_REC_loss: 0.5447 - val_KL: -4.1049\n",
      "Epoch 15/30\n",
      "156283/156283 [==============================] - 5s 30us/step - loss: 0.6742 - REC_loss: 0.6828 - KL: -4.3806 - val_loss: 0.5353 - val_REC_loss: 0.5434 - val_KL: -4.1230\n",
      "Epoch 16/30\n",
      "156283/156283 [==============================] - 5s 31us/step - loss: 0.6740 - REC_loss: 0.6825 - KL: -4.3673 - val_loss: 0.5356 - val_REC_loss: 0.5437 - val_KL: -4.1528\n",
      "Epoch 17/30\n",
      "156283/156283 [==============================] - 5s 31us/step - loss: 0.6734 - REC_loss: 0.6819 - KL: -4.3411 - val_loss: 0.5359 - val_REC_loss: 0.5439 - val_KL: -4.1118\n",
      "Epoch 18/30\n",
      "156283/156283 [==============================] - 5s 31us/step - loss: 0.6731 - REC_loss: 0.6816 - KL: -4.3297 - val_loss: 0.5354 - val_REC_loss: 0.5434 - val_KL: -4.1133\n",
      "Epoch 19/30\n",
      "156283/156283 [==============================] - 5s 31us/step - loss: 0.6729 - REC_loss: 0.6813 - KL: -4.3121 - val_loss: 0.5350 - val_REC_loss: 0.5430 - val_KL: -4.1153\n",
      "Epoch 20/30\n",
      "156283/156283 [==============================] - 5s 32us/step - loss: 0.6726 - REC_loss: 0.6810 - KL: -4.2940 - val_loss: 0.5351 - val_REC_loss: 0.5432 - val_KL: -4.1064\n",
      "Epoch 21/30\n",
      "156283/156283 [==============================] - 5s 32us/step - loss: 0.6723 - REC_loss: 0.6807 - KL: -4.2833 - val_loss: 0.5342 - val_REC_loss: 0.5422 - val_KL: -4.0943\n",
      "Epoch 22/30\n",
      "156283/156283 [==============================] - 5s 31us/step - loss: 0.6721 - REC_loss: 0.6804 - KL: -4.2712 - val_loss: 0.5351 - val_REC_loss: 0.5430 - val_KL: -4.0304\n",
      "Epoch 23/30\n",
      "156283/156283 [==============================] - 5s 29us/step - loss: 0.6718 - REC_loss: 0.6801 - KL: -4.2467 - val_loss: 0.5348 - val_REC_loss: 0.5427 - val_KL: -4.0415\n",
      "Epoch 24/30\n",
      "156283/156283 [==============================] - 5s 30us/step - loss: 0.6716 - REC_loss: 0.6799 - KL: -4.2384 - val_loss: 0.5342 - val_REC_loss: 0.5422 - val_KL: -4.0686\n",
      "Epoch 25/30\n",
      "156283/156283 [==============================] - 5s 31us/step - loss: 0.6715 - REC_loss: 0.6798 - KL: -4.2169 - val_loss: 0.5342 - val_REC_loss: 0.5421 - val_KL: -4.0522\n",
      "Epoch 26/30\n",
      "156283/156283 [==============================] - 5s 32us/step - loss: 0.6713 - REC_loss: 0.6796 - KL: -4.2069 - val_loss: 0.5341 - val_REC_loss: 0.5421 - val_KL: -4.0631\n",
      "Epoch 27/30\n",
      "156283/156283 [==============================] - 5s 31us/step - loss: 0.6710 - REC_loss: 0.6792 - KL: -4.2181 - val_loss: 0.5341 - val_REC_loss: 0.5419 - val_KL: -4.0039\n",
      "Epoch 28/30\n",
      "156283/156283 [==============================] - 5s 31us/step - loss: 0.6709 - REC_loss: 0.6791 - KL: -4.2143 - val_loss: 0.5342 - val_REC_loss: 0.5420 - val_KL: -4.0105\n",
      "Epoch 29/30\n",
      "156283/156283 [==============================] - 5s 31us/step - loss: 0.6707 - REC_loss: 0.6789 - KL: -4.1995 - val_loss: 0.5342 - val_REC_loss: 0.5421 - val_KL: -4.0321\n",
      "Epoch 30/30\n",
      "156283/156283 [==============================] - 5s 32us/step - loss: 0.6706 - REC_loss: 0.6788 - KL: -4.1940 - val_loss: 0.5338 - val_REC_loss: 0.5416 - val_KL: -4.0239\n"
     ]
    }
   ],
   "source": [
    "binary_vae,encoder_Bvae,generator_Bvae= binary_VAE(X_total_input.shape[1],Nb=32,units=500,layers_e=2,layers_d=0,beta=beta_B)\n",
    "\n",
    "hist2 = binary_vae.fit(X_total_input, X_total, epochs=epochs, batch_size=batch_size\n",
    "                           ,validation_data=(X_test_input,X_test) )\n",
    "                       #,callbacks=[Tau_Call(tau)] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another intrinsic measure: *Classification*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#function to define and train model\n",
    "from sklearn.metrics import jaccard_score\n",
    "from utils import define_fit, MedianHashing, visualize_probas, visualize_mean, calculate_hash, visualize_probas_byB\n",
    "\n",
    "results = []\n",
    "results_S = []\n",
    "results_B = []\n",
    "results_O_B = [] #original testing on tresholded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 1., 0., ..., 0., 1., 0.]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#codify input data (binarize -- or aprox)\n",
    "X_train_logits = encoder_Bvae.predict(X_total_input)\n",
    "X_val_logits = encoder_Bvae.predict(X_test_input)\n",
    "\n",
    "#probabilities\n",
    "X_train_Bcode = expit(X_train_logits)\n",
    "X_val_Bcode = expit(X_val_logits)\n",
    "\n",
    "#Z-mean\n",
    "X_train_Tcode = encoder_Tvae.predict(X_total_input)\n",
    "X_val_Tcode = encoder_Tvae.predict(X_test_input)\n",
    "\n",
    "##codify labels\n",
    "labels_aux = np.asarray(labels)\n",
    "def codify_labels(inputs):\n",
    "    inputs = np.asarray(inputs)\n",
    "    matrix_labels = np.zeros((inputs.shape[0],labels_aux.shape[0]))\n",
    "    for i,aux_labels in enumerate(inputs):\n",
    "        if type(aux_labels) == list or type(aux_labels) == np.ndarray :\n",
    "            for aux_label in aux_labels:\n",
    "                idx = np.where(aux_label==labels_aux)[0]\n",
    "                matrix_labels[i,idx] = 1 #various-multiple\n",
    "        else:\n",
    "            idx = np.where(aux_labels==labels_aux)[0]\n",
    "            matrix_labels[i,idx] = 1 #only one\n",
    "    return matrix_labels\n",
    "\n",
    "C_train = codify_labels(labels_total)\n",
    "#C_val = codify_labels(labels_val)\n",
    "C_val = codify_labels(labels_test)\n",
    "C_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "aux = [name_dat.lower()]\n",
    "multi_label = \"celeba\" in aux or \"nus-wide\" in aux\n",
    "\n",
    "model1_O = define_fit(multi_label,X_train_Bcode,C_train)\n",
    "model2_O = define_fit(multi_label,X_train_Tcode,C_train)\n",
    "\n",
    "if not multi_label:\n",
    "    aux.append(model2_O.evaluate(X_train_Tcode,C_train,verbose=0)[1])\n",
    "    aux.append(model2_O.evaluate(X_val_Tcode,C_val,verbose=0)[1])\n",
    "\n",
    "    aux.append(model1_O.evaluate(X_train_Bcode,C_train,verbose=0)[1])\n",
    "    aux.append(model1_O.evaluate(X_val_Bcode,C_val,verbose=0)[1])\n",
    "else:\n",
    "    aux.append(jaccard_score(C_train, (model2_O.predict(X_train_Tcode)>=0.5)*1, average='micro'))\n",
    "    aux.append(jaccard_score(C_val, (model2_O.predict(X_val_Tcode)>=0.5)*1, average='micro'))\n",
    "\n",
    "    aux.append(jaccard_score(C_train, (model1_O.predict(X_train_Bcode)>=0.5)*1, average='micro'))\n",
    "    aux.append(jaccard_score(C_val, (model1_O.predict(X_val_Bcode)>=0.5)*1, average='micro'))\n",
    "    \n",
    "results.append(aux)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy on dataset:  mnist-raw\n",
      "Traditional VAE (train-val): 0.995681 - 0.979000\n",
      "Binary VAE (train-val): 0.968841 - 0.944000\n",
      "\n",
      "Accuracy on dataset:  mnist\n",
      "Traditional VAE (train-val): 0.978986 - 0.963000\n",
      "Binary VAE (train-val): 0.940667 - 0.947000\n",
      "\n",
      "Accuracy on dataset:  cifar-10\n",
      "Traditional VAE (train-val): 0.876610 - 0.850000\n",
      "Binary VAE (train-val): 0.806525 - 0.799000\n",
      "\n",
      "Accuracy on dataset:  nus-wide\n",
      "Traditional VAE (train-val): 0.537198 - 0.493226\n",
      "Binary VAE (train-val): 0.499151 - 0.448710\n"
     ]
    }
   ],
   "source": [
    "for valores in results:\n",
    "    print(\"\\nAccuracy on dataset: \",valores[0])\n",
    "    print(\"Traditional VAE (train-val): %f - %f\"%(valores[1],valores[2]))\n",
    "    print(\"Binary VAE (train-val): %f - %f\"%(valores[3],valores[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, ..., 0, 0, 1],\n",
       "       [0, 0, 1, ..., 0, 1, 0],\n",
       "       [0, 1, 1, ..., 1, 1, 0],\n",
       "       ...,\n",
       "       [1, 1, 1, ..., 0, 1, 1],\n",
       "       [0, 1, 1, ..., 1, 0, 1],\n",
       "       [1, 0, 0, ..., 1, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## binary // treshold it\n",
    "\n",
    "X_train_Tcode_B = calculate_hash(X_train_Tcode, from_probas=False)\n",
    "X_val_Tcode_B = calculate_hash(X_val_Tcode, from_probas=False)\n",
    "\n",
    "X_train_Bcode_B = calculate_hash(X_train_Bcode, from_probas=True, from_logits=False)\n",
    "X_val_Bcode_B = calculate_hash(X_val_Bcode, from_probas=True, from_logits=False)\n",
    "\n",
    "X_train_Tcode_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "### trained models over encoder representation testing on tresholded representation (see how decrease)\n",
    "aux = [name_dat.lower()]\n",
    "multi_label = \"celeba\" in aux or \"nus-wide\" in aux\n",
    "\n",
    "if not multi_label:\n",
    "    aux.append(model2_O.evaluate(X_train_Tcode_B,C_train,verbose=0)[1])\n",
    "    aux.append(model2_O.evaluate(X_val_Tcode_B,C_val,verbose=0)[1])\n",
    "\n",
    "    aux.append(model1_O.evaluate(X_train_Bcode_B,C_train,verbose=0)[1])\n",
    "    aux.append(model1_O.evaluate(X_val_Bcode_B,C_val,verbose=0)[1])\n",
    "else:\n",
    "    aux.append(jaccard_score(C_train, (model2_O.predict(X_train_Tcode_B)>=0.5)*1, average='micro'))\n",
    "    aux.append(jaccard_score(C_val, (model2_O.predict(X_val_Tcode_B)>=0.5)*1, average='micro'))\n",
    "\n",
    "    aux.append(jaccard_score(C_train, (model1_O.predict(X_train_Bcode_B)>=0.5)*1, average='micro'))\n",
    "    aux.append(jaccard_score(C_val, (model1_O.predict(X_val_Bcode_B)>=0.5)*1, average='micro'))\n",
    "    \n",
    "results_O_B.append(aux)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification on Binary (thresholded) representation\n",
      "\n",
      "Accuracy on dataset:  mnist-raw\n",
      "Traditional VAE (train-val): 0.441710 - 0.443000\n",
      "Binary VAE (train-val): 0.941000 - 0.915000\n",
      "\n",
      "Accuracy on dataset:  mnist\n",
      "Traditional VAE (train-val): 0.492754 - 0.506000\n",
      "Binary VAE (train-val): 0.914710 - 0.921000\n",
      "\n",
      "Accuracy on dataset:  cifar-10\n",
      "Traditional VAE (train-val): 0.385678 - 0.410000\n",
      "Binary VAE (train-val): 0.734542 - 0.731000\n",
      "\n",
      "Accuracy on dataset:  nus-wide\n",
      "Traditional VAE (train-val): 0.219203 - 0.166017\n",
      "Binary VAE (train-val): 0.462315 - 0.411896\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification on Binary (thresholded) representation\")\n",
    "for valores in results_O_B:\n",
    "    print(\"\\nAccuracy on dataset: \",valores[0])\n",
    "    print(\"Traditional VAE (train-val): %f - %f\"%(valores[1],valores[2]))\n",
    "    print(\"Binary VAE (train-val): %f - %f\"%(valores[3],valores[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "aux = [name_dat.lower()]\n",
    "multi_label = \"celeba\" in aux or \"nus-wide\" in aux\n",
    "\n",
    "model1 = define_fit(multi_label,X_train_Bcode_B,C_train)\n",
    "model2 = define_fit(multi_label,X_train_Tcode_B,C_train)\n",
    "\n",
    "if not multi_label:\n",
    "    aux.append(model2.evaluate(X_train_Tcode_B,C_train,verbose=0)[1])\n",
    "    aux.append(model2.evaluate(X_val_Tcode_B,C_val,verbose=0)[1])\n",
    "\n",
    "    aux.append(model1.evaluate(X_train_Bcode_B,C_train,verbose=0)[1])\n",
    "    aux.append(model1.evaluate(X_val_Bcode_B,C_val,verbose=0)[1])\n",
    "else:\n",
    "    aux.append(jaccard_score(C_train, (model2.predict(X_train_Tcode_B)>=0.5)*1, average='micro')) \n",
    "    aux.append(jaccard_score(C_val, (model2.predict(X_val_Tcode_B)>=0.5)*1, average='micro'))\n",
    "\n",
    "    aux.append(jaccard_score(C_train, (model1.predict(X_train_Bcode_B)>=0.5)*1, average='micro'))\n",
    "    aux.append(jaccard_score(C_val, (model1.predict(X_val_Bcode_B)>=0.5)*1, average='micro'))\n",
    "    \n",
    "results_B.append(aux)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification on Binary representation\n",
      "\n",
      "Accuracy on dataset:  mnist-raw\n",
      "Traditional VAE (train-val): 0.911884 - 0.898000\n",
      "Binary VAE (train-val): 0.950000 - 0.925000\n",
      "\n",
      "Accuracy on dataset:  mnist\n",
      "Traditional VAE (train-val): 0.905072 - 0.905000\n",
      "Binary VAE (train-val): 0.920246 - 0.916000\n",
      "\n",
      "Accuracy on dataset:  cifar-10\n",
      "Traditional VAE (train-val): 0.734508 - 0.738000\n",
      "Binary VAE (train-val): 0.752322 - 0.729000\n",
      "\n",
      "Accuracy on dataset:  nus-wide\n",
      "Traditional VAE (train-val): 0.436969 - 0.379390\n",
      "Binary VAE (train-val): 0.450483 - 0.391193\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification on Binary representation\")\n",
    "for valores in results_B:\n",
    "    print(\"\\nAccuracy on dataset: \",valores[0])\n",
    "    print(\"Traditional VAE (train-val): %f - %f\"%(valores[1],valores[2]))\n",
    "    print(\"Binary VAE (train-val): %f - %f\"%(valores[3],valores[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Results\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils import MedianHashing, get_similar, measure_metrics, calculate_hash\n",
    "from utils import MAP_atk, M_P_atk, AP_atk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentación\n",
    "---\n",
    "A continación se realizan las experimentaciones correspondientes para encontrar la mejor arquitectura y mejor configuración del modelo propuesto en base a las métricas *precision* y *recall* del conjunto de validación en el **top 100** de elementos recuperados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_hashing_DE(train_hash,test_hash,labels_trainn,labels_testt,tipo=\"topK\",eval_tipo='PRatk',K=100):\n",
    "    \"\"\"\n",
    "        Evaluate Hashing correclty: Query and retrieve on a different set\n",
    "    \"\"\"\n",
    "    test_similares_train =  get_similar(test_hash,train_hash,tipo=tipo,K=K)\n",
    "    if eval_tipo==\"MAP\":\n",
    "        return MAP_atk(test_similares_train,labels_query=labels_testt, labels_source=labels_trainn, K=0) #all ranking\n",
    "    elif eval_tipo == \"PRatk\":\n",
    "        return measure_metrics(labels,test_similares_train,labels_testt,labels_source=labels_trainn)\n",
    "    elif eval_tipo == \"Patk\":\n",
    "        return M_P_atk(test_similares_train, labels_query=labels_testt, labels_source=labels_trainn, K=K)\n",
    "\n",
    "def hash_data(model, x_train, x_test, binary=True):\n",
    "    encode_train = model.predict(x_train)\n",
    "    encode_test = model.predict(x_test)\n",
    "    \n",
    "    train_hash = calculate_hash(encode_train, from_probas=binary )\n",
    "    test_hash = calculate_hash(encode_test, from_probas = binary)\n",
    "    return train_hash, test_hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Experimentando variando el #Bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "traditional_dat = {\"mnist\":{\"p\":[],\"r\":[]},\"cifar-10\":{\"p\":[],\"r\":[]},\n",
    "                   \"celeba\":{\"p\":[],\"r\":[]},'nus-wide':{\"p\":[],\"r\":[]},\n",
    "                  \"mnist-raw\":{\"p\":[],\"r\":[]}}\n",
    "binary_dat = {\"mnist\":{\"p\":[],\"r\":[]},\"cifar-10\":{\"p\":[],\"r\":[]},\n",
    "              \"celeba\":{\"p\":[],\"r\":[]},'nus-wide':{\"p\":[],\"r\":[]},\n",
    "               \"mnist-raw\":{\"p\":[],\"r\":[]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-encoder network:\n",
      "Model: \"pre-encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_18 (Dense)             (None, 500)               256500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 511,000\n",
      "Trainable params: 509,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "Model: \"generator/decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_20 (Dense)             (None, 512)               2560      \n",
      "=================================================================\n",
      "Total params: 2,560\n",
      "Trainable params: 2,560\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "pre-encoder network:\n",
      "Model: \"pre-encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 500)               256500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 511,000\n",
      "Trainable params: 509,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "Model: \"generator/decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 512)               2560      \n",
      "=================================================================\n",
      "Total params: 2,560\n",
      "Trainable params: 2,560\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "pre-encoder network:\n",
      "Model: \"pre-encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 500)               256500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 511,000\n",
      "Trainable params: 509,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "Model: \"generator/decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 512)               4608      \n",
      "=================================================================\n",
      "Total params: 4,608\n",
      "Trainable params: 4,608\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "pre-encoder network:\n",
      "Model: \"pre-encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 500)               256500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 511,000\n",
      "Trainable params: 509,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "Model: \"generator/decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 512)               4608      \n",
      "=================================================================\n",
      "Total params: 4,608\n",
      "Trainable params: 4,608\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "pre-encoder network:\n",
      "Model: \"pre-encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 500)               256500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 511,000\n",
      "Trainable params: 509,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "Model: \"generator/decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 512)               8704      \n",
      "=================================================================\n",
      "Total params: 8,704\n",
      "Trainable params: 8,704\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "pre-encoder network:\n",
      "Model: \"pre-encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 500)               256500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 511,000\n",
      "Trainable params: 509,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "Model: \"generator/decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 512)               8704      \n",
      "=================================================================\n",
      "Total params: 8,704\n",
      "Trainable params: 8,704\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-encoder network:\n",
      "Model: \"pre-encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 500)               256500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 511,000\n",
      "Trainable params: 509,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "Model: \"generator/decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 512)               16896     \n",
      "=================================================================\n",
      "Total params: 16,896\n",
      "Trainable params: 16,896\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "pre-encoder network:\n",
      "Model: \"pre-encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 500)               256500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 511,000\n",
      "Trainable params: 509,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "Model: \"generator/decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 512)               16896     \n",
      "=================================================================\n",
      "Total params: 16,896\n",
      "Trainable params: 16,896\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "pre-encoder network:\n",
      "Model: \"pre-encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 500)               256500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 511,000\n",
      "Trainable params: 509,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "Model: \"generator/decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 512)               33280     \n",
      "=================================================================\n",
      "Total params: 33,280\n",
      "Trainable params: 33,280\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "pre-encoder network:\n",
      "Model: \"pre-encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 500)               256500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 511,000\n",
      "Trainable params: 509,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "Model: \"generator/decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 512)               33280     \n",
      "=================================================================\n",
      "Total params: 33,280\n",
      "Trainable params: 33,280\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Nbits = np.asarray([4,8,16,32,64])\n",
    "dataset = name_dat.lower()\n",
    "\n",
    "for Nbit in Nbits:\n",
    "    traditional_vae,encoder_Tvae,generator_Tvae = traditional_VAE(X_total_input.shape[1],Nb=Nbit,units=500,layers_e=2,layers_d=0,beta=beta_T)\n",
    "    hist1 = traditional_vae.fit(X_total_input, X_total, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    total_hash_VAE, test_hash_VAE = hash_data(encoder_Tvae,X_total_input,X_test_input, binary=False)\n",
    "    \n",
    "    p_t,r_t = evaluate_hashing_DE(total_hash_VAE,test_hash_VAE,labels_total,labels_test,tipo=\"topK\")\n",
    "    traditional_dat[dataset][\"p\"].append(p_t) \n",
    "    traditional_dat[dataset][\"r\"].append(r_t) \n",
    "    del traditional_vae, encoder_Tvae, generator_Tvae\n",
    "    keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "    \n",
    "    binary_vae,encoder_Bvae,generator_Bvae= binary_VAE(X_total_input.shape[1],Nb=Nbit,units=500,layers_e=2,layers_d=0,beta=beta_B)\n",
    "    hist2 = binary_vae.fit(X_total_input, X_total, epochs=epochs, batch_size=batch_size, verbose=0 )\n",
    "    total_hash_BVAE, test_hash_BVAE = hash_data(encoder_Bvae,X_total_input,X_test_input)\n",
    "\n",
    "    p_b,r_b = evaluate_hashing_DE(total_hash_BVAE, test_hash_BVAE,labels_total,labels_test,tipo=\"topK\")\n",
    "    binary_dat[dataset][\"p\"].append(p_b) \n",
    "    binary_dat[dataset][\"r\"].append(r_b) \n",
    "    del binary_vae, encoder_Bvae, generator_Bvae\n",
    "    keras.backend.clear_session()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados de Precision en validación\n",
      "\n",
      "*** VAE Traditional***\n",
      "                 0        1          2          3          4\n",
      "N bits    4.000000  8.00000  16.000000  32.000000  64.000000\n",
      "MNIST-R   0.206590  0.43554   0.597050   0.700910   0.767840\n",
      "MNIST     0.295920  0.55293   0.695330   0.776230   0.814720\n",
      "CIFAR-10  0.306600  0.39327   0.467690   0.500460   0.535180\n",
      "Nus-Wide  0.560267  0.65929   0.725848   0.766086   0.786838\n",
      "\n",
      "*** VAE Binary***\n",
      "                 0         1          2          3          4\n",
      "N bits    4.000000  8.000000  16.000000  32.000000  64.000000\n",
      "MNIST-R   0.350050  0.581790   0.740360   0.811650   0.846380\n",
      "MNIST     0.361750  0.654270   0.756700   0.841760   0.856510\n",
      "CIFAR-10  0.324620  0.436160   0.494240   0.548240   0.574500\n",
      "Nus-Wide  0.593871  0.668467   0.739533   0.775852   0.802776\n",
      "\n",
      "Resultados de Recall en validación\n",
      "\n",
      "*** VAE Traditional***\n",
      "                 0         1          2          3          4\n",
      "N bits    4.000000  8.000000  16.000000  32.000000  64.000000\n",
      "MNIST-R   0.002942  0.006250   0.008624   0.010128   0.011108\n",
      "MNIST     0.004257  0.007967   0.010041   0.011240   0.011809\n",
      "CIFAR-10  0.005197  0.006666   0.007927   0.008482   0.009071\n",
      "Nus-Wide  0.000925  0.001189   0.001429   0.001555   0.001625\n",
      "\n",
      "*** VAE Binary***\n",
      "                 0         1          2          3          4\n",
      "N bits    4.000000  8.000000  16.000000  32.000000  64.000000\n",
      "MNIST-R   0.004981  0.008384   0.010710   0.011754   0.012267\n",
      "MNIST     0.005225  0.009424   0.010929   0.012204   0.012427\n",
      "CIFAR-10  0.005502  0.007393   0.008377   0.009292   0.009737\n",
      "Nus-Wide  0.001005  0.001233   0.001462   0.001585   0.001688\n"
     ]
    }
   ],
   "source": [
    "print(\"Resultados de Precision en validación\")\n",
    "t = pd.DataFrame() #Table()\n",
    "t[\"N bits\"] = Nbits\n",
    "t[\"MNIST-R\"] = traditional_dat[\"mnist-raw\"][\"p\"]\n",
    "t[\"MNIST\"] = traditional_dat[\"mnist\"][\"p\"]\n",
    "t[\"CIFAR-10\"] = traditional_dat[\"cifar-10\"][\"p\"]\n",
    "t[\"Nus-Wide\"] = traditional_dat[\"nus-wide\"][\"p\"]\n",
    "print(\"\\n*** VAE Traditional***\")\n",
    "print(t.T)\n",
    "\n",
    "t = pd.DataFrame() #Table()\n",
    "t[\"N bits\"] = Nbits\n",
    "t[\"MNIST-R\"] = binary_dat[\"mnist-raw\"][\"p\"]\n",
    "t[\"MNIST\"] = binary_dat[\"mnist\"][\"p\"]\n",
    "t[\"CIFAR-10\"] = binary_dat[\"cifar-10\"][\"p\"]\n",
    "t[\"Nus-Wide\"] = binary_dat[\"nus-wide\"][\"p\"]\n",
    "print(\"\\n*** VAE Binary***\")\n",
    "print(t.T)\n",
    "\n",
    "print(\"\\nResultados de Recall en validación\")\n",
    "t = pd.DataFrame() #Table()\n",
    "t[\"N bits\"] = Nbits\n",
    "t[\"MNIST-R\"] = traditional_dat[\"mnist-raw\"][\"r\"]\n",
    "t[\"MNIST\"] = traditional_dat[\"mnist\"][\"r\"]\n",
    "t[\"CIFAR-10\"] = traditional_dat[\"cifar-10\"][\"r\"]\n",
    "t[\"Nus-Wide\"] = traditional_dat[\"nus-wide\"][\"r\"]\n",
    "print(\"\\n*** VAE Traditional***\")\n",
    "print(t.T)\n",
    "\n",
    "t = pd.DataFrame() #Table()\n",
    "t[\"N bits\"] = Nbits\n",
    "t[\"MNIST-R\"] = binary_dat[\"mnist-raw\"][\"r\"]\n",
    "t[\"MNIST\"] = binary_dat[\"mnist\"][\"r\"]\n",
    "t[\"CIFAR-10\"] = binary_dat[\"cifar-10\"][\"r\"]\n",
    "t[\"Nus-Wide\"] = binary_dat[\"nus-wide\"][\"r\"]\n",
    "print(\"\\n*** VAE Binary***\")\n",
    "print(t.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_map = {}\n",
    "results_p5000 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-encoder network:\n",
      "Model: \"pre-encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 500)               392500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 647,000\n",
      "Trainable params: 645,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "Model: \"generator/decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 784)               3920      \n",
      "=================================================================\n",
      "Total params: 3,920\n",
      "Trainable params: 3,920\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "pre-encoder network:\n",
      "Model: \"pre-encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 500)               392500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 647,000\n",
      "Trainable params: 645,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "Model: \"generator/decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 784)               3920      \n",
      "=================================================================\n",
      "Total params: 3,920\n",
      "Trainable params: 3,920\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "pre-encoder network:\n",
      "Model: \"pre-encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 500)               392500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 647,000\n",
      "Trainable params: 645,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "Model: \"generator/decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 784)               7056      \n",
      "=================================================================\n",
      "Total params: 7,056\n",
      "Trainable params: 7,056\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "pre-encoder network:\n",
      "Model: \"pre-encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 500)               392500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 647,000\n",
      "Trainable params: 645,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "Model: \"generator/decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 784)               7056      \n",
      "=================================================================\n",
      "Total params: 7,056\n",
      "Trainable params: 7,056\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "pre-encoder network:\n",
      "Model: \"pre-encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 500)               392500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 647,000\n",
      "Trainable params: 645,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "Model: \"generator/decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 784)               13328     \n",
      "=================================================================\n",
      "Total params: 13,328\n",
      "Trainable params: 13,328\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "pre-encoder network:\n",
      "Model: \"pre-encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 500)               392500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 647,000\n",
      "Trainable params: 645,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "Model: \"generator/decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 784)               13328     \n",
      "=================================================================\n",
      "Total params: 13,328\n",
      "Trainable params: 13,328\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-encoder network:\n",
      "Model: \"pre-encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 500)               392500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 647,000\n",
      "Trainable params: 645,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "Model: \"generator/decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 784)               25872     \n",
      "=================================================================\n",
      "Total params: 25,872\n",
      "Trainable params: 25,872\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "pre-encoder network:\n",
      "Model: \"pre-encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 500)               392500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 647,000\n",
      "Trainable params: 645,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "Model: \"generator/decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 784)               25872     \n",
      "=================================================================\n",
      "Total params: 25,872\n",
      "Trainable params: 25,872\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "pre-encoder network:\n",
      "Model: \"pre-encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 500)               392500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 647,000\n",
      "Trainable params: 645,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "Model: \"generator/decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 784)               50960     \n",
      "=================================================================\n",
      "Total params: 50,960\n",
      "Trainable params: 50,960\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "pre-encoder network:\n",
      "Model: \"pre-encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 500)               392500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 500)               2000      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 500)               2000      \n",
      "=================================================================\n",
      "Total params: 647,000\n",
      "Trainable params: 645,000\n",
      "Non-trainable params: 2,000\n",
      "_________________________________________________________________\n",
      "generator network:\n",
      "Model: \"generator/decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 784)               50960     \n",
      "=================================================================\n",
      "Total params: 50,960\n",
      "Trainable params: 50,960\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Nbits = np.asarray([4,8,16,32,64])\n",
    "dataset = name_dat.lower()\n",
    "\n",
    "results_map[dataset] = {\"vdsh\":[], \"bvae\":[]}\n",
    "results_p5000[dataset] = {\"vdsh\":[], \"bvae\":[]}\n",
    "\n",
    "for Nbit in Nbits:\n",
    "    traditional_vae,encoder_Tvae,generator_Tvae = traditional_VAE(X_total_input.shape[1],Nb=Nbit,units=500,layers_e=2,layers_d=0,beta=beta_T)\n",
    "    traditional_vae.fit(X_total_input, X_total, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    total_hash_VAE, test_hash_VAE = hash_data(encoder_Tvae,X_total_input,X_test_input, binary=False)\n",
    "    \n",
    "    map_t= evaluate_hashing_DE(total_hash_VAE,test_hash_VAE,labels_total,labels_test,eval_tipo=\"MAP\",K=9999999)\n",
    "    results_map[dataset][\"vdsh\"].append(map_t) \n",
    "    p5k_t = evaluate_hashing_DE(total_hash_VAE,test_hash_VAE,labels_total,labels_test,eval_tipo=\"Patk\",K=5000)\n",
    "    results_p5000[dataset][\"vdsh\"].append(p5k_t) \n",
    "    del traditional_vae, encoder_Tvae, generator_Tvae, total_hash_VAE, test_hash_VAE\n",
    "    keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "    \n",
    "    binary_vae,encoder_Bvae,generator_Bvae= binary_VAE(X_total_input.shape[1],Nb=Nbit,units=500,layers_e=2,layers_d=0,beta=beta_B)\n",
    "    binary_vae.fit(X_total_input, X_total, epochs=epochs, batch_size=batch_size, verbose=0 )\n",
    "    total_hash_BVAE, test_hash_BVAE = hash_data(encoder_Bvae,X_total_input,X_test_input)\n",
    "\n",
    "    map_b= evaluate_hashing_DE(total_hash_BVAE, test_hash_BVAE,labels_total,labels_test,eval_tipo=\"MAP\",K=9999999)\n",
    "    results_map[dataset][\"bvae\"].append(map_b) \n",
    "    p5k_b = evaluate_hashing_DE(total_hash_BVAE, test_hash_BVAE,labels_total,labels_test,eval_tipo=\"Patk\",K=5000)\n",
    "    results_p5000[dataset][\"bvae\"].append(p5k_b) \n",
    "    del binary_vae, encoder_Bvae, generator_Bvae, total_hash_BVAE, test_hash_BVAE\n",
    "    keras.backend.clear_session()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados de MAP baseline VDSH\n",
      "                 0         1          2          3          4\n",
      "N bits    4.000000  8.000000  16.000000  32.000000  64.000000\n",
      "MNIST     0.266510  0.349000   0.372094   0.392492   0.418837\n",
      "CIFAR10   0.230067  0.253973   0.235928   0.250337   0.251910\n",
      "Nus-Wide  0.515406  0.509889   0.520266   0.514649   0.506073\n",
      "\n",
      "Resultados de MAP proposal B-VAE\n",
      "                 0         1          2          3          4\n",
      "N bits    4.000000  8.000000  16.000000  32.000000  64.000000\n",
      "MNIST     0.303718  0.471363   0.505084   0.515641   0.474598\n",
      "CIFAR10   0.250369  0.287261   0.278468   0.270590   0.263636\n",
      "Nus-Wide  0.512154  0.531447   0.543008   0.547219   0.535714\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nResultados de MAP baseline VDSH\")\n",
    "t = pd.DataFrame() #Table()\n",
    "t[\"N bits\"] = Nbits\n",
    "t[\"MNIST\"] = results_map[\"mnist\"][\"vdsh\"]\n",
    "t[\"CIFAR10\"] = results_map[\"cifar-10\"][\"vdsh\"]\n",
    "t[\"Nus-Wide\"] = results_map[\"nus-wide\"][\"vdsh\"]\n",
    "print(t.T)\n",
    "\n",
    "print(\"\\nResultados de MAP proposal B-VAE\")\n",
    "t = pd.DataFrame() #Table()\n",
    "t[\"N bits\"] = Nbits\n",
    "t[\"MNIST\"] = results_map[\"mnist\"][\"bvae\"]\n",
    "t[\"CIFAR10\"] = results_map[\"cifar-10\"][\"bvae\"]\n",
    "t[\"Nus-Wide\"] = results_map[\"nus-wide\"][\"bvae\"]\n",
    "print(t.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados de P@5000 baseline VDSH\n",
      "                 0         1          2          3          4\n",
      "N bits    4.000000  8.000000  16.000000  32.000000  64.000000\n",
      "MNIST     0.312042  0.400094   0.428143   0.454135   0.481903\n",
      "CIFAR10   0.259382  0.285574   0.273450   0.290236   0.292835\n",
      "Nus-Wide  0.598701  0.625218   0.659350   0.666783   0.675772\n",
      "\n",
      "Resultados de P@5000 proposal B-VAE\n",
      "                 0         1          2          3          4\n",
      "N bits    4.000000  8.000000  16.000000  32.000000  64.000000\n",
      "MNIST     0.341123  0.535714   0.557387   0.572716   0.534882\n",
      "CIFAR10   0.287411  0.321964   0.313243   0.308242   0.303815\n",
      "Nus-Wide  0.578909  0.628041   0.677754   0.693372   0.691353\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nResultados de P@5000 baseline VDSH\")\n",
    "t = pd.DataFrame() #Table()\n",
    "t[\"N bits\"] = Nbits\n",
    "t[\"MNIST\"] = results_p5000[\"mnist\"][\"vdsh\"]\n",
    "t[\"CIFAR10\"] = results_p5000[\"cifar-10\"][\"vdsh\"]\n",
    "t[\"Nus-Wide\"] = results_p5000[\"nus-wide\"][\"vdsh\"]\n",
    "print(t.T)\n",
    "\n",
    "print(\"\\nResultados de P@5000 proposal B-VAE\")\n",
    "t = pd.DataFrame() #Table()\n",
    "t[\"N bits\"] = Nbits\n",
    "t[\"MNIST\"] = results_p5000[\"mnist\"][\"bvae\"]\n",
    "t[\"CIFAR10\"] = results_p5000[\"cifar-10\"][\"bvae\"]\n",
    "t[\"Nus-Wide\"] = results_p5000[\"nus-wide\"][\"bvae\"]\n",
    "print(t.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:newpy3_tf1]",
   "language": "python",
   "name": "conda-env-newpy3_tf1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
